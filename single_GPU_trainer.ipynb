{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353aadda-b8ec-438c-aa3a-fc4d66347a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 18:56:04.016128: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-21 18:56:04.611609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 18:56:07.993101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "from helper_function import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from models_1 import *\n",
    "from load_gpt2 import *\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0b8968-ad60-4ef3-aaec-d2a3d666bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30996f2d-69cd-49e4-bbd1-a1777aab12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    file_path = 'openorca_30.json'\n",
    "    with open(file_path, \"r\") as file:\n",
    "        inst_data = json.load(file)\n",
    "    # devide data into training testing validating\n",
    "    train_portion = int(len(inst_data) * 0.85)  # 85% for training\n",
    "    test_portion = int(len(inst_data) * 0.1)    # 10% for testing\n",
    "    val_portion = len(inst_data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "    \n",
    "    train_data = inst_data[:train_portion]\n",
    "    test_data = inst_data[train_portion:train_portion + test_portion]\n",
    "    val_data = inst_data[train_portion + test_portion:]\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #set up tokenizer\n",
    "    \n",
    "    # set up datasets\n",
    "    train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "    val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "    test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "def prepare_model():\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.0,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "    \n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "    \n",
    "    CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "    \n",
    "    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "    \n",
    "    model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "    settings, params = download_and_load_gpt2(\n",
    "        model_size=model_size, \n",
    "        models_dir=\"gpt2\"\n",
    "    )\n",
    "    \n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    load_weights_into_gpt(model, params)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        tokenizer,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler,\n",
    "        gpu_id,\n",
    "    ) -> None:\n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def _run_batch(self, inputs, targets, eval=False):\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "        if eval == False:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #self.scheduler.step()\n",
    "        else:\n",
    "            return loss.item()\n",
    "        return loss.item()\n",
    "\n",
    "    def _run_epoch(self, epoch, eval_step):\n",
    "        step = 0\n",
    "        total_loss = 0\n",
    "        self.model.train()\n",
    "        for input_batch, target_batch in tqdm(self.train_loader, desc=f\"Epoch {epoch}\"): \n",
    "            input_batch = input_batch.to(self.gpu_id)\n",
    "            target_batch = target_batch.to(self.gpu_id)\n",
    "            loss = self._run_batch(input_batch, target_batch)\n",
    "            total_loss += loss\n",
    "            step += 1\n",
    "            if step % eval_step == 0:\n",
    "                avg_train_loss = total_loss / eval_step\n",
    "                val_loss = self._validate()\n",
    "                print(f'Epoch {epoch}, Step {step}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "                total_loss = 0\n",
    "                self.model.train()\n",
    "                \n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "        num_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for input_batch, target_batch in self.val_loader:\n",
    "                input_batch = input_batch.to(self.gpu_id)\n",
    "                target_batch = target_batch.to(self.gpu_id)\n",
    "                loss = self._run_batch(input_batch, target_batch, eval=True)\n",
    "                total_val_loss += loss\n",
    "                num_val_batches += 1\n",
    "                if num_val_batches >= 10:\n",
    "                    break\n",
    "        return total_val_loss / num_val_batches\n",
    "        \n",
    "    def train(self, max_epochs, eval_step):\n",
    "        for epoch in range(max_epochs):\n",
    "            self._run_epoch(epoch, eval_step)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.module.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c300705-5698-45d0-876e-56a05a234a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1079649/1079649 [08:52<00:00, 2028.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 63510/63510 [00:34<00:00, 1832.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 127017/127017 [01:24<00:00, 1507.88it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set, test_set = prepare_dataset()\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device = 'cpu',\n",
    "    allowed_max_length=1024\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=4,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=4,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=4,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235b6eac-a7f4-488a-b7fc-3784217f3e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\") #set up tokenizer\n",
    "# model ready================\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=len(train_loader), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eb60dba-d19e-4c50-acdc-cb9ff450ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, tokenizer, train_loader, val_loader, test_loader, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "465800c6-b770-4728-94ed-d3ad2a8fd810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|███▍                                                                     | 11/233 [00:02<00:54,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 10, Train Loss: 0.6073, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|██████▌                                                                  | 21/233 [00:04<00:52,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 20, Train Loss: 0.6013, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  13%|█████████▋                                                               | 31/233 [00:06<00:51,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 30, Train Loss: 0.5437, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  18%|████████████▊                                                            | 41/233 [00:08<00:47,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 40, Train Loss: 0.5549, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  22%|███████████████▉                                                         | 51/233 [00:10<00:45,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 50, Train Loss: 0.5708, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  26%|███████████████████                                                      | 61/233 [00:12<00:42,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 60, Train Loss: 0.4433, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  30%|██████████████████████▏                                                  | 71/233 [00:14<00:39,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 70, Train Loss: 0.4218, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  35%|█████████████████████████▍                                               | 81/233 [00:16<00:37,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 80, Train Loss: 0.4421, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|████████████████████████████▌                                            | 91/233 [00:18<00:35,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 90, Train Loss: 0.3956, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  43%|███████████████████████████████▏                                        | 101/233 [00:20<00:33,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 100, Train Loss: 0.3831, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  48%|██████████████████████████████████▎                                     | 111/233 [00:22<00:31,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 110, Train Loss: 0.4420, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  52%|█████████████████████████████████████▍                                  | 121/233 [00:24<00:28,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 120, Train Loss: 0.4698, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  56%|████████████████████████████████████████▍                               | 131/233 [00:26<00:25,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 130, Train Loss: 0.4207, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  61%|███████████████████████████████████████████▌                            | 141/233 [00:28<00:22,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 140, Train Loss: 0.4758, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  65%|██████████████████████████████████████████████▋                         | 151/233 [00:29<00:15,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 150, Train Loss: 0.5165, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  69%|█████████████████████████████████████████████████▊                      | 161/233 [00:31<00:13,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 160, Train Loss: 0.4333, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  73%|████████████████████████████████████████████████████▊                   | 171/233 [00:32<00:11,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 170, Train Loss: 0.5059, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  78%|███████████████████████████████████████████████████████▉                | 181/233 [00:34<00:09,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 180, Train Loss: 0.4903, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  82%|███████████████████████████████████████████████████████████             | 191/233 [00:35<00:07,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 190, Train Loss: 0.5689, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  86%|██████████████████████████████████████████████████████████████          | 201/233 [00:37<00:05,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 200, Train Loss: 0.6205, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  91%|█████████████████████████████████████████████████████████████████▏      | 211/233 [00:38<00:04,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 210, Train Loss: 0.6840, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|████████████████████████████████████████████████████████████████████▎   | 221/233 [00:40<00:02,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 220, Train Loss: 0.7095, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|███████████████████████████████████████████████████████████████████████▍| 231/233 [00:42<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 230, Train Loss: 0.6524, Val Loss: 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████| 233/233 [00:42<00:00,  5.51it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c881f1c2-b05f-42e7-a353-29a32fba6774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
