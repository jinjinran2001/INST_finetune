{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06e59fe-ae72-4022-8291-3b13ac72cf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 00:58:49.828237: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 00:58:50.299325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 00:58:54.000040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from ddp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a520388d-295d-420a-9787-ee8dfed03b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a892814a-ae07-41c9-b323-263cdd044a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 4  # Number of processes\n",
    "n_epoch = 2 # Number of epochs\n",
    "eval_iter = 25 # evaluate per eval_iter\n",
    "save_every = 1000 # save per save_every steps\n",
    "data_path = 'data/math_inst.json' # data file\n",
    "ckpt_path = 'models/math_ckpt_2_epochs.pth' # check point file path\n",
    "batch_size = 2 # batch size, highest 2 for the current gpu\n",
    "load_from = None # load trained model for continue training\n",
    "lr = 5e-5 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f499d9ea-7c76-45e2-8801-d5ac76c35173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 01:00:14.092622: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 01:00:14.092644: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 01:00:14.092621: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 01:00:14.092622: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 01:00:15.212447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 01:00:15.212447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 01:00:15.212447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 01:00:15.212447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 01:00:19.102098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-08-02 01:00:19.111919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-08-02 01:00:19.121054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-08-02 01:00:19.162193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 222733/222733 [00:39<00:00, 5573.66it/s]\n",
      "100%|██████████| 222733/222733 [00:40<00:00, 5465.37it/s]\n",
      "100%|██████████| 222733/222733 [00:40<00:00, 5455.28it/s]\n",
      "100%|██████████| 13103/13103 [00:02<00:00, 5444.60it/s]s]\n",
      "100%|██████████| 222733/222733 [00:42<00:00, 5256.61it/s]\n",
      "100%|██████████| 13103/13103 [00:02<00:00, 5417.97it/s]\n",
      "100%|██████████| 13103/13103 [00:02<00:00, 5380.21it/s]\n",
      "100%|██████████| 13103/13103 [00:02<00:00, 5097.19it/s]\n",
      "100%|██████████| 26203/26203 [00:04<00:00, 5469.54it/s]\n",
      "100%|██████████| 26203/26203 [00:04<00:00, 5331.51it/s]\n",
      "100%|██████████| 26203/26203 [00:04<00:00, 5342.41it/s]\n",
      "100%|██████████| 26203/26203 [00:05<00:00, 5081.56it/s]\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ready\n",
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "trainer ready, start training\n",
      "27842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  13%|█▎        | 3624/27842 [26:53<2:53:37,  2.32it/s, train_loss=1.1746, val_loss=1.0672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 25, Train Loss: 2.4185, Val Loss: 2.4577\n",
      "Epoch 0, Step 50, Train Loss: 2.2540, Val Loss: 2.1700\n",
      "Epoch 0, Step 75, Train Loss: 1.9902, Val Loss: 1.8829\n",
      "Epoch 0, Step 100, Train Loss: 1.6671, Val Loss: 1.6147\n",
      "Epoch 0, Step 125, Train Loss: 1.5897, Val Loss: 1.5403\n",
      "Epoch 0, Step 150, Train Loss: 1.5713, Val Loss: 1.4979\n",
      "Epoch 0, Step 175, Train Loss: 1.5095, Val Loss: 1.4710\n",
      "Epoch 0, Step 200, Train Loss: 1.5424, Val Loss: 1.4471\n",
      "Epoch 0, Step 225, Train Loss: 1.4993, Val Loss: 1.4275\n",
      "Epoch 0, Step 250, Train Loss: 1.4482, Val Loss: 1.4086\n",
      "Epoch 0, Step 275, Train Loss: 1.4401, Val Loss: 1.3906\n",
      "Epoch 0, Step 300, Train Loss: 1.4767, Val Loss: 1.3765\n",
      "Epoch 0, Step 325, Train Loss: 1.3662, Val Loss: 1.3606\n",
      "Epoch 0, Step 350, Train Loss: 1.4474, Val Loss: 1.3456\n",
      "Epoch 0, Step 375, Train Loss: 1.4407, Val Loss: 1.3362\n",
      "Epoch 0, Step 400, Train Loss: 1.4403, Val Loss: 1.3263\n",
      "Epoch 0, Step 425, Train Loss: 1.3606, Val Loss: 1.3149\n",
      "Epoch 0, Step 450, Train Loss: 1.3398, Val Loss: 1.3015\n",
      "Epoch 0, Step 475, Train Loss: 1.4080, Val Loss: 1.2905\n",
      "Epoch 0, Step 500, Train Loss: 1.3350, Val Loss: 1.2808\n",
      "Epoch 0, Step 525, Train Loss: 1.3179, Val Loss: 1.2755\n",
      "Epoch 0, Step 550, Train Loss: 1.2429, Val Loss: 1.2689\n",
      "Epoch 0, Step 575, Train Loss: 1.4183, Val Loss: 1.2641\n",
      "Epoch 0, Step 600, Train Loss: 1.3787, Val Loss: 1.2587\n",
      "Epoch 0, Step 625, Train Loss: 1.2951, Val Loss: 1.2537\n",
      "Epoch 0, Step 650, Train Loss: 1.3048, Val Loss: 1.2499\n",
      "Epoch 0, Step 675, Train Loss: 1.3005, Val Loss: 1.2445\n",
      "Epoch 0, Step 700, Train Loss: 1.3639, Val Loss: 1.2365\n",
      "Epoch 0, Step 725, Train Loss: 1.3483, Val Loss: 1.2367\n",
      "Epoch 0, Step 750, Train Loss: 1.3088, Val Loss: 1.2358\n",
      "Epoch 0, Step 775, Train Loss: 1.3325, Val Loss: 1.2300\n",
      "Epoch 0, Step 800, Train Loss: 1.2222, Val Loss: 1.2276\n",
      "Epoch 0, Step 825, Train Loss: 1.3228, Val Loss: 1.2228\n",
      "Epoch 0, Step 850, Train Loss: 1.2821, Val Loss: 1.2200\n",
      "Epoch 0, Step 875, Train Loss: 1.3452, Val Loss: 1.2165\n",
      "Epoch 0, Step 900, Train Loss: 1.3028, Val Loss: 1.2124\n",
      "Epoch 0, Step 925, Train Loss: 1.2611, Val Loss: 1.2098\n",
      "Epoch 0, Step 950, Train Loss: 1.2850, Val Loss: 1.2062\n",
      "Epoch 0, Step 975, Train Loss: 1.2744, Val Loss: 1.2050\n",
      "Epoch 0, Step 1000, Train Loss: 1.3560, Val Loss: 1.2008\n",
      "Epoch 0, Step 1025, Train Loss: 1.3099, Val Loss: 1.1989\n",
      "Epoch 0, Step 1050, Train Loss: 1.3543, Val Loss: 1.1966\n",
      "Epoch 0, Step 1075, Train Loss: 1.2684, Val Loss: 1.1909\n",
      "Epoch 0, Step 1100, Train Loss: 1.2896, Val Loss: 1.1897\n",
      "Epoch 0, Step 1125, Train Loss: 1.2809, Val Loss: 1.1861\n",
      "Epoch 0, Step 1150, Train Loss: 1.3289, Val Loss: 1.1820\n",
      "Epoch 0, Step 1175, Train Loss: 1.2269, Val Loss: 1.1794\n",
      "Epoch 0, Step 1200, Train Loss: 1.2310, Val Loss: 1.1762\n",
      "Epoch 0, Step 1225, Train Loss: 1.3361, Val Loss: 1.1769\n",
      "Epoch 0, Step 1250, Train Loss: 1.2972, Val Loss: 1.1739\n",
      "Epoch 0, Step 1275, Train Loss: 1.2370, Val Loss: 1.1718\n",
      "Epoch 0, Step 1300, Train Loss: 1.2320, Val Loss: 1.1717\n",
      "Epoch 0, Step 1325, Train Loss: 1.2936, Val Loss: 1.1684\n",
      "Epoch 0, Step 1350, Train Loss: 1.2994, Val Loss: 1.1678\n",
      "Epoch 0, Step 1375, Train Loss: 1.2907, Val Loss: 1.1649\n",
      "Epoch 0, Step 1400, Train Loss: 1.2964, Val Loss: 1.1631\n",
      "Epoch 0, Step 1425, Train Loss: 1.2438, Val Loss: 1.1609\n",
      "Epoch 0, Step 1450, Train Loss: 1.2922, Val Loss: 1.1575\n",
      "Epoch 0, Step 1475, Train Loss: 1.2808, Val Loss: 1.1599\n",
      "Epoch 0, Step 1500, Train Loss: 1.1555, Val Loss: 1.1556\n",
      "Epoch 0, Step 1525, Train Loss: 1.2202, Val Loss: 1.1542\n",
      "Epoch 0, Step 1550, Train Loss: 1.2483, Val Loss: 1.1523\n",
      "Epoch 0, Step 1575, Train Loss: 1.1722, Val Loss: 1.1517\n",
      "Epoch 0, Step 1600, Train Loss: 1.1068, Val Loss: 1.1528\n",
      "Epoch 0, Step 1625, Train Loss: 1.2698, Val Loss: 1.1560\n",
      "Epoch 0, Step 1650, Train Loss: 1.2478, Val Loss: 1.1512\n",
      "Epoch 0, Step 1675, Train Loss: 1.2609, Val Loss: 1.1485\n",
      "Epoch 0, Step 1700, Train Loss: 1.2252, Val Loss: 1.1444\n",
      "Epoch 0, Step 1725, Train Loss: 1.2618, Val Loss: 1.1451\n",
      "Epoch 0, Step 1750, Train Loss: 1.2070, Val Loss: 1.1444\n",
      "Epoch 0, Step 1775, Train Loss: 1.2078, Val Loss: 1.1424\n",
      "Epoch 0, Step 1800, Train Loss: 1.1661, Val Loss: 1.1407\n",
      "Epoch 0, Step 1825, Train Loss: 1.2527, Val Loss: 1.1389\n",
      "Epoch 0, Step 1850, Train Loss: 1.2480, Val Loss: 1.1366\n",
      "Epoch 0, Step 1875, Train Loss: 1.2395, Val Loss: 1.1327\n",
      "Epoch 0, Step 1900, Train Loss: 1.3135, Val Loss: 1.1316\n",
      "Epoch 0, Step 1925, Train Loss: 1.1883, Val Loss: 1.1325\n",
      "Epoch 0, Step 1950, Train Loss: 1.2137, Val Loss: 1.1310\n",
      "Epoch 0, Step 1975, Train Loss: 1.3008, Val Loss: 1.1250\n",
      "Epoch 0, Step 2000, Train Loss: 1.2220, Val Loss: 1.1291\n",
      "Epoch 0, Step 2025, Train Loss: 1.1580, Val Loss: 1.1275\n",
      "Epoch 0, Step 2050, Train Loss: 1.1970, Val Loss: 1.1279\n",
      "Epoch 0, Step 2075, Train Loss: 1.2566, Val Loss: 1.1251\n",
      "Epoch 0, Step 2100, Train Loss: 1.1872, Val Loss: 1.1249\n",
      "Epoch 0, Step 2125, Train Loss: 1.1965, Val Loss: 1.1204\n",
      "Epoch 0, Step 2150, Train Loss: 1.2434, Val Loss: 1.1184\n",
      "Epoch 0, Step 2175, Train Loss: 1.2212, Val Loss: 1.1191\n",
      "Epoch 0, Step 2200, Train Loss: 1.1520, Val Loss: 1.1149\n",
      "Epoch 0, Step 2225, Train Loss: 1.0857, Val Loss: 1.1166\n",
      "Epoch 0, Step 2250, Train Loss: 1.0465, Val Loss: 1.1123\n",
      "Epoch 0, Step 2275, Train Loss: 1.1204, Val Loss: 1.1124\n",
      "Epoch 0, Step 2300, Train Loss: 1.1308, Val Loss: 1.1104\n",
      "Epoch 0, Step 2325, Train Loss: 1.2089, Val Loss: 1.1102\n",
      "Epoch 0, Step 2350, Train Loss: 1.1913, Val Loss: 1.1112\n",
      "Epoch 0, Step 2375, Train Loss: 1.1655, Val Loss: 1.1054\n",
      "Epoch 0, Step 2400, Train Loss: 1.1866, Val Loss: 1.1078\n",
      "Epoch 0, Step 2425, Train Loss: 1.2151, Val Loss: 1.1104\n",
      "Epoch 0, Step 2450, Train Loss: 1.1345, Val Loss: 1.1091\n",
      "Epoch 0, Step 2475, Train Loss: 1.2148, Val Loss: 1.1039\n",
      "Epoch 0, Step 2500, Train Loss: 1.1360, Val Loss: 1.1079\n",
      "Epoch 0, Step 2525, Train Loss: 1.1903, Val Loss: 1.1048\n",
      "Epoch 0, Step 2550, Train Loss: 1.2118, Val Loss: 1.1037\n",
      "Epoch 0, Step 2575, Train Loss: 1.2618, Val Loss: 1.1041\n",
      "Epoch 0, Step 2600, Train Loss: 1.2124, Val Loss: 1.1037\n",
      "Epoch 0, Step 2625, Train Loss: 1.0540, Val Loss: 1.1052\n",
      "Epoch 0, Step 2650, Train Loss: 1.2006, Val Loss: 1.1041\n",
      "Epoch 0, Step 2675, Train Loss: 1.2799, Val Loss: 1.1005\n",
      "Epoch 0, Step 2700, Train Loss: 1.0448, Val Loss: 1.0945\n",
      "Epoch 0, Step 2725, Train Loss: 1.2840, Val Loss: 1.0945\n",
      "Epoch 0, Step 2750, Train Loss: 1.1700, Val Loss: 1.0969\n",
      "Epoch 0, Step 2775, Train Loss: 1.1228, Val Loss: 1.0961\n",
      "Epoch 0, Step 2800, Train Loss: 1.2131, Val Loss: 1.0920\n",
      "Epoch 0, Step 2825, Train Loss: 1.2509, Val Loss: 1.0916\n",
      "Epoch 0, Step 2850, Train Loss: 1.1351, Val Loss: 1.0898\n",
      "Epoch 0, Step 2875, Train Loss: 1.2374, Val Loss: 1.0868\n",
      "Epoch 0, Step 2900, Train Loss: 1.1440, Val Loss: 1.0872\n",
      "Epoch 0, Step 2925, Train Loss: 1.1691, Val Loss: 1.0876\n",
      "Epoch 0, Step 2950, Train Loss: 1.1803, Val Loss: 1.0913\n",
      "Epoch 0, Step 2975, Train Loss: 1.1098, Val Loss: 1.0898\n",
      "Epoch 0, Step 3000, Train Loss: 1.1788, Val Loss: 1.0830\n",
      "Epoch 0, Step 3025, Train Loss: 1.1958, Val Loss: 1.0793\n",
      "Epoch 0, Step 3050, Train Loss: 1.1269, Val Loss: 1.0817\n",
      "Epoch 0, Step 3075, Train Loss: 1.1723, Val Loss: 1.0791\n",
      "Epoch 0, Step 3100, Train Loss: 1.1564, Val Loss: 1.0776\n",
      "Epoch 0, Step 3125, Train Loss: 1.1638, Val Loss: 1.0769\n",
      "Epoch 0, Step 3150, Train Loss: 1.1672, Val Loss: 1.0770\n",
      "Epoch 0, Step 3175, Train Loss: 1.0897, Val Loss: 1.0782\n",
      "Epoch 0, Step 3200, Train Loss: 1.1261, Val Loss: 1.0781\n",
      "Epoch 0, Step 3225, Train Loss: 1.1444, Val Loss: 1.0788\n",
      "Epoch 0, Step 3250, Train Loss: 1.1452, Val Loss: 1.0747\n",
      "Epoch 0, Step 3275, Train Loss: 1.2175, Val Loss: 1.0780\n",
      "Epoch 0, Step 3300, Train Loss: 1.1817, Val Loss: 1.0784\n",
      "Epoch 0, Step 3325, Train Loss: 1.0710, Val Loss: 1.0746\n",
      "Epoch 0, Step 3350, Train Loss: 1.2077, Val Loss: 1.0766\n",
      "Epoch 0, Step 3375, Train Loss: 1.1310, Val Loss: 1.0738\n",
      "Epoch 0, Step 3400, Train Loss: 1.1456, Val Loss: 1.0765\n",
      "Epoch 0, Step 3425, Train Loss: 1.1083, Val Loss: 1.0712\n",
      "Epoch 0, Step 3450, Train Loss: 1.1508, Val Loss: 1.0744\n",
      "Epoch 0, Step 3475, Train Loss: 1.0819, Val Loss: 1.0687\n",
      "Epoch 0, Step 3500, Train Loss: 1.1400, Val Loss: 1.0688\n",
      "Epoch 0, Step 3525, Train Loss: 1.1438, Val Loss: 1.0692\n",
      "Epoch 0, Step 3550, Train Loss: 1.0105, Val Loss: 1.0676\n",
      "Epoch 0, Step 3575, Train Loss: 0.9450, Val Loss: 1.0656\n",
      "Epoch 0, Step 3600, Train Loss: 1.1095, Val Loss: 1.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  26%|██▌       | 7199/27842 [54:38<2:57:20,  1.94it/s, train_loss=0.9577, val_loss=0.9488]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 3625, Train Loss: 1.1746, Val Loss: 1.0672\n",
      "Epoch 0, Step 3650, Train Loss: 1.1837, Val Loss: 1.0687\n",
      "Epoch 0, Step 3675, Train Loss: 1.1560, Val Loss: 1.0704\n",
      "Epoch 0, Step 3700, Train Loss: 1.1559, Val Loss: 1.0691\n",
      "Epoch 0, Step 3725, Train Loss: 1.1678, Val Loss: 1.0575\n",
      "Epoch 0, Step 3750, Train Loss: 1.1035, Val Loss: 1.0580\n",
      "Epoch 0, Step 3775, Train Loss: 1.0929, Val Loss: 1.0552\n",
      "Epoch 0, Step 3800, Train Loss: 1.1975, Val Loss: 1.0513\n",
      "Epoch 0, Step 3825, Train Loss: 1.1269, Val Loss: 1.0522\n",
      "Epoch 0, Step 3850, Train Loss: 1.0921, Val Loss: 1.0549\n",
      "Epoch 0, Step 3875, Train Loss: 1.0501, Val Loss: 1.0545\n",
      "Epoch 0, Step 3900, Train Loss: 1.1276, Val Loss: 1.0520\n",
      "Epoch 0, Step 3925, Train Loss: 1.1091, Val Loss: 1.0470\n",
      "Epoch 0, Step 3950, Train Loss: 1.1800, Val Loss: 1.0475\n",
      "Epoch 0, Step 3975, Train Loss: 1.1025, Val Loss: 1.0467\n",
      "Epoch 0, Step 4000, Train Loss: 1.1569, Val Loss: 1.0503\n",
      "Epoch 0, Step 4025, Train Loss: 1.0163, Val Loss: 1.0480\n",
      "Epoch 0, Step 4050, Train Loss: 1.0662, Val Loss: 1.0431\n",
      "Epoch 0, Step 4075, Train Loss: 1.0539, Val Loss: 1.0446\n",
      "Epoch 0, Step 4100, Train Loss: 1.2419, Val Loss: 1.0469\n",
      "Epoch 0, Step 4125, Train Loss: 1.1881, Val Loss: 1.0471\n",
      "Epoch 0, Step 4150, Train Loss: 1.1421, Val Loss: 1.0456\n",
      "Epoch 0, Step 4175, Train Loss: 1.0409, Val Loss: 1.0418\n",
      "Epoch 0, Step 4200, Train Loss: 1.2002, Val Loss: 1.0373\n",
      "Epoch 0, Step 4225, Train Loss: 1.1527, Val Loss: 1.0432\n",
      "Epoch 0, Step 4250, Train Loss: 1.0978, Val Loss: 1.0382\n",
      "Epoch 0, Step 4275, Train Loss: 1.0310, Val Loss: 1.0412\n",
      "Epoch 0, Step 4300, Train Loss: 1.1829, Val Loss: 1.0453\n",
      "Epoch 0, Step 4325, Train Loss: 1.2099, Val Loss: 1.0351\n",
      "Epoch 0, Step 4350, Train Loss: 1.0987, Val Loss: 1.0440\n",
      "Epoch 0, Step 4375, Train Loss: 1.0290, Val Loss: 1.0396\n",
      "Epoch 0, Step 4400, Train Loss: 1.0047, Val Loss: 1.0376\n",
      "Epoch 0, Step 4425, Train Loss: 1.0150, Val Loss: 1.0373\n",
      "Epoch 0, Step 4450, Train Loss: 1.0961, Val Loss: 1.0405\n",
      "Epoch 0, Step 4475, Train Loss: 1.0664, Val Loss: 1.0363\n",
      "Epoch 0, Step 4500, Train Loss: 1.1451, Val Loss: 1.0388\n",
      "Epoch 0, Step 4525, Train Loss: 1.1191, Val Loss: 1.0337\n",
      "Epoch 0, Step 4550, Train Loss: 1.1355, Val Loss: 1.0325\n",
      "Epoch 0, Step 4575, Train Loss: 1.1778, Val Loss: 1.0280\n",
      "Epoch 0, Step 4600, Train Loss: 1.1214, Val Loss: 1.0287\n",
      "Epoch 0, Step 4625, Train Loss: 1.0666, Val Loss: 1.0305\n",
      "Epoch 0, Step 4650, Train Loss: 1.0959, Val Loss: 1.0264\n",
      "Epoch 0, Step 4675, Train Loss: 1.0564, Val Loss: 1.0250\n",
      "Epoch 0, Step 4700, Train Loss: 1.1277, Val Loss: 1.0242\n",
      "Epoch 0, Step 4725, Train Loss: 1.0995, Val Loss: 1.0259\n",
      "Epoch 0, Step 4750, Train Loss: 1.0340, Val Loss: 1.0205\n",
      "Epoch 0, Step 4775, Train Loss: 1.1580, Val Loss: 1.0262\n",
      "Epoch 0, Step 4800, Train Loss: 1.0995, Val Loss: 1.0281\n",
      "Epoch 0, Step 4825, Train Loss: 1.0881, Val Loss: 1.0298\n",
      "Epoch 0, Step 4850, Train Loss: 1.0869, Val Loss: 1.0282\n",
      "Epoch 0, Step 4875, Train Loss: 1.0046, Val Loss: 1.0271\n",
      "Epoch 0, Step 4900, Train Loss: 0.9616, Val Loss: 1.0250\n",
      "Epoch 0, Step 4925, Train Loss: 1.0500, Val Loss: 1.0257\n",
      "Epoch 0, Step 4950, Train Loss: 1.0413, Val Loss: 1.0181\n",
      "Epoch 0, Step 4975, Train Loss: 1.1641, Val Loss: 1.0120\n",
      "Epoch 0, Step 5000, Train Loss: 1.0518, Val Loss: 1.0139\n",
      "Epoch 0, Step 5025, Train Loss: 1.1289, Val Loss: 1.0143\n",
      "Epoch 0, Step 5050, Train Loss: 1.0866, Val Loss: 1.0073\n",
      "Epoch 0, Step 5075, Train Loss: 1.0251, Val Loss: 1.0014\n",
      "Epoch 0, Step 5100, Train Loss: 1.0805, Val Loss: 1.0001\n",
      "Epoch 0, Step 5125, Train Loss: 1.0049, Val Loss: 1.0010\n",
      "Epoch 0, Step 5150, Train Loss: 1.0601, Val Loss: 0.9994\n",
      "Epoch 0, Step 5175, Train Loss: 0.9807, Val Loss: 0.9997\n",
      "Epoch 0, Step 5200, Train Loss: 0.9800, Val Loss: 1.0021\n",
      "Epoch 0, Step 5225, Train Loss: 1.0424, Val Loss: 1.0139\n",
      "Epoch 0, Step 5250, Train Loss: 1.1233, Val Loss: 1.0033\n",
      "Epoch 0, Step 5275, Train Loss: 0.9494, Val Loss: 0.9993\n",
      "Epoch 0, Step 5300, Train Loss: 1.0695, Val Loss: 1.0037\n",
      "Epoch 0, Step 5325, Train Loss: 1.0822, Val Loss: 0.9996\n",
      "Epoch 0, Step 5350, Train Loss: 1.0772, Val Loss: 1.0036\n",
      "Epoch 0, Step 5375, Train Loss: 1.0543, Val Loss: 0.9979\n",
      "Epoch 0, Step 5400, Train Loss: 1.0278, Val Loss: 0.9983\n",
      "Epoch 0, Step 5425, Train Loss: 1.0959, Val Loss: 0.9998\n",
      "Epoch 0, Step 5450, Train Loss: 1.1044, Val Loss: 0.9966\n",
      "Epoch 0, Step 5475, Train Loss: 1.0663, Val Loss: 1.0025\n",
      "Epoch 0, Step 5500, Train Loss: 1.0465, Val Loss: 0.9970\n",
      "Epoch 0, Step 5525, Train Loss: 1.0646, Val Loss: 0.9947\n",
      "Epoch 0, Step 5550, Train Loss: 1.0058, Val Loss: 0.9857\n",
      "Epoch 0, Step 5575, Train Loss: 1.0978, Val Loss: 0.9894\n",
      "Epoch 0, Step 5600, Train Loss: 1.0950, Val Loss: 0.9901\n",
      "Epoch 0, Step 5625, Train Loss: 1.0742, Val Loss: 1.0010\n",
      "Epoch 0, Step 5650, Train Loss: 1.0426, Val Loss: 0.9970\n",
      "Epoch 0, Step 5675, Train Loss: 1.0156, Val Loss: 0.9900\n",
      "Epoch 0, Step 5700, Train Loss: 0.9786, Val Loss: 0.9988\n",
      "Epoch 0, Step 5725, Train Loss: 0.9980, Val Loss: 0.9957\n",
      "Epoch 0, Step 5750, Train Loss: 1.1062, Val Loss: 0.9877\n",
      "Epoch 0, Step 5775, Train Loss: 1.0525, Val Loss: 0.9903\n",
      "Epoch 0, Step 5800, Train Loss: 1.1368, Val Loss: 0.9927\n",
      "Epoch 0, Step 5825, Train Loss: 1.0483, Val Loss: 0.9868\n",
      "Epoch 0, Step 5850, Train Loss: 1.0732, Val Loss: 0.9979\n",
      "Epoch 0, Step 5875, Train Loss: 1.1272, Val Loss: 0.9927\n",
      "Epoch 0, Step 5900, Train Loss: 1.0161, Val Loss: 0.9927\n",
      "Epoch 0, Step 5925, Train Loss: 1.0486, Val Loss: 0.9959\n",
      "Epoch 0, Step 5950, Train Loss: 0.9872, Val Loss: 0.9833\n",
      "Epoch 0, Step 5975, Train Loss: 1.0406, Val Loss: 0.9894\n",
      "Epoch 0, Step 6000, Train Loss: 1.1373, Val Loss: 0.9882\n",
      "Epoch 0, Step 6025, Train Loss: 0.9385, Val Loss: 0.9936\n",
      "Epoch 0, Step 6050, Train Loss: 1.0873, Val Loss: 0.9907\n",
      "Epoch 0, Step 6075, Train Loss: 1.0212, Val Loss: 0.9928\n",
      "Epoch 0, Step 6100, Train Loss: 1.0041, Val Loss: 0.9901\n",
      "Epoch 0, Step 6125, Train Loss: 1.0128, Val Loss: 0.9880\n",
      "Epoch 0, Step 6150, Train Loss: 1.1100, Val Loss: 0.9860\n",
      "Epoch 0, Step 6175, Train Loss: 1.0552, Val Loss: 0.9829\n",
      "Epoch 0, Step 6200, Train Loss: 1.0619, Val Loss: 0.9798\n",
      "Epoch 0, Step 6225, Train Loss: 0.9796, Val Loss: 0.9823\n",
      "Epoch 0, Step 6250, Train Loss: 1.0653, Val Loss: 0.9819\n",
      "Epoch 0, Step 6275, Train Loss: 0.9959, Val Loss: 0.9846\n",
      "Epoch 0, Step 6300, Train Loss: 1.1022, Val Loss: 0.9787\n",
      "Epoch 0, Step 6325, Train Loss: 0.8981, Val Loss: 0.9824\n",
      "Epoch 0, Step 6350, Train Loss: 1.0053, Val Loss: 0.9814\n",
      "Epoch 0, Step 6375, Train Loss: 1.0097, Val Loss: 0.9888\n",
      "Epoch 0, Step 6400, Train Loss: 0.9990, Val Loss: 0.9728\n",
      "Epoch 0, Step 6425, Train Loss: 1.0380, Val Loss: 0.9779\n",
      "Epoch 0, Step 6450, Train Loss: 0.9640, Val Loss: 0.9791\n",
      "Epoch 0, Step 6475, Train Loss: 0.9904, Val Loss: 0.9793\n",
      "Epoch 0, Step 6500, Train Loss: 0.9287, Val Loss: 0.9829\n",
      "Epoch 0, Step 6525, Train Loss: 0.9984, Val Loss: 0.9837\n",
      "Epoch 0, Step 6550, Train Loss: 1.0067, Val Loss: 0.9735\n",
      "Epoch 0, Step 6575, Train Loss: 0.9531, Val Loss: 0.9743\n",
      "Epoch 0, Step 6600, Train Loss: 1.0889, Val Loss: 0.9710\n",
      "Epoch 0, Step 6625, Train Loss: 1.0229, Val Loss: 0.9690\n",
      "Epoch 0, Step 6650, Train Loss: 1.0840, Val Loss: 0.9672\n",
      "Epoch 0, Step 6675, Train Loss: 0.8997, Val Loss: 0.9714\n",
      "Epoch 0, Step 6700, Train Loss: 0.9974, Val Loss: 0.9798\n",
      "Epoch 0, Step 6725, Train Loss: 1.0080, Val Loss: 0.9769\n",
      "Epoch 0, Step 6750, Train Loss: 1.0339, Val Loss: 0.9829\n",
      "Epoch 0, Step 6775, Train Loss: 0.9720, Val Loss: 0.9805\n",
      "Epoch 0, Step 6800, Train Loss: 1.0751, Val Loss: 0.9807\n",
      "Epoch 0, Step 6825, Train Loss: 1.0718, Val Loss: 0.9821\n",
      "Epoch 0, Step 6850, Train Loss: 1.0176, Val Loss: 0.9738\n",
      "Epoch 0, Step 6875, Train Loss: 1.0716, Val Loss: 0.9755\n",
      "Epoch 0, Step 6900, Train Loss: 0.9609, Val Loss: 0.9711\n",
      "Epoch 0, Step 6925, Train Loss: 1.0580, Val Loss: 0.9767\n",
      "Epoch 0, Step 6950, Train Loss: 1.0367, Val Loss: 0.9731\n",
      "Epoch 0, Step 6975, Train Loss: 1.0324, Val Loss: 0.9621\n",
      "Epoch 0, Step 7000, Train Loss: 1.0135, Val Loss: 0.9787\n",
      "Epoch 0, Step 7025, Train Loss: 0.9925, Val Loss: 0.9619\n",
      "Epoch 0, Step 7050, Train Loss: 1.0063, Val Loss: 0.9581\n",
      "Epoch 0, Step 7075, Train Loss: 0.9616, Val Loss: 0.9547\n",
      "Epoch 0, Step 7100, Train Loss: 1.0141, Val Loss: 0.9492\n",
      "Epoch 0, Step 7125, Train Loss: 1.0619, Val Loss: 0.9486\n",
      "Epoch 0, Step 7150, Train Loss: 0.9975, Val Loss: 0.9442\n",
      "Epoch 0, Step 7175, Train Loss: 1.0198, Val Loss: 0.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|███▊      | 10774/27842 [1:22:04<2:12:40,  2.14it/s, train_loss=0.9466, val_loss=0.8995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 7200, Train Loss: 0.9577, Val Loss: 0.9488\n",
      "Epoch 0, Step 7225, Train Loss: 0.9817, Val Loss: 0.9461\n",
      "Epoch 0, Step 7250, Train Loss: 1.0972, Val Loss: 0.9374\n",
      "Epoch 0, Step 7275, Train Loss: 0.9198, Val Loss: 0.9473\n",
      "Epoch 0, Step 7300, Train Loss: 1.0617, Val Loss: 0.9472\n",
      "Epoch 0, Step 7325, Train Loss: 0.9895, Val Loss: 0.9518\n",
      "Epoch 0, Step 7350, Train Loss: 1.0224, Val Loss: 0.9453\n",
      "Epoch 0, Step 7375, Train Loss: 0.9510, Val Loss: 0.9521\n",
      "Epoch 0, Step 7400, Train Loss: 0.9722, Val Loss: 0.9525\n",
      "Epoch 0, Step 7425, Train Loss: 1.1091, Val Loss: 0.9539\n",
      "Epoch 0, Step 7450, Train Loss: 1.0603, Val Loss: 0.9478\n",
      "Epoch 0, Step 7475, Train Loss: 0.9920, Val Loss: 0.9437\n",
      "Epoch 0, Step 7500, Train Loss: 1.0952, Val Loss: 0.9452\n",
      "Epoch 0, Step 7525, Train Loss: 0.9795, Val Loss: 0.9423\n",
      "Epoch 0, Step 7550, Train Loss: 0.9618, Val Loss: 0.9523\n",
      "Epoch 0, Step 7575, Train Loss: 0.9440, Val Loss: 0.9488\n",
      "Epoch 0, Step 7600, Train Loss: 0.9483, Val Loss: 0.9434\n",
      "Epoch 0, Step 7625, Train Loss: 0.8643, Val Loss: 0.9437\n",
      "Epoch 0, Step 7650, Train Loss: 0.9762, Val Loss: 0.9383\n",
      "Epoch 0, Step 7675, Train Loss: 1.0629, Val Loss: 0.9426\n",
      "Epoch 0, Step 7700, Train Loss: 0.9672, Val Loss: 0.9454\n",
      "Epoch 0, Step 7725, Train Loss: 0.9960, Val Loss: 0.9431\n",
      "Epoch 0, Step 7750, Train Loss: 1.0162, Val Loss: 0.9545\n",
      "Epoch 0, Step 7775, Train Loss: 0.9944, Val Loss: 0.9458\n",
      "Epoch 0, Step 7800, Train Loss: 1.0150, Val Loss: 0.9537\n",
      "Epoch 0, Step 7825, Train Loss: 1.1019, Val Loss: 0.9487\n",
      "Epoch 0, Step 7850, Train Loss: 1.0175, Val Loss: 0.9494\n",
      "Epoch 0, Step 7875, Train Loss: 1.0122, Val Loss: 0.9418\n",
      "Epoch 0, Step 7900, Train Loss: 0.9642, Val Loss: 0.9419\n",
      "Epoch 0, Step 7925, Train Loss: 0.9750, Val Loss: 0.9519\n",
      "Epoch 0, Step 7950, Train Loss: 0.9496, Val Loss: 0.9551\n",
      "Epoch 0, Step 7975, Train Loss: 1.0141, Val Loss: 0.9490\n",
      "Epoch 0, Step 8000, Train Loss: 1.0103, Val Loss: 0.9543\n",
      "Epoch 0, Step 8025, Train Loss: 0.8830, Val Loss: 0.9419\n",
      "Epoch 0, Step 8050, Train Loss: 1.0339, Val Loss: 0.9387\n",
      "Epoch 0, Step 8075, Train Loss: 1.0188, Val Loss: 0.9439\n",
      "Epoch 0, Step 8100, Train Loss: 0.9640, Val Loss: 0.9479\n",
      "Epoch 0, Step 8125, Train Loss: 0.9360, Val Loss: 0.9488\n",
      "Epoch 0, Step 8150, Train Loss: 0.9306, Val Loss: 0.9556\n",
      "Epoch 0, Step 8175, Train Loss: 0.9909, Val Loss: 0.9517\n",
      "Epoch 0, Step 8200, Train Loss: 0.9860, Val Loss: 0.9515\n",
      "Epoch 0, Step 8225, Train Loss: 0.8982, Val Loss: 0.9443\n",
      "Epoch 0, Step 8250, Train Loss: 0.9899, Val Loss: 0.9461\n",
      "Epoch 0, Step 8275, Train Loss: 0.9075, Val Loss: 0.9548\n",
      "Epoch 0, Step 8300, Train Loss: 0.8176, Val Loss: 0.9491\n",
      "Epoch 0, Step 8325, Train Loss: 0.9896, Val Loss: 0.9544\n",
      "Epoch 0, Step 8350, Train Loss: 0.8679, Val Loss: 0.9409\n",
      "Epoch 0, Step 8375, Train Loss: 0.9179, Val Loss: 0.9491\n",
      "Epoch 0, Step 8400, Train Loss: 1.0911, Val Loss: 0.9511\n",
      "Epoch 0, Step 8425, Train Loss: 1.0201, Val Loss: 0.9533\n",
      "Epoch 0, Step 8450, Train Loss: 0.9532, Val Loss: 0.9596\n",
      "Epoch 0, Step 8475, Train Loss: 0.9798, Val Loss: 0.9471\n",
      "Epoch 0, Step 8500, Train Loss: 0.9027, Val Loss: 0.9501\n",
      "Epoch 0, Step 8525, Train Loss: 0.9709, Val Loss: 0.9480\n",
      "Epoch 0, Step 8550, Train Loss: 0.9740, Val Loss: 0.9509\n",
      "Epoch 0, Step 8575, Train Loss: 0.8764, Val Loss: 0.9506\n",
      "Epoch 0, Step 8600, Train Loss: 1.0191, Val Loss: 0.9445\n",
      "Epoch 0, Step 8625, Train Loss: 1.0032, Val Loss: 0.9405\n",
      "Epoch 0, Step 8650, Train Loss: 0.9086, Val Loss: 0.9445\n",
      "Epoch 0, Step 8675, Train Loss: 0.9147, Val Loss: 0.9501\n",
      "Epoch 0, Step 8700, Train Loss: 1.1169, Val Loss: 0.9487\n",
      "Epoch 0, Step 8725, Train Loss: 0.8944, Val Loss: 0.9419\n",
      "Epoch 0, Step 8750, Train Loss: 0.9345, Val Loss: 0.9377\n",
      "Epoch 0, Step 8775, Train Loss: 1.0174, Val Loss: 0.9336\n",
      "Epoch 0, Step 8800, Train Loss: 0.9925, Val Loss: 0.9506\n",
      "Epoch 0, Step 8825, Train Loss: 0.9602, Val Loss: 0.9400\n",
      "Epoch 0, Step 8850, Train Loss: 0.9286, Val Loss: 0.9424\n",
      "Epoch 0, Step 8875, Train Loss: 0.8692, Val Loss: 0.9409\n",
      "Epoch 0, Step 8900, Train Loss: 0.9385, Val Loss: 0.9383\n",
      "Epoch 0, Step 8925, Train Loss: 0.9460, Val Loss: 0.9337\n",
      "Epoch 0, Step 8950, Train Loss: 0.9440, Val Loss: 0.9398\n",
      "Epoch 0, Step 8975, Train Loss: 0.9641, Val Loss: 0.9378\n",
      "Epoch 0, Step 9000, Train Loss: 0.8755, Val Loss: 0.9414\n",
      "Epoch 0, Step 9025, Train Loss: 1.0275, Val Loss: 0.9482\n",
      "Epoch 0, Step 9050, Train Loss: 1.0144, Val Loss: 0.9397\n",
      "Epoch 0, Step 9075, Train Loss: 0.8675, Val Loss: 0.9434\n",
      "Epoch 0, Step 9100, Train Loss: 1.0527, Val Loss: 0.9456\n",
      "Epoch 0, Step 9125, Train Loss: 0.9434, Val Loss: 0.9448\n",
      "Epoch 0, Step 9150, Train Loss: 1.0153, Val Loss: 0.9461\n",
      "Epoch 0, Step 9175, Train Loss: 0.8918, Val Loss: 0.9391\n",
      "Epoch 0, Step 9200, Train Loss: 0.9665, Val Loss: 0.9392\n",
      "Epoch 0, Step 9225, Train Loss: 0.8971, Val Loss: 0.9413\n",
      "Epoch 0, Step 9250, Train Loss: 1.0264, Val Loss: 0.9410\n",
      "Epoch 0, Step 9275, Train Loss: 0.8992, Val Loss: 0.9442\n",
      "Epoch 0, Step 9300, Train Loss: 0.9011, Val Loss: 0.9337\n",
      "Epoch 0, Step 9325, Train Loss: 0.9468, Val Loss: 0.9395\n",
      "Epoch 0, Step 9350, Train Loss: 0.8781, Val Loss: 0.9321\n",
      "Epoch 0, Step 9375, Train Loss: 1.0609, Val Loss: 0.9267\n",
      "Epoch 0, Step 9400, Train Loss: 0.9885, Val Loss: 0.9295\n",
      "Epoch 0, Step 9425, Train Loss: 0.9043, Val Loss: 0.9369\n",
      "Epoch 0, Step 9450, Train Loss: 1.0094, Val Loss: 0.9295\n",
      "Epoch 0, Step 9475, Train Loss: 0.9017, Val Loss: 0.9311\n",
      "Epoch 0, Step 9500, Train Loss: 1.0361, Val Loss: 0.9325\n",
      "Epoch 0, Step 9525, Train Loss: 0.8983, Val Loss: 0.9242\n",
      "Epoch 0, Step 9550, Train Loss: 0.9499, Val Loss: 0.9315\n",
      "Epoch 0, Step 9575, Train Loss: 0.9271, Val Loss: 0.9275\n",
      "Epoch 0, Step 9600, Train Loss: 0.9251, Val Loss: 0.9277\n",
      "Epoch 0, Step 9625, Train Loss: 0.9240, Val Loss: 0.9225\n",
      "Epoch 0, Step 9650, Train Loss: 0.9513, Val Loss: 0.9216\n",
      "Epoch 0, Step 9675, Train Loss: 0.8370, Val Loss: 0.9280\n",
      "Epoch 0, Step 9700, Train Loss: 0.9141, Val Loss: 0.9269\n",
      "Epoch 0, Step 9725, Train Loss: 0.9961, Val Loss: 0.9199\n",
      "Epoch 0, Step 9750, Train Loss: 0.9894, Val Loss: 0.9205\n",
      "Epoch 0, Step 9775, Train Loss: 0.8391, Val Loss: 0.9300\n",
      "Epoch 0, Step 9800, Train Loss: 1.0098, Val Loss: 0.9258\n",
      "Epoch 0, Step 9825, Train Loss: 0.9326, Val Loss: 0.9246\n",
      "Epoch 0, Step 9850, Train Loss: 0.8406, Val Loss: 0.9273\n",
      "Epoch 0, Step 9875, Train Loss: 0.8100, Val Loss: 0.9239\n",
      "Epoch 0, Step 9900, Train Loss: 0.8760, Val Loss: 0.9229\n",
      "Epoch 0, Step 9925, Train Loss: 0.9140, Val Loss: 0.9330\n",
      "Epoch 0, Step 9950, Train Loss: 0.9534, Val Loss: 0.9202\n",
      "Epoch 0, Step 9975, Train Loss: 0.9808, Val Loss: 0.9154\n",
      "Epoch 0, Step 10000, Train Loss: 1.0294, Val Loss: 0.9223\n",
      "Epoch 0, Step 10025, Train Loss: 0.9481, Val Loss: 0.9125\n",
      "Epoch 0, Step 10050, Train Loss: 0.9612, Val Loss: 0.9140\n",
      "Epoch 0, Step 10075, Train Loss: 0.9163, Val Loss: 0.9153\n",
      "Epoch 0, Step 10100, Train Loss: 0.9340, Val Loss: 0.9145\n",
      "Epoch 0, Step 10125, Train Loss: 1.0326, Val Loss: 0.9168\n",
      "Epoch 0, Step 10150, Train Loss: 0.9080, Val Loss: 0.9105\n",
      "Epoch 0, Step 10175, Train Loss: 0.8836, Val Loss: 0.9066\n",
      "Epoch 0, Step 10200, Train Loss: 0.9390, Val Loss: 0.9115\n",
      "Epoch 0, Step 10225, Train Loss: 0.9739, Val Loss: 0.9206\n",
      "Epoch 0, Step 10250, Train Loss: 0.9754, Val Loss: 0.9078\n",
      "Epoch 0, Step 10275, Train Loss: 0.9274, Val Loss: 0.9072\n",
      "Epoch 0, Step 10300, Train Loss: 0.9316, Val Loss: 0.9148\n",
      "Epoch 0, Step 10325, Train Loss: 0.9950, Val Loss: 0.9184\n",
      "Epoch 0, Step 10350, Train Loss: 0.9046, Val Loss: 0.9087\n",
      "Epoch 0, Step 10375, Train Loss: 0.8061, Val Loss: 0.9088\n",
      "Epoch 0, Step 10400, Train Loss: 0.9913, Val Loss: 0.9135\n",
      "Epoch 0, Step 10425, Train Loss: 0.9539, Val Loss: 0.9121\n",
      "Epoch 0, Step 10450, Train Loss: 0.9437, Val Loss: 0.8930\n",
      "Epoch 0, Step 10475, Train Loss: 1.0535, Val Loss: 0.8916\n",
      "Epoch 0, Step 10500, Train Loss: 0.8937, Val Loss: 0.8964\n",
      "Epoch 0, Step 10525, Train Loss: 0.9488, Val Loss: 0.8883\n",
      "Epoch 0, Step 10550, Train Loss: 0.9076, Val Loss: 0.8862\n",
      "Epoch 0, Step 10575, Train Loss: 0.9396, Val Loss: 0.8939\n",
      "Epoch 0, Step 10600, Train Loss: 0.9413, Val Loss: 0.8935\n",
      "Epoch 0, Step 10625, Train Loss: 0.9053, Val Loss: 0.8966\n",
      "Epoch 0, Step 10650, Train Loss: 0.9971, Val Loss: 0.8883\n",
      "Epoch 0, Step 10675, Train Loss: 0.9004, Val Loss: 0.8939\n",
      "Epoch 0, Step 10700, Train Loss: 0.9496, Val Loss: 0.8958\n",
      "Epoch 0, Step 10725, Train Loss: 0.8747, Val Loss: 0.8998\n",
      "Epoch 0, Step 10750, Train Loss: 0.8928, Val Loss: 0.9014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  51%|█████▏    | 14299/27842 [1:49:43<2:18:49,  1.63it/s, train_loss=0.8766, val_loss=0.8369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 10775, Train Loss: 0.9466, Val Loss: 0.8995\n",
      "Epoch 0, Step 10800, Train Loss: 1.0129, Val Loss: 0.8968\n",
      "Epoch 0, Step 10825, Train Loss: 0.9340, Val Loss: 0.8931\n",
      "Epoch 0, Step 10850, Train Loss: 0.9965, Val Loss: 0.9098\n",
      "Epoch 0, Step 10875, Train Loss: 0.9644, Val Loss: 0.9002\n",
      "Epoch 0, Step 10900, Train Loss: 0.9661, Val Loss: 0.9007\n",
      "Epoch 0, Step 10925, Train Loss: 0.9040, Val Loss: 0.8988\n",
      "Epoch 0, Step 10950, Train Loss: 0.9434, Val Loss: 0.9056\n",
      "Epoch 0, Step 10975, Train Loss: 0.8894, Val Loss: 0.9022\n",
      "Epoch 0, Step 11000, Train Loss: 0.8916, Val Loss: 0.8885\n",
      "Epoch 0, Step 11025, Train Loss: 0.9257, Val Loss: 0.8885\n",
      "Epoch 0, Step 11050, Train Loss: 0.9077, Val Loss: 0.8867\n",
      "Epoch 0, Step 11075, Train Loss: 0.9067, Val Loss: 0.8961\n",
      "Epoch 0, Step 11100, Train Loss: 0.8510, Val Loss: 0.8768\n",
      "Epoch 0, Step 11125, Train Loss: 0.7681, Val Loss: 0.8921\n",
      "Epoch 0, Step 11150, Train Loss: 1.0087, Val Loss: 0.8898\n",
      "Epoch 0, Step 11175, Train Loss: 0.8519, Val Loss: 0.8927\n",
      "Epoch 0, Step 11200, Train Loss: 0.9373, Val Loss: 0.8949\n",
      "Epoch 0, Step 11225, Train Loss: 0.9855, Val Loss: 0.8956\n",
      "Epoch 0, Step 11250, Train Loss: 0.8494, Val Loss: 0.8939\n",
      "Epoch 0, Step 11275, Train Loss: 0.9565, Val Loss: 0.8908\n",
      "Epoch 0, Step 11300, Train Loss: 0.9550, Val Loss: 0.8903\n",
      "Epoch 0, Step 11325, Train Loss: 0.8255, Val Loss: 0.8841\n",
      "Epoch 0, Step 11350, Train Loss: 0.9171, Val Loss: 0.8876\n",
      "Epoch 0, Step 11375, Train Loss: 0.8211, Val Loss: 0.8932\n",
      "Epoch 0, Step 11400, Train Loss: 0.9364, Val Loss: 0.9017\n",
      "Epoch 0, Step 11425, Train Loss: 0.8987, Val Loss: 0.8928\n",
      "Epoch 0, Step 11450, Train Loss: 0.8515, Val Loss: 0.8980\n",
      "Epoch 0, Step 11475, Train Loss: 0.9041, Val Loss: 0.9042\n",
      "Epoch 0, Step 11500, Train Loss: 0.9362, Val Loss: 0.8970\n",
      "Epoch 0, Step 11525, Train Loss: 0.8002, Val Loss: 0.8924\n",
      "Epoch 0, Step 11550, Train Loss: 0.9784, Val Loss: 0.8966\n",
      "Epoch 0, Step 11575, Train Loss: 0.8558, Val Loss: 0.9051\n",
      "Epoch 0, Step 11600, Train Loss: 0.9635, Val Loss: 0.8943\n",
      "Epoch 0, Step 11625, Train Loss: 0.8670, Val Loss: 0.8948\n",
      "Epoch 0, Step 11650, Train Loss: 0.9035, Val Loss: 0.8970\n",
      "Epoch 0, Step 11675, Train Loss: 0.8641, Val Loss: 0.9161\n",
      "Epoch 0, Step 11700, Train Loss: 0.9162, Val Loss: 0.9065\n",
      "Epoch 0, Step 11725, Train Loss: 0.9333, Val Loss: 0.9108\n",
      "Epoch 0, Step 11750, Train Loss: 0.8956, Val Loss: 0.9032\n",
      "Epoch 0, Step 11775, Train Loss: 0.8745, Val Loss: 0.8982\n",
      "Epoch 0, Step 11800, Train Loss: 0.9016, Val Loss: 0.9024\n",
      "Epoch 0, Step 11825, Train Loss: 0.9351, Val Loss: 0.9004\n",
      "Epoch 0, Step 11850, Train Loss: 0.9589, Val Loss: 0.8985\n",
      "Epoch 0, Step 11875, Train Loss: 0.9423, Val Loss: 0.8833\n",
      "Epoch 0, Step 11900, Train Loss: 0.8581, Val Loss: 0.8847\n",
      "Epoch 0, Step 11925, Train Loss: 0.8378, Val Loss: 0.8925\n",
      "Epoch 0, Step 11950, Train Loss: 0.8636, Val Loss: 0.8930\n",
      "Epoch 0, Step 11975, Train Loss: 0.9130, Val Loss: 0.8819\n",
      "Epoch 0, Step 12000, Train Loss: 0.8803, Val Loss: 0.8938\n",
      "Epoch 0, Step 12025, Train Loss: 0.8876, Val Loss: 0.8885\n",
      "Epoch 0, Step 12050, Train Loss: 0.9539, Val Loss: 0.8873\n",
      "Epoch 0, Step 12075, Train Loss: 0.8321, Val Loss: 0.8933\n",
      "Epoch 0, Step 12100, Train Loss: 0.9492, Val Loss: 0.8976\n",
      "Epoch 0, Step 12125, Train Loss: 0.8178, Val Loss: 0.9027\n",
      "Epoch 0, Step 12150, Train Loss: 0.9233, Val Loss: 0.9037\n",
      "Epoch 0, Step 12175, Train Loss: 0.8650, Val Loss: 0.8967\n",
      "Epoch 0, Step 12200, Train Loss: 0.9327, Val Loss: 0.8949\n",
      "Epoch 0, Step 12225, Train Loss: 0.9533, Val Loss: 0.8874\n",
      "Epoch 0, Step 12250, Train Loss: 0.8046, Val Loss: 0.8837\n",
      "Epoch 0, Step 12275, Train Loss: 0.9524, Val Loss: 0.8814\n",
      "Epoch 0, Step 12300, Train Loss: 0.8296, Val Loss: 0.8850\n",
      "Epoch 0, Step 12325, Train Loss: 0.8438, Val Loss: 0.8921\n",
      "Epoch 0, Step 12350, Train Loss: 0.8347, Val Loss: 0.8501\n",
      "Epoch 0, Step 12375, Train Loss: 0.9556, Val Loss: 0.8550\n",
      "Epoch 0, Step 12400, Train Loss: 0.9876, Val Loss: 0.8645\n",
      "Epoch 0, Step 12425, Train Loss: 0.9254, Val Loss: 0.8649\n",
      "Epoch 0, Step 12450, Train Loss: 0.8862, Val Loss: 0.8655\n",
      "Epoch 0, Step 12475, Train Loss: 0.8843, Val Loss: 0.8699\n",
      "Epoch 0, Step 12500, Train Loss: 0.8615, Val Loss: 0.8684\n",
      "Epoch 0, Step 12525, Train Loss: 0.8421, Val Loss: 0.8633\n",
      "Epoch 0, Step 12550, Train Loss: 0.7640, Val Loss: 0.8655\n",
      "Epoch 0, Step 12575, Train Loss: 0.9233, Val Loss: 0.8646\n",
      "Epoch 0, Step 12600, Train Loss: 0.8242, Val Loss: 0.8513\n",
      "Epoch 0, Step 12625, Train Loss: 0.8899, Val Loss: 0.8485\n",
      "Epoch 0, Step 12650, Train Loss: 0.9112, Val Loss: 0.8586\n",
      "Epoch 0, Step 12675, Train Loss: 0.8403, Val Loss: 0.8604\n",
      "Epoch 0, Step 12700, Train Loss: 0.8211, Val Loss: 0.8570\n",
      "Epoch 0, Step 12725, Train Loss: 0.9390, Val Loss: 0.8574\n",
      "Epoch 0, Step 12750, Train Loss: 0.8929, Val Loss: 0.8611\n",
      "Epoch 0, Step 12775, Train Loss: 0.9594, Val Loss: 0.8623\n",
      "Epoch 0, Step 12800, Train Loss: 0.8337, Val Loss: 0.8519\n",
      "Epoch 0, Step 12825, Train Loss: 0.8521, Val Loss: 0.8670\n",
      "Epoch 0, Step 12850, Train Loss: 0.9129, Val Loss: 0.8592\n",
      "Epoch 0, Step 12875, Train Loss: 0.8957, Val Loss: 0.8615\n",
      "Epoch 0, Step 12900, Train Loss: 0.8135, Val Loss: 0.8575\n",
      "Epoch 0, Step 12925, Train Loss: 0.8606, Val Loss: 0.8600\n",
      "Epoch 0, Step 12950, Train Loss: 0.9621, Val Loss: 0.8639\n",
      "Epoch 0, Step 12975, Train Loss: 0.7608, Val Loss: 0.8769\n",
      "Epoch 0, Step 13000, Train Loss: 0.8288, Val Loss: 0.8599\n",
      "Epoch 0, Step 13025, Train Loss: 0.8178, Val Loss: 0.8602\n",
      "Epoch 0, Step 13050, Train Loss: 0.9578, Val Loss: 0.8576\n",
      "Epoch 0, Step 13075, Train Loss: 0.9403, Val Loss: 0.8476\n",
      "Epoch 0, Step 13100, Train Loss: 0.8594, Val Loss: 0.8484\n",
      "Epoch 0, Step 13125, Train Loss: 0.9085, Val Loss: 0.8478\n",
      "Epoch 0, Step 13150, Train Loss: 0.8385, Val Loss: 0.8404\n",
      "Epoch 0, Step 13175, Train Loss: 0.8169, Val Loss: 0.8461\n",
      "Epoch 0, Step 13200, Train Loss: 0.9280, Val Loss: 0.8599\n",
      "Epoch 0, Step 13225, Train Loss: 0.8593, Val Loss: 0.8587\n",
      "Epoch 0, Step 13250, Train Loss: 0.8276, Val Loss: 0.8591\n",
      "Epoch 0, Step 13275, Train Loss: 0.9450, Val Loss: 0.8539\n",
      "Epoch 0, Step 13300, Train Loss: 0.9392, Val Loss: 0.8410\n",
      "Epoch 0, Step 13325, Train Loss: 0.9050, Val Loss: 0.8279\n",
      "Epoch 0, Step 13350, Train Loss: 0.9942, Val Loss: 0.8202\n",
      "Epoch 0, Step 13375, Train Loss: 0.9186, Val Loss: 0.8240\n",
      "Epoch 0, Step 13400, Train Loss: 0.8449, Val Loss: 0.8263\n",
      "Epoch 0, Step 13425, Train Loss: 0.8703, Val Loss: 0.8311\n",
      "Epoch 0, Step 13450, Train Loss: 0.9051, Val Loss: 0.8215\n",
      "Epoch 0, Step 13475, Train Loss: 0.9095, Val Loss: 0.8228\n",
      "Epoch 0, Step 13500, Train Loss: 0.8922, Val Loss: 0.8325\n",
      "Epoch 0, Step 13525, Train Loss: 0.9125, Val Loss: 0.8211\n",
      "Epoch 0, Step 13550, Train Loss: 0.9339, Val Loss: 0.8204\n",
      "Epoch 0, Step 13575, Train Loss: 0.9813, Val Loss: 0.8236\n",
      "Epoch 0, Step 13600, Train Loss: 0.8834, Val Loss: 0.8291\n",
      "Epoch 0, Step 13625, Train Loss: 0.9658, Val Loss: 0.8303\n",
      "Epoch 0, Step 13650, Train Loss: 0.8105, Val Loss: 0.8231\n",
      "Epoch 0, Step 13675, Train Loss: 0.8264, Val Loss: 0.8253\n",
      "Epoch 0, Step 13700, Train Loss: 0.8045, Val Loss: 0.8312\n",
      "Epoch 0, Step 13725, Train Loss: 0.8717, Val Loss: 0.8299\n",
      "Epoch 0, Step 13750, Train Loss: 0.8792, Val Loss: 0.8305\n",
      "Epoch 0, Step 13775, Train Loss: 0.8986, Val Loss: 0.8307\n",
      "Epoch 0, Step 13800, Train Loss: 0.8817, Val Loss: 0.8377\n",
      "Epoch 0, Step 13825, Train Loss: 0.8141, Val Loss: 0.8397\n",
      "Epoch 0, Step 13850, Train Loss: 0.7997, Val Loss: 0.8372\n",
      "Epoch 0, Step 13875, Train Loss: 0.9466, Val Loss: 0.8365\n",
      "Epoch 0, Step 13900, Train Loss: 0.8632, Val Loss: 0.8368\n",
      "Epoch 0, Step 13925, Train Loss: 0.8994, Val Loss: 0.8355\n",
      "Epoch 0, Step 13950, Train Loss: 0.8731, Val Loss: 0.8285\n",
      "Epoch 0, Step 13975, Train Loss: 0.8344, Val Loss: 0.8374\n",
      "Epoch 0, Step 14000, Train Loss: 0.9412, Val Loss: 0.8321\n",
      "Epoch 0, Step 14025, Train Loss: 0.7914, Val Loss: 0.8357\n",
      "Epoch 0, Step 14050, Train Loss: 0.7577, Val Loss: 0.8320\n",
      "Epoch 0, Step 14075, Train Loss: 0.8407, Val Loss: 0.8316\n",
      "Epoch 0, Step 14100, Train Loss: 0.8710, Val Loss: 0.8366\n",
      "Epoch 0, Step 14125, Train Loss: 0.8819, Val Loss: 0.8324\n",
      "Epoch 0, Step 14150, Train Loss: 0.9092, Val Loss: 0.8347\n",
      "Epoch 0, Step 14175, Train Loss: 0.9299, Val Loss: 0.8229\n",
      "Epoch 0, Step 14200, Train Loss: 0.9121, Val Loss: 0.8329\n",
      "Epoch 0, Step 14225, Train Loss: 0.8987, Val Loss: 0.8361\n",
      "Epoch 0, Step 14250, Train Loss: 0.9351, Val Loss: 0.8237\n",
      "Epoch 0, Step 14275, Train Loss: 0.8920, Val Loss: 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  64%|██████▍   | 17824/27842 [2:18:39<1:28:59,  1.88it/s, train_loss=0.8531, val_loss=0.7613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 14300, Train Loss: 0.8766, Val Loss: 0.8369\n",
      "Epoch 0, Step 14325, Train Loss: 0.8469, Val Loss: 0.8353\n",
      "Epoch 0, Step 14350, Train Loss: 0.8590, Val Loss: 0.8411\n",
      "Epoch 0, Step 14375, Train Loss: 0.8638, Val Loss: 0.8353\n",
      "Epoch 0, Step 14400, Train Loss: 0.8884, Val Loss: 0.8424\n",
      "Epoch 0, Step 14425, Train Loss: 0.9077, Val Loss: 0.8282\n",
      "Epoch 0, Step 14450, Train Loss: 0.8643, Val Loss: 0.8270\n",
      "Epoch 0, Step 14475, Train Loss: 0.9435, Val Loss: 0.8158\n",
      "Epoch 0, Step 14500, Train Loss: 0.7693, Val Loss: 0.8120\n",
      "Epoch 0, Step 14525, Train Loss: 0.8013, Val Loss: 0.8208\n",
      "Epoch 0, Step 14550, Train Loss: 0.9002, Val Loss: 0.8219\n",
      "Epoch 0, Step 14575, Train Loss: 0.7981, Val Loss: 0.8187\n",
      "Epoch 0, Step 14600, Train Loss: 0.8104, Val Loss: 0.7799\n",
      "Epoch 0, Step 14625, Train Loss: 0.7898, Val Loss: 0.7918\n",
      "Epoch 0, Step 14650, Train Loss: 0.8707, Val Loss: 0.7974\n",
      "Epoch 0, Step 14675, Train Loss: 0.8365, Val Loss: 0.7850\n",
      "Epoch 0, Step 14700, Train Loss: 0.7656, Val Loss: 0.7937\n",
      "Epoch 0, Step 14725, Train Loss: 0.6797, Val Loss: 0.7922\n",
      "Epoch 0, Step 14750, Train Loss: 0.7629, Val Loss: 0.8027\n",
      "Epoch 0, Step 14775, Train Loss: 0.8045, Val Loss: 0.7993\n",
      "Epoch 0, Step 14800, Train Loss: 0.9120, Val Loss: 0.8035\n",
      "Epoch 0, Step 14825, Train Loss: 0.8708, Val Loss: 0.8129\n",
      "Epoch 0, Step 14850, Train Loss: 0.7723, Val Loss: 0.8053\n",
      "Epoch 0, Step 14875, Train Loss: 0.8744, Val Loss: 0.8073\n",
      "Epoch 0, Step 14900, Train Loss: 0.8553, Val Loss: 0.7973\n",
      "Epoch 0, Step 14925, Train Loss: 0.8930, Val Loss: 0.8107\n",
      "Epoch 0, Step 14950, Train Loss: 0.8281, Val Loss: 0.8154\n",
      "Epoch 0, Step 14975, Train Loss: 0.8003, Val Loss: 0.8202\n",
      "Epoch 0, Step 15000, Train Loss: 0.9291, Val Loss: 0.8089\n",
      "Epoch 0, Step 15025, Train Loss: 0.8346, Val Loss: 0.8089\n",
      "Epoch 0, Step 15050, Train Loss: 0.8564, Val Loss: 0.8005\n",
      "Epoch 0, Step 15075, Train Loss: 0.8532, Val Loss: 0.8036\n",
      "Epoch 0, Step 15100, Train Loss: 0.7527, Val Loss: 0.8033\n",
      "Epoch 0, Step 15125, Train Loss: 0.7068, Val Loss: 0.8055\n",
      "Epoch 0, Step 15150, Train Loss: 0.7643, Val Loss: 0.8051\n",
      "Epoch 0, Step 15175, Train Loss: 0.8301, Val Loss: 0.8067\n",
      "Epoch 0, Step 15200, Train Loss: 0.7974, Val Loss: 0.8051\n",
      "Epoch 0, Step 15225, Train Loss: 0.8589, Val Loss: 0.8092\n",
      "Epoch 0, Step 15250, Train Loss: 0.7565, Val Loss: 0.8100\n",
      "Epoch 0, Step 15275, Train Loss: 0.8483, Val Loss: 0.8100\n",
      "Epoch 0, Step 15300, Train Loss: 0.8109, Val Loss: 0.7938\n",
      "Epoch 0, Step 15325, Train Loss: 0.8160, Val Loss: 0.8071\n",
      "Epoch 0, Step 15350, Train Loss: 0.8968, Val Loss: 0.8105\n",
      "Epoch 0, Step 15375, Train Loss: 0.8140, Val Loss: 0.8091\n",
      "Epoch 0, Step 15400, Train Loss: 0.8438, Val Loss: 0.8002\n",
      "Epoch 0, Step 15425, Train Loss: 0.8311, Val Loss: 0.8059\n",
      "Epoch 0, Step 15450, Train Loss: 0.8216, Val Loss: 0.8188\n",
      "Epoch 0, Step 15475, Train Loss: 0.8238, Val Loss: 0.8104\n",
      "Epoch 0, Step 15500, Train Loss: 0.7647, Val Loss: 0.8177\n",
      "Epoch 0, Step 15525, Train Loss: 0.8428, Val Loss: 0.8242\n",
      "Epoch 0, Step 15550, Train Loss: 0.7663, Val Loss: 0.8152\n",
      "Epoch 0, Step 15575, Train Loss: 0.8886, Val Loss: 0.8151\n",
      "Epoch 0, Step 15600, Train Loss: 0.8020, Val Loss: 0.8075\n",
      "Epoch 0, Step 15625, Train Loss: 0.8446, Val Loss: 0.8163\n",
      "Epoch 0, Step 15650, Train Loss: 0.7410, Val Loss: 0.8051\n",
      "Epoch 0, Step 15675, Train Loss: 0.7259, Val Loss: 0.8098\n",
      "Epoch 0, Step 15700, Train Loss: 0.8354, Val Loss: 0.8045\n",
      "Epoch 0, Step 15725, Train Loss: 0.8226, Val Loss: 0.7986\n",
      "Epoch 0, Step 15750, Train Loss: 0.8185, Val Loss: 0.7890\n",
      "Epoch 0, Step 15775, Train Loss: 0.8690, Val Loss: 0.7917\n",
      "Epoch 0, Step 15800, Train Loss: 0.8748, Val Loss: 0.7934\n",
      "Epoch 0, Step 15825, Train Loss: 0.8193, Val Loss: 0.7858\n",
      "Epoch 0, Step 15850, Train Loss: 0.8154, Val Loss: 0.7600\n",
      "Epoch 0, Step 15875, Train Loss: 0.7703, Val Loss: 0.7641\n",
      "Epoch 0, Step 15900, Train Loss: 0.8959, Val Loss: 0.7591\n",
      "Epoch 0, Step 15925, Train Loss: 0.8778, Val Loss: 0.7661\n",
      "Epoch 0, Step 15950, Train Loss: 0.8083, Val Loss: 0.7740\n",
      "Epoch 0, Step 15975, Train Loss: 0.8547, Val Loss: 0.7658\n",
      "Epoch 0, Step 16000, Train Loss: 0.7271, Val Loss: 0.7696\n",
      "Epoch 0, Step 16025, Train Loss: 0.6688, Val Loss: 0.7630\n",
      "Epoch 0, Step 16050, Train Loss: 0.7825, Val Loss: 0.7719\n",
      "Epoch 0, Step 16075, Train Loss: 0.8418, Val Loss: 0.7784\n",
      "Epoch 0, Step 16100, Train Loss: 0.8294, Val Loss: 0.7729\n",
      "Epoch 0, Step 16125, Train Loss: 0.9008, Val Loss: 0.7676\n",
      "Epoch 0, Step 16150, Train Loss: 0.7468, Val Loss: 0.7671\n",
      "Epoch 0, Step 16175, Train Loss: 0.9035, Val Loss: 0.7620\n",
      "Epoch 0, Step 16200, Train Loss: 0.9515, Val Loss: 0.7608\n",
      "Epoch 0, Step 16225, Train Loss: 0.7014, Val Loss: 0.7614\n",
      "Epoch 0, Step 16250, Train Loss: 0.8816, Val Loss: 0.7733\n",
      "Epoch 0, Step 16275, Train Loss: 0.7609, Val Loss: 0.7600\n",
      "Epoch 0, Step 16300, Train Loss: 0.7996, Val Loss: 0.7685\n",
      "Epoch 0, Step 16325, Train Loss: 0.8293, Val Loss: 0.7552\n",
      "Epoch 0, Step 16350, Train Loss: 0.8794, Val Loss: 0.7635\n",
      "Epoch 0, Step 16375, Train Loss: 0.8216, Val Loss: 0.7685\n",
      "Epoch 0, Step 16400, Train Loss: 0.7236, Val Loss: 0.7668\n",
      "Epoch 0, Step 16425, Train Loss: 0.8970, Val Loss: 0.7623\n",
      "Epoch 0, Step 16450, Train Loss: 0.8210, Val Loss: 0.7726\n",
      "Epoch 0, Step 16475, Train Loss: 0.8950, Val Loss: 0.7707\n",
      "Epoch 0, Step 16500, Train Loss: 0.7555, Val Loss: 0.7541\n",
      "Epoch 0, Step 16525, Train Loss: 0.8498, Val Loss: 0.7661\n",
      "Epoch 0, Step 16550, Train Loss: 0.8105, Val Loss: 0.7631\n",
      "Epoch 0, Step 16575, Train Loss: 0.7130, Val Loss: 0.7649\n",
      "Epoch 0, Step 16600, Train Loss: 0.7434, Val Loss: 0.7691\n",
      "Epoch 0, Step 16625, Train Loss: 0.8115, Val Loss: 0.7674\n",
      "Epoch 0, Step 16650, Train Loss: 0.8194, Val Loss: 0.7632\n",
      "Epoch 0, Step 16675, Train Loss: 0.8332, Val Loss: 0.7749\n",
      "Epoch 0, Step 16700, Train Loss: 0.8040, Val Loss: 0.7628\n",
      "Epoch 0, Step 16725, Train Loss: 0.7482, Val Loss: 0.7519\n",
      "Epoch 0, Step 16750, Train Loss: 0.7457, Val Loss: 0.7571\n",
      "Epoch 0, Step 16775, Train Loss: 0.8034, Val Loss: 0.7483\n",
      "Epoch 0, Step 16800, Train Loss: 0.8018, Val Loss: 0.7582\n",
      "Epoch 0, Step 16825, Train Loss: 0.9206, Val Loss: 0.7616\n",
      "Epoch 0, Step 16850, Train Loss: 0.8148, Val Loss: 0.7593\n",
      "Epoch 0, Step 16875, Train Loss: 0.8432, Val Loss: 0.7585\n",
      "Epoch 0, Step 16900, Train Loss: 0.7750, Val Loss: 0.7583\n",
      "Epoch 0, Step 16925, Train Loss: 0.8040, Val Loss: 0.7696\n",
      "Epoch 0, Step 16950, Train Loss: 0.7747, Val Loss: 0.7624\n",
      "Epoch 0, Step 16975, Train Loss: 0.8307, Val Loss: 0.7689\n",
      "Epoch 0, Step 17000, Train Loss: 0.8136, Val Loss: 0.7555\n",
      "Epoch 0, Step 17025, Train Loss: 0.7812, Val Loss: 0.7560\n",
      "Epoch 0, Step 17050, Train Loss: 0.8041, Val Loss: 0.7535\n",
      "Epoch 0, Step 17075, Train Loss: 0.7722, Val Loss: 0.7623\n",
      "Epoch 0, Step 17100, Train Loss: 0.8028, Val Loss: 0.7634\n",
      "Epoch 0, Step 17125, Train Loss: 0.7528, Val Loss: 0.7619\n",
      "Epoch 0, Step 17150, Train Loss: 0.6964, Val Loss: 0.7667\n",
      "Epoch 0, Step 17175, Train Loss: 0.8463, Val Loss: 0.7579\n",
      "Epoch 0, Step 17200, Train Loss: 0.7848, Val Loss: 0.7622\n",
      "Epoch 0, Step 17225, Train Loss: 0.8234, Val Loss: 0.7690\n",
      "Epoch 0, Step 17250, Train Loss: 0.8429, Val Loss: 0.7739\n",
      "Epoch 0, Step 17275, Train Loss: 0.8190, Val Loss: 0.7756\n",
      "Epoch 0, Step 17300, Train Loss: 0.7669, Val Loss: 0.7685\n",
      "Epoch 0, Step 17325, Train Loss: 0.8615, Val Loss: 0.7693\n",
      "Epoch 0, Step 17350, Train Loss: 0.8619, Val Loss: 0.7639\n",
      "Epoch 0, Step 17375, Train Loss: 0.7757, Val Loss: 0.7598\n",
      "Epoch 0, Step 17400, Train Loss: 0.7785, Val Loss: 0.7593\n",
      "Epoch 0, Step 17425, Train Loss: 0.7753, Val Loss: 0.7635\n",
      "Epoch 0, Step 17450, Train Loss: 0.7567, Val Loss: 0.7634\n",
      "Epoch 0, Step 17475, Train Loss: 0.7979, Val Loss: 0.7655\n",
      "Epoch 0, Step 17500, Train Loss: 0.8488, Val Loss: 0.7723\n",
      "Epoch 0, Step 17525, Train Loss: 0.8232, Val Loss: 0.7586\n",
      "Epoch 0, Step 17550, Train Loss: 0.9559, Val Loss: 0.7591\n",
      "Epoch 0, Step 17575, Train Loss: 0.7464, Val Loss: 0.7582\n",
      "Epoch 0, Step 17600, Train Loss: 0.8048, Val Loss: 0.7632\n",
      "Epoch 0, Step 17625, Train Loss: 0.8271, Val Loss: 0.7648\n",
      "Epoch 0, Step 17650, Train Loss: 0.7807, Val Loss: 0.7655\n",
      "Epoch 0, Step 17675, Train Loss: 0.8490, Val Loss: 0.7696\n",
      "Epoch 0, Step 17700, Train Loss: 0.8079, Val Loss: 0.7698\n",
      "Epoch 0, Step 17725, Train Loss: 0.8015, Val Loss: 0.7687\n",
      "Epoch 0, Step 17750, Train Loss: 0.8401, Val Loss: 0.7598\n",
      "Epoch 0, Step 17775, Train Loss: 0.7672, Val Loss: 0.7701\n",
      "Epoch 0, Step 17800, Train Loss: 0.8676, Val Loss: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  77%|███████▋  | 21349/27842 [2:42:54<40:33,  2.67it/s, train_loss=0.8470, val_loss=0.6864]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 17825, Train Loss: 0.8531, Val Loss: 0.7613\n",
      "Epoch 0, Step 17850, Train Loss: 0.8059, Val Loss: 0.7589\n",
      "Epoch 0, Step 17875, Train Loss: 0.7766, Val Loss: 0.7677\n",
      "Epoch 0, Step 17900, Train Loss: 0.7835, Val Loss: 0.7579\n",
      "Epoch 0, Step 17925, Train Loss: 0.7768, Val Loss: 0.7634\n",
      "Epoch 0, Step 17950, Train Loss: 0.8675, Val Loss: 0.7612\n",
      "Epoch 0, Step 17975, Train Loss: 0.8302, Val Loss: 0.7497\n",
      "Epoch 0, Step 18000, Train Loss: 0.7020, Val Loss: 0.7538\n",
      "Epoch 0, Step 18025, Train Loss: 0.7770, Val Loss: 0.7593\n",
      "Epoch 0, Step 18050, Train Loss: 0.8750, Val Loss: 0.7522\n",
      "Epoch 0, Step 18075, Train Loss: 0.8661, Val Loss: 0.7606\n",
      "Epoch 0, Step 18100, Train Loss: 0.8403, Val Loss: 0.7611\n",
      "Epoch 0, Step 18125, Train Loss: 0.7450, Val Loss: 0.7690\n",
      "Epoch 0, Step 18150, Train Loss: 0.7876, Val Loss: 0.7672\n",
      "Epoch 0, Step 18175, Train Loss: 0.7707, Val Loss: 0.7632\n",
      "Epoch 0, Step 18200, Train Loss: 0.8467, Val Loss: 0.7689\n",
      "Epoch 0, Step 18225, Train Loss: 0.7208, Val Loss: 0.7724\n",
      "Epoch 0, Step 18250, Train Loss: 0.8400, Val Loss: 0.7684\n",
      "Epoch 0, Step 18275, Train Loss: 0.8413, Val Loss: 0.7621\n",
      "Epoch 0, Step 18300, Train Loss: 0.7861, Val Loss: 0.7637\n",
      "Epoch 0, Step 18325, Train Loss: 0.8319, Val Loss: 0.7626\n",
      "Epoch 0, Step 18350, Train Loss: 0.7716, Val Loss: 0.7572\n",
      "Epoch 0, Step 18375, Train Loss: 0.8928, Val Loss: 0.7549\n",
      "Epoch 0, Step 18400, Train Loss: 0.7676, Val Loss: 0.7518\n",
      "Epoch 0, Step 18425, Train Loss: 0.7670, Val Loss: 0.7547\n",
      "Epoch 0, Step 18450, Train Loss: 0.8027, Val Loss: 0.7554\n",
      "Epoch 0, Step 18475, Train Loss: 0.7766, Val Loss: 0.7416\n",
      "Epoch 0, Step 18500, Train Loss: 0.7909, Val Loss: 0.7515\n",
      "Epoch 0, Step 18525, Train Loss: 0.7501, Val Loss: 0.7493\n",
      "Epoch 0, Step 18550, Train Loss: 0.8486, Val Loss: 0.7565\n",
      "Epoch 0, Step 18575, Train Loss: 0.7119, Val Loss: 0.7511\n",
      "Epoch 0, Step 18600, Train Loss: 0.7225, Val Loss: 0.7545\n",
      "Epoch 0, Step 18625, Train Loss: 0.8552, Val Loss: 0.7506\n",
      "Epoch 0, Step 18650, Train Loss: 0.7514, Val Loss: 0.7516\n",
      "Epoch 0, Step 18675, Train Loss: 0.7705, Val Loss: 0.7566\n",
      "Epoch 0, Step 18700, Train Loss: 0.7408, Val Loss: 0.7462\n",
      "Epoch 0, Step 18725, Train Loss: 0.7589, Val Loss: 0.7612\n",
      "Epoch 0, Step 18750, Train Loss: 0.7298, Val Loss: 0.7516\n",
      "Epoch 0, Step 18775, Train Loss: 0.7963, Val Loss: 0.7535\n",
      "Epoch 0, Step 18800, Train Loss: 0.8037, Val Loss: 0.7606\n",
      "Epoch 0, Step 18825, Train Loss: 0.8491, Val Loss: 0.7512\n",
      "Epoch 0, Step 18850, Train Loss: 0.7120, Val Loss: 0.7548\n",
      "Epoch 0, Step 18875, Train Loss: 0.8279, Val Loss: 0.7573\n",
      "Epoch 0, Step 18900, Train Loss: 0.7888, Val Loss: 0.7554\n",
      "Epoch 0, Step 18925, Train Loss: 0.7711, Val Loss: 0.7510\n",
      "Epoch 0, Step 18950, Train Loss: 0.8369, Val Loss: 0.7541\n",
      "Epoch 0, Step 18975, Train Loss: 0.6846, Val Loss: 0.7521\n",
      "Epoch 0, Step 19000, Train Loss: 0.7065, Val Loss: 0.7567\n",
      "Epoch 0, Step 19025, Train Loss: 0.6842, Val Loss: 0.7554\n",
      "Epoch 0, Step 19050, Train Loss: 0.7384, Val Loss: 0.7542\n",
      "Epoch 0, Step 19075, Train Loss: 0.7341, Val Loss: 0.7577\n",
      "Epoch 0, Step 19100, Train Loss: 0.7498, Val Loss: 0.7553\n",
      "Epoch 0, Step 19125, Train Loss: 0.7524, Val Loss: 0.7622\n",
      "Epoch 0, Step 19150, Train Loss: 0.6701, Val Loss: 0.7561\n",
      "Epoch 0, Step 19175, Train Loss: 0.7509, Val Loss: 0.7604\n",
      "Epoch 0, Step 19200, Train Loss: 0.7338, Val Loss: 0.7530\n",
      "Epoch 0, Step 19225, Train Loss: 0.7112, Val Loss: 0.7567\n",
      "Epoch 0, Step 19250, Train Loss: 0.7648, Val Loss: 0.7573\n",
      "Epoch 0, Step 19275, Train Loss: 0.7819, Val Loss: 0.7594\n",
      "Epoch 0, Step 19300, Train Loss: 0.6987, Val Loss: 0.7431\n",
      "Epoch 0, Step 19325, Train Loss: 0.7466, Val Loss: 0.7535\n",
      "Epoch 0, Step 19350, Train Loss: 0.6885, Val Loss: 0.7616\n",
      "Epoch 0, Step 19375, Train Loss: 0.7113, Val Loss: 0.7479\n",
      "Epoch 0, Step 19400, Train Loss: 0.7279, Val Loss: 0.7463\n",
      "Epoch 0, Step 19425, Train Loss: 0.7926, Val Loss: 0.7125\n",
      "Epoch 0, Step 19450, Train Loss: 0.7746, Val Loss: 0.6942\n",
      "Epoch 0, Step 19475, Train Loss: 0.6986, Val Loss: 0.7012\n",
      "Epoch 0, Step 19500, Train Loss: 0.7106, Val Loss: 0.6960\n",
      "Epoch 0, Step 19525, Train Loss: 0.8404, Val Loss: 0.6954\n",
      "Epoch 0, Step 19550, Train Loss: 0.7940, Val Loss: 0.7069\n",
      "Epoch 0, Step 19575, Train Loss: 0.7834, Val Loss: 0.7069\n",
      "Epoch 0, Step 19600, Train Loss: 0.7920, Val Loss: 0.7041\n",
      "Epoch 0, Step 19625, Train Loss: 0.7007, Val Loss: 0.7159\n",
      "Epoch 0, Step 19650, Train Loss: 0.7405, Val Loss: 0.7132\n",
      "Epoch 0, Step 19675, Train Loss: 0.7329, Val Loss: 0.7123\n",
      "Epoch 0, Step 19700, Train Loss: 0.6795, Val Loss: 0.7005\n",
      "Epoch 0, Step 19725, Train Loss: 0.7634, Val Loss: 0.7026\n",
      "Epoch 0, Step 19750, Train Loss: 0.7356, Val Loss: 0.7029\n",
      "Epoch 0, Step 19775, Train Loss: 0.7279, Val Loss: 0.7065\n",
      "Epoch 0, Step 19800, Train Loss: 0.6722, Val Loss: 0.7074\n",
      "Epoch 0, Step 19825, Train Loss: 0.7268, Val Loss: 0.7051\n",
      "Epoch 0, Step 19850, Train Loss: 0.7167, Val Loss: 0.6956\n",
      "Epoch 0, Step 19875, Train Loss: 0.7729, Val Loss: 0.6737\n",
      "Epoch 0, Step 19900, Train Loss: 0.8412, Val Loss: 0.6765\n",
      "Epoch 0, Step 19925, Train Loss: 0.7500, Val Loss: 0.6737\n",
      "Epoch 0, Step 19950, Train Loss: 0.7772, Val Loss: 0.6745\n",
      "Epoch 0, Step 19975, Train Loss: 0.7405, Val Loss: 0.6751\n",
      "Epoch 0, Step 20000, Train Loss: 0.7021, Val Loss: 0.6700\n",
      "Epoch 0, Step 20025, Train Loss: 0.6835, Val Loss: 0.6688\n",
      "Epoch 0, Step 20050, Train Loss: 0.7583, Val Loss: 0.6719\n",
      "Epoch 0, Step 20075, Train Loss: 0.7953, Val Loss: 0.6784\n",
      "Epoch 0, Step 20100, Train Loss: 0.8613, Val Loss: 0.6800\n",
      "Epoch 0, Step 20125, Train Loss: 0.7623, Val Loss: 0.6711\n",
      "Epoch 0, Step 20150, Train Loss: 0.7645, Val Loss: 0.6796\n",
      "Epoch 0, Step 20175, Train Loss: 0.7707, Val Loss: 0.6707\n",
      "Epoch 0, Step 20200, Train Loss: 0.7096, Val Loss: 0.6795\n",
      "Epoch 0, Step 20225, Train Loss: 0.7683, Val Loss: 0.6887\n",
      "Epoch 0, Step 20250, Train Loss: 0.6984, Val Loss: 0.6795\n",
      "Epoch 0, Step 20275, Train Loss: 0.7588, Val Loss: 0.6813\n",
      "Epoch 0, Step 20300, Train Loss: 0.8325, Val Loss: 0.6786\n",
      "Epoch 0, Step 20325, Train Loss: 0.7064, Val Loss: 0.6828\n",
      "Epoch 0, Step 20350, Train Loss: 0.8247, Val Loss: 0.6853\n",
      "Epoch 0, Step 20375, Train Loss: 0.6655, Val Loss: 0.6808\n",
      "Epoch 0, Step 20400, Train Loss: 0.8366, Val Loss: 0.6800\n",
      "Epoch 0, Step 20425, Train Loss: 0.6655, Val Loss: 0.6858\n",
      "Epoch 0, Step 20450, Train Loss: 0.8210, Val Loss: 0.6591\n",
      "Epoch 0, Step 20475, Train Loss: 0.7366, Val Loss: 0.6629\n",
      "Epoch 0, Step 20500, Train Loss: 0.7030, Val Loss: 0.6679\n",
      "Epoch 0, Step 20525, Train Loss: 0.7400, Val Loss: 0.6642\n",
      "Epoch 0, Step 20550, Train Loss: 0.6526, Val Loss: 0.6753\n",
      "Epoch 0, Step 20575, Train Loss: 0.7596, Val Loss: 0.6759\n",
      "Epoch 0, Step 20600, Train Loss: 0.7284, Val Loss: 0.6767\n",
      "Epoch 0, Step 20625, Train Loss: 0.8233, Val Loss: 0.6739\n",
      "Epoch 0, Step 20650, Train Loss: 0.7342, Val Loss: 0.6794\n",
      "Epoch 0, Step 20675, Train Loss: 0.8307, Val Loss: 0.6735\n",
      "Epoch 0, Step 20700, Train Loss: 0.7591, Val Loss: 0.6723\n",
      "Epoch 0, Step 20725, Train Loss: 0.7692, Val Loss: 0.6715\n",
      "Epoch 0, Step 20750, Train Loss: 0.6373, Val Loss: 0.6774\n",
      "Epoch 0, Step 20775, Train Loss: 0.6974, Val Loss: 0.6888\n",
      "Epoch 0, Step 20800, Train Loss: 0.7975, Val Loss: 0.6742\n",
      "Epoch 0, Step 20825, Train Loss: 0.7440, Val Loss: 0.6835\n",
      "Epoch 0, Step 20850, Train Loss: 0.7692, Val Loss: 0.6800\n",
      "Epoch 0, Step 20875, Train Loss: 0.8021, Val Loss: 0.6789\n",
      "Epoch 0, Step 20900, Train Loss: 0.7052, Val Loss: 0.6782\n",
      "Epoch 0, Step 20925, Train Loss: 0.7255, Val Loss: 0.6730\n",
      "Epoch 0, Step 20950, Train Loss: 0.7052, Val Loss: 0.6733\n",
      "Epoch 0, Step 20975, Train Loss: 0.7410, Val Loss: 0.6737\n",
      "Epoch 0, Step 21000, Train Loss: 0.8136, Val Loss: 0.6759\n",
      "Epoch 0, Step 21025, Train Loss: 0.7750, Val Loss: 0.6775\n",
      "Epoch 0, Step 21050, Train Loss: 0.7343, Val Loss: 0.6836\n",
      "Epoch 0, Step 21075, Train Loss: 0.7140, Val Loss: 0.6865\n",
      "Epoch 0, Step 21100, Train Loss: 0.7209, Val Loss: 0.6918\n",
      "Epoch 0, Step 21125, Train Loss: 0.6551, Val Loss: 0.6772\n",
      "Epoch 0, Step 21150, Train Loss: 0.6930, Val Loss: 0.6759\n",
      "Epoch 0, Step 21175, Train Loss: 0.7311, Val Loss: 0.6812\n",
      "Epoch 0, Step 21200, Train Loss: 0.7426, Val Loss: 0.6728\n",
      "Epoch 0, Step 21225, Train Loss: 0.8067, Val Loss: 0.6825\n",
      "Epoch 0, Step 21250, Train Loss: 0.6763, Val Loss: 0.6852\n",
      "Epoch 0, Step 21275, Train Loss: 0.7838, Val Loss: 0.6816\n",
      "Epoch 0, Step 21300, Train Loss: 0.7639, Val Loss: 0.6806\n",
      "Epoch 0, Step 21325, Train Loss: 0.8213, Val Loss: 0.6813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  85%|████████▌ | 23703/27842 [2:58:25<36:31,  1.89it/s, train_loss=0.7040, val_loss=0.6623]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 0:  89%|████████▉ | 24875/27842 [3:06:13<23:14,  2.13it/s, train_loss=0.6497, val_loss=0.5979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 21350, Train Loss: 0.8470, Val Loss: 0.6864\n",
      "Epoch 0, Step 21375, Train Loss: 0.7646, Val Loss: 0.6810\n",
      "Epoch 0, Step 21400, Train Loss: 0.7178, Val Loss: 0.6838\n",
      "Epoch 0, Step 21425, Train Loss: 0.7806, Val Loss: 0.6887\n",
      "Epoch 0, Step 21450, Train Loss: 0.7616, Val Loss: 0.6934\n",
      "Epoch 0, Step 21475, Train Loss: 0.7436, Val Loss: 0.6762\n",
      "Epoch 0, Step 21500, Train Loss: 0.7770, Val Loss: 0.6786\n",
      "Epoch 0, Step 21525, Train Loss: 0.7654, Val Loss: 0.6729\n",
      "Epoch 0, Step 21550, Train Loss: 0.7225, Val Loss: 0.6795\n",
      "Epoch 0, Step 21575, Train Loss: 0.7026, Val Loss: 0.6845\n",
      "Epoch 0, Step 21600, Train Loss: 0.8330, Val Loss: 0.6659\n",
      "Epoch 0, Step 21625, Train Loss: 0.6979, Val Loss: 0.6658\n",
      "Epoch 0, Step 21650, Train Loss: 0.8503, Val Loss: 0.6691\n",
      "Epoch 0, Step 21675, Train Loss: 0.6899, Val Loss: 0.6768\n",
      "Epoch 0, Step 21700, Train Loss: 0.7898, Val Loss: 0.6566\n",
      "Epoch 0, Step 21725, Train Loss: 0.7623, Val Loss: 0.6530\n",
      "Epoch 0, Step 21750, Train Loss: 0.7609, Val Loss: 0.6509\n",
      "Epoch 0, Step 21775, Train Loss: 0.7423, Val Loss: 0.6505\n",
      "Epoch 0, Step 21800, Train Loss: 0.7709, Val Loss: 0.6591\n",
      "Epoch 0, Step 21825, Train Loss: 0.7127, Val Loss: 0.6536\n",
      "Epoch 0, Step 21850, Train Loss: 0.7208, Val Loss: 0.6602\n",
      "Epoch 0, Step 21875, Train Loss: 0.7481, Val Loss: 0.6566\n",
      "Epoch 0, Step 21900, Train Loss: 0.7199, Val Loss: 0.6549\n",
      "Epoch 0, Step 21925, Train Loss: 0.7911, Val Loss: 0.6554\n",
      "Epoch 0, Step 21950, Train Loss: 0.7532, Val Loss: 0.6495\n",
      "Epoch 0, Step 21975, Train Loss: 0.7964, Val Loss: 0.6531\n",
      "Epoch 0, Step 22000, Train Loss: 0.7650, Val Loss: 0.6512\n",
      "Epoch 0, Step 22025, Train Loss: 0.7704, Val Loss: 0.6538\n",
      "Epoch 0, Step 22050, Train Loss: 0.7145, Val Loss: 0.6617\n",
      "Epoch 0, Step 22075, Train Loss: 0.7811, Val Loss: 0.6569\n",
      "Epoch 0, Step 22100, Train Loss: 0.7024, Val Loss: 0.6638\n",
      "Epoch 0, Step 22125, Train Loss: 0.6440, Val Loss: 0.6562\n",
      "Epoch 0, Step 22150, Train Loss: 0.7363, Val Loss: 0.6595\n",
      "Epoch 0, Step 22175, Train Loss: 0.7518, Val Loss: 0.6671\n",
      "Epoch 0, Step 22200, Train Loss: 0.7102, Val Loss: 0.6612\n",
      "Epoch 0, Step 22225, Train Loss: 0.7152, Val Loss: 0.6622\n",
      "Epoch 0, Step 22250, Train Loss: 0.7160, Val Loss: 0.6603\n",
      "Epoch 0, Step 22275, Train Loss: 0.6738, Val Loss: 0.6580\n",
      "Epoch 0, Step 22300, Train Loss: 0.6880, Val Loss: 0.6572\n",
      "Epoch 0, Step 22325, Train Loss: 0.6885, Val Loss: 0.6629\n",
      "Epoch 0, Step 22350, Train Loss: 0.6712, Val Loss: 0.6710\n",
      "Epoch 0, Step 22375, Train Loss: 0.7467, Val Loss: 0.6655\n",
      "Epoch 0, Step 22400, Train Loss: 0.7662, Val Loss: 0.6672\n",
      "Epoch 0, Step 22425, Train Loss: 0.7606, Val Loss: 0.6828\n",
      "Epoch 0, Step 22450, Train Loss: 0.7017, Val Loss: 0.6665\n",
      "Epoch 0, Step 22475, Train Loss: 0.7976, Val Loss: 0.6685\n",
      "Epoch 0, Step 22500, Train Loss: 0.6984, Val Loss: 0.6625\n",
      "Epoch 0, Step 22525, Train Loss: 0.6592, Val Loss: 0.6677\n",
      "Epoch 0, Step 22550, Train Loss: 0.7065, Val Loss: 0.6725\n",
      "Epoch 0, Step 22575, Train Loss: 0.6502, Val Loss: 0.6678\n",
      "Epoch 0, Step 22600, Train Loss: 0.5764, Val Loss: 0.6716\n",
      "Epoch 0, Step 22625, Train Loss: 0.6698, Val Loss: 0.6771\n",
      "Epoch 0, Step 22650, Train Loss: 0.6607, Val Loss: 0.6742\n",
      "Epoch 0, Step 22675, Train Loss: 0.7618, Val Loss: 0.6788\n",
      "Epoch 0, Step 22700, Train Loss: 0.7035, Val Loss: 0.6796\n",
      "Epoch 0, Step 22725, Train Loss: 0.7005, Val Loss: 0.6825\n",
      "Epoch 0, Step 22750, Train Loss: 0.7582, Val Loss: 0.6722\n",
      "Epoch 0, Step 22775, Train Loss: 0.7360, Val Loss: 0.6749\n",
      "Epoch 0, Step 22800, Train Loss: 0.7576, Val Loss: 0.6848\n",
      "Epoch 0, Step 22825, Train Loss: 0.6680, Val Loss: 0.6767\n",
      "Epoch 0, Step 22850, Train Loss: 0.7854, Val Loss: 0.6637\n",
      "Epoch 0, Step 22875, Train Loss: 0.6780, Val Loss: 0.6717\n",
      "Epoch 0, Step 22900, Train Loss: 0.6345, Val Loss: 0.6739\n",
      "Epoch 0, Step 22925, Train Loss: 0.6277, Val Loss: 0.6693\n",
      "Epoch 0, Step 22950, Train Loss: 0.7050, Val Loss: 0.6668\n",
      "Epoch 0, Step 22975, Train Loss: 0.6752, Val Loss: 0.6718\n",
      "Epoch 0, Step 23000, Train Loss: 0.6198, Val Loss: 0.6681\n",
      "Epoch 0, Step 23025, Train Loss: 0.7844, Val Loss: 0.6720\n",
      "Epoch 0, Step 23050, Train Loss: 0.7516, Val Loss: 0.6708\n",
      "Epoch 0, Step 23075, Train Loss: 0.6221, Val Loss: 0.6734\n",
      "Epoch 0, Step 23100, Train Loss: 0.6784, Val Loss: 0.6643\n",
      "Epoch 0, Step 23125, Train Loss: 0.7148, Val Loss: 0.6618\n",
      "Epoch 0, Step 23150, Train Loss: 0.6442, Val Loss: 0.6631\n",
      "Epoch 0, Step 23175, Train Loss: 0.6888, Val Loss: 0.6692\n",
      "Epoch 0, Step 23200, Train Loss: 0.7763, Val Loss: 0.6632\n",
      "Epoch 0, Step 23225, Train Loss: 0.6404, Val Loss: 0.6700\n",
      "Epoch 0, Step 23250, Train Loss: 0.7544, Val Loss: 0.6612\n",
      "Epoch 0, Step 23275, Train Loss: 0.7407, Val Loss: 0.6625\n",
      "Epoch 0, Step 23300, Train Loss: 0.6823, Val Loss: 0.6638\n",
      "Epoch 0, Step 23325, Train Loss: 0.6933, Val Loss: 0.6645\n",
      "Epoch 0, Step 23350, Train Loss: 0.6459, Val Loss: 0.6596\n",
      "Epoch 0, Step 23375, Train Loss: 0.7002, Val Loss: 0.6667\n",
      "Epoch 0, Step 23400, Train Loss: 0.8122, Val Loss: 0.6699\n",
      "Epoch 0, Step 23425, Train Loss: 0.6432, Val Loss: 0.6740\n",
      "Epoch 0, Step 23450, Train Loss: 0.6651, Val Loss: 0.6646\n",
      "Epoch 0, Step 23475, Train Loss: 0.8118, Val Loss: 0.6758\n",
      "Epoch 0, Step 23500, Train Loss: 0.6925, Val Loss: 0.6673\n",
      "Epoch 0, Step 23525, Train Loss: 0.7100, Val Loss: 0.6675\n",
      "Epoch 0, Step 23550, Train Loss: 0.6791, Val Loss: 0.6692\n",
      "Epoch 0, Step 23575, Train Loss: 0.7471, Val Loss: 0.6601\n",
      "Epoch 0, Step 23600, Train Loss: 0.7174, Val Loss: 0.6642\n",
      "Epoch 0, Step 23625, Train Loss: 0.6371, Val Loss: 0.6670\n",
      "Epoch 0, Step 23650, Train Loss: 0.7126, Val Loss: 0.6642\n",
      "Epoch 0, Step 23675, Train Loss: 0.7308, Val Loss: 0.6598\n",
      "Epoch 0, Step 23700, Train Loss: 0.7040, Val Loss: 0.6623\n",
      "Epoch 0, Step 23725, Train Loss: 0.6971, Val Loss: 0.6632\n",
      "Epoch 0, Step 23750, Train Loss: 0.7363, Val Loss: 0.6701\n",
      "Epoch 0, Step 23775, Train Loss: 0.7002, Val Loss: 0.6714\n",
      "Epoch 0, Step 23800, Train Loss: 0.6798, Val Loss: 0.6707\n",
      "Epoch 0, Step 23825, Train Loss: 0.6633, Val Loss: 0.6761\n",
      "Epoch 0, Step 23850, Train Loss: 0.7356, Val Loss: 0.6758\n",
      "Epoch 0, Step 23875, Train Loss: 0.8250, Val Loss: 0.6708\n",
      "Epoch 0, Step 23900, Train Loss: 0.6678, Val Loss: 0.6735\n",
      "Epoch 0, Step 23925, Train Loss: 0.6736, Val Loss: 0.6694\n",
      "Epoch 0, Step 23950, Train Loss: 0.7851, Val Loss: 0.6715\n",
      "Epoch 0, Step 23975, Train Loss: 0.6695, Val Loss: 0.6728\n",
      "Epoch 0, Step 24000, Train Loss: 0.7467, Val Loss: 0.6678\n",
      "Epoch 0, Step 24025, Train Loss: 0.6908, Val Loss: 0.6770\n",
      "Epoch 0, Step 24050, Train Loss: 0.7007, Val Loss: 0.6620\n",
      "Epoch 0, Step 24075, Train Loss: 0.6898, Val Loss: 0.6593\n",
      "Epoch 0, Step 24100, Train Loss: 0.6343, Val Loss: 0.6638\n",
      "Epoch 0, Step 24125, Train Loss: 0.6279, Val Loss: 0.6698\n",
      "Epoch 0, Step 24150, Train Loss: 0.6916, Val Loss: 0.6560\n",
      "Epoch 0, Step 24175, Train Loss: 0.6670, Val Loss: 0.6706\n",
      "Epoch 0, Step 24200, Train Loss: 0.6963, Val Loss: 0.6676\n",
      "Epoch 0, Step 24225, Train Loss: 0.6708, Val Loss: 0.6737\n",
      "Epoch 0, Step 24250, Train Loss: 0.6977, Val Loss: 0.6597\n",
      "Epoch 0, Step 24275, Train Loss: 0.7592, Val Loss: 0.6614\n",
      "Epoch 0, Step 24300, Train Loss: 0.8221, Val Loss: 0.6676\n",
      "Epoch 0, Step 24325, Train Loss: 0.5895, Val Loss: 0.6589\n",
      "Epoch 0, Step 24350, Train Loss: 0.6099, Val Loss: 0.6411\n",
      "Epoch 0, Step 24375, Train Loss: 0.6276, Val Loss: 0.6436\n",
      "Epoch 0, Step 24400, Train Loss: 0.6650, Val Loss: 0.6513\n",
      "Epoch 0, Step 24425, Train Loss: 0.7761, Val Loss: 0.6525\n",
      "Epoch 0, Step 24450, Train Loss: 0.6640, Val Loss: 0.6552\n",
      "Epoch 0, Step 24475, Train Loss: 0.7259, Val Loss: 0.6497\n",
      "Epoch 0, Step 24500, Train Loss: 0.7836, Val Loss: 0.6501\n",
      "Epoch 0, Step 24525, Train Loss: 0.6524, Val Loss: 0.6549\n",
      "Epoch 0, Step 24550, Train Loss: 0.6879, Val Loss: 0.6582\n",
      "Epoch 0, Step 24575, Train Loss: 0.7141, Val Loss: 0.6582\n",
      "Epoch 0, Step 24600, Train Loss: 0.6225, Val Loss: 0.6457\n",
      "Epoch 0, Step 24625, Train Loss: 0.6070, Val Loss: 0.6420\n",
      "Epoch 0, Step 24650, Train Loss: 0.6133, Val Loss: 0.6364\n",
      "Epoch 0, Step 24675, Train Loss: 0.7493, Val Loss: 0.6414\n",
      "Epoch 0, Step 24700, Train Loss: 0.6805, Val Loss: 0.6009\n",
      "Epoch 0, Step 24725, Train Loss: 0.7384, Val Loss: 0.6031\n",
      "Epoch 0, Step 24750, Train Loss: 0.6221, Val Loss: 0.6032\n",
      "Epoch 0, Step 24775, Train Loss: 0.7298, Val Loss: 0.6066\n",
      "Epoch 0, Step 24800, Train Loss: 0.6774, Val Loss: 0.6036\n",
      "Epoch 0, Step 24825, Train Loss: 0.6278, Val Loss: 0.5919\n",
      "Epoch 0, Step 24850, Train Loss: 0.6895, Val Loss: 0.5958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 27842/27842 [3:26:11<00:00,  2.25it/s, train_loss=0.6544, val_loss=0.5867]\n",
      "Epoch 1:   0%|          | 0/27842 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 24875, Train Loss: 0.6497, Val Loss: 0.5979\n",
      "Epoch 0, Step 24900, Train Loss: 0.6512, Val Loss: 0.5931\n",
      "Epoch 0, Step 24925, Train Loss: 0.6442, Val Loss: 0.6012\n",
      "Epoch 0, Step 24950, Train Loss: 0.6082, Val Loss: 0.6009\n",
      "Epoch 0, Step 24975, Train Loss: 0.6198, Val Loss: 0.6047\n",
      "Epoch 0, Step 25000, Train Loss: 0.7348, Val Loss: 0.6075\n",
      "Epoch 0, Step 25025, Train Loss: 0.6876, Val Loss: 0.5936\n",
      "Epoch 0, Step 25050, Train Loss: 0.6892, Val Loss: 0.5970\n",
      "Epoch 0, Step 25075, Train Loss: 0.7111, Val Loss: 0.5801\n",
      "Epoch 0, Step 25100, Train Loss: 0.7005, Val Loss: 0.5857\n",
      "Epoch 0, Step 25125, Train Loss: 0.6943, Val Loss: 0.5876\n",
      "Epoch 0, Step 25150, Train Loss: 0.6473, Val Loss: 0.5852\n",
      "Epoch 0, Step 25175, Train Loss: 0.6096, Val Loss: 0.6001\n",
      "Epoch 0, Step 25200, Train Loss: 0.7912, Val Loss: 0.5956\n",
      "Epoch 0, Step 25225, Train Loss: 0.7065, Val Loss: 0.5930\n",
      "Epoch 0, Step 25250, Train Loss: 0.6133, Val Loss: 0.5913\n",
      "Epoch 0, Step 25275, Train Loss: 0.6389, Val Loss: 0.5879\n",
      "Epoch 0, Step 25300, Train Loss: 0.6570, Val Loss: 0.5891\n",
      "Epoch 0, Step 25325, Train Loss: 0.5916, Val Loss: 0.5859\n",
      "Epoch 0, Step 25350, Train Loss: 0.6865, Val Loss: 0.5874\n",
      "Epoch 0, Step 25375, Train Loss: 0.6174, Val Loss: 0.5841\n",
      "Epoch 0, Step 25400, Train Loss: 0.7475, Val Loss: 0.5820\n",
      "Epoch 0, Step 25425, Train Loss: 0.6369, Val Loss: 0.5878\n",
      "Epoch 0, Step 25450, Train Loss: 0.6737, Val Loss: 0.5908\n",
      "Epoch 0, Step 25475, Train Loss: 0.6942, Val Loss: 0.5879\n",
      "Epoch 0, Step 25500, Train Loss: 0.7549, Val Loss: 0.5946\n",
      "Epoch 0, Step 25525, Train Loss: 0.6517, Val Loss: 0.5887\n",
      "Epoch 0, Step 25550, Train Loss: 0.7683, Val Loss: 0.5886\n",
      "Epoch 0, Step 25575, Train Loss: 0.6428, Val Loss: 0.5940\n",
      "Epoch 0, Step 25600, Train Loss: 0.7542, Val Loss: 0.6040\n",
      "Epoch 0, Step 25625, Train Loss: 0.7408, Val Loss: 0.5947\n",
      "Epoch 0, Step 25650, Train Loss: 0.6481, Val Loss: 0.5978\n",
      "Epoch 0, Step 25675, Train Loss: 0.7340, Val Loss: 0.6024\n",
      "Epoch 0, Step 25700, Train Loss: 0.6617, Val Loss: 0.5935\n",
      "Epoch 0, Step 25725, Train Loss: 0.6662, Val Loss: 0.5947\n",
      "Epoch 0, Step 25750, Train Loss: 0.6662, Val Loss: 0.5995\n",
      "Epoch 0, Step 25775, Train Loss: 0.5966, Val Loss: 0.5963\n",
      "Epoch 0, Step 25800, Train Loss: 0.6117, Val Loss: 0.5973\n",
      "Epoch 0, Step 25825, Train Loss: 0.7226, Val Loss: 0.5941\n",
      "Epoch 0, Step 25850, Train Loss: 0.6733, Val Loss: 0.5913\n",
      "Epoch 0, Step 25875, Train Loss: 0.6524, Val Loss: 0.5945\n",
      "Epoch 0, Step 25900, Train Loss: 0.6019, Val Loss: 0.5920\n",
      "Epoch 0, Step 25925, Train Loss: 0.6794, Val Loss: 0.5911\n",
      "Epoch 0, Step 25950, Train Loss: 0.6723, Val Loss: 0.5886\n",
      "Epoch 0, Step 25975, Train Loss: 0.6959, Val Loss: 0.5977\n",
      "Epoch 0, Step 26000, Train Loss: 0.7065, Val Loss: 0.5886\n",
      "Epoch 0, Step 26025, Train Loss: 0.6239, Val Loss: 0.5885\n",
      "Epoch 0, Step 26050, Train Loss: 0.6610, Val Loss: 0.5863\n",
      "Epoch 0, Step 26075, Train Loss: 0.8010, Val Loss: 0.5957\n",
      "Epoch 0, Step 26100, Train Loss: 0.6878, Val Loss: 0.5924\n",
      "Epoch 0, Step 26125, Train Loss: 0.6901, Val Loss: 0.5847\n",
      "Epoch 0, Step 26150, Train Loss: 0.6626, Val Loss: 0.5782\n",
      "Epoch 0, Step 26175, Train Loss: 0.6436, Val Loss: 0.5834\n",
      "Epoch 0, Step 26200, Train Loss: 0.6478, Val Loss: 0.5864\n",
      "Epoch 0, Step 26225, Train Loss: 0.6699, Val Loss: 0.5849\n",
      "Epoch 0, Step 26250, Train Loss: 0.6517, Val Loss: 0.5802\n",
      "Epoch 0, Step 26275, Train Loss: 0.6842, Val Loss: 0.5834\n",
      "Epoch 0, Step 26300, Train Loss: 0.6081, Val Loss: 0.5910\n",
      "Epoch 0, Step 26325, Train Loss: 0.6328, Val Loss: 0.5886\n",
      "Epoch 0, Step 26350, Train Loss: 0.5840, Val Loss: 0.5942\n",
      "Epoch 0, Step 26375, Train Loss: 0.5723, Val Loss: 0.5851\n",
      "Epoch 0, Step 26400, Train Loss: 0.7086, Val Loss: 0.5917\n",
      "Epoch 0, Step 26425, Train Loss: 0.6223, Val Loss: 0.5919\n",
      "Epoch 0, Step 26450, Train Loss: 0.6648, Val Loss: 0.5963\n",
      "Epoch 0, Step 26475, Train Loss: 0.6458, Val Loss: 0.5911\n",
      "Epoch 0, Step 26500, Train Loss: 0.6375, Val Loss: 0.5911\n",
      "Epoch 0, Step 26525, Train Loss: 0.6341, Val Loss: 0.5930\n",
      "Epoch 0, Step 26550, Train Loss: 0.6656, Val Loss: 0.5820\n",
      "Epoch 0, Step 26575, Train Loss: 0.6689, Val Loss: 0.5842\n",
      "Epoch 0, Step 26600, Train Loss: 0.7109, Val Loss: 0.5844\n",
      "Epoch 0, Step 26625, Train Loss: 0.7310, Val Loss: 0.5918\n",
      "Epoch 0, Step 26650, Train Loss: 0.6572, Val Loss: 0.5893\n",
      "Epoch 0, Step 26675, Train Loss: 0.7175, Val Loss: 0.5908\n",
      "Epoch 0, Step 26700, Train Loss: 0.7686, Val Loss: 0.5892\n",
      "Epoch 0, Step 26725, Train Loss: 0.6814, Val Loss: 0.5912\n",
      "Epoch 0, Step 26750, Train Loss: 0.7281, Val Loss: 0.5867\n",
      "Epoch 0, Step 26775, Train Loss: 0.7335, Val Loss: 0.5876\n",
      "Epoch 0, Step 26800, Train Loss: 0.6731, Val Loss: 0.5942\n",
      "Epoch 0, Step 26825, Train Loss: 0.6235, Val Loss: 0.5977\n",
      "Epoch 0, Step 26850, Train Loss: 0.6889, Val Loss: 0.5926\n",
      "Epoch 0, Step 26875, Train Loss: 0.6447, Val Loss: 0.5880\n",
      "Epoch 0, Step 26900, Train Loss: 0.7511, Val Loss: 0.5925\n",
      "Epoch 0, Step 26925, Train Loss: 0.6287, Val Loss: 0.5991\n",
      "Epoch 0, Step 26950, Train Loss: 0.5993, Val Loss: 0.5996\n",
      "Epoch 0, Step 26975, Train Loss: 0.6007, Val Loss: 0.6023\n",
      "Epoch 0, Step 27000, Train Loss: 0.6299, Val Loss: 0.5877\n",
      "Epoch 0, Step 27025, Train Loss: 0.6993, Val Loss: 0.5914\n",
      "Epoch 0, Step 27050, Train Loss: 0.7487, Val Loss: 0.5939\n",
      "Epoch 0, Step 27075, Train Loss: 0.6292, Val Loss: 0.5934\n",
      "Epoch 0, Step 27100, Train Loss: 0.6207, Val Loss: 0.5879\n",
      "Epoch 0, Step 27125, Train Loss: 0.7914, Val Loss: 0.5876\n",
      "Epoch 0, Step 27150, Train Loss: 0.5266, Val Loss: 0.5951\n",
      "Epoch 0, Step 27175, Train Loss: 0.5544, Val Loss: 0.5985\n",
      "Epoch 0, Step 27200, Train Loss: 0.5511, Val Loss: 0.5909\n",
      "Epoch 0, Step 27225, Train Loss: 0.6491, Val Loss: 0.5894\n",
      "Epoch 0, Step 27250, Train Loss: 0.6812, Val Loss: 0.5846\n",
      "Epoch 0, Step 27275, Train Loss: 0.6335, Val Loss: 0.5745\n",
      "Epoch 0, Step 27300, Train Loss: 0.6527, Val Loss: 0.5791\n",
      "Epoch 0, Step 27325, Train Loss: 0.6434, Val Loss: 0.5803\n",
      "Epoch 0, Step 27350, Train Loss: 0.5875, Val Loss: 0.5867\n",
      "Epoch 0, Step 27375, Train Loss: 0.6520, Val Loss: 0.5852\n",
      "Epoch 0, Step 27400, Train Loss: 0.6330, Val Loss: 0.5856\n",
      "Epoch 0, Step 27425, Train Loss: 0.6121, Val Loss: 0.5815\n",
      "Epoch 0, Step 27450, Train Loss: 0.5874, Val Loss: 0.5821\n",
      "Epoch 0, Step 27475, Train Loss: 0.7699, Val Loss: 0.5813\n",
      "Epoch 0, Step 27500, Train Loss: 0.6161, Val Loss: 0.5782\n",
      "Epoch 0, Step 27525, Train Loss: 0.5770, Val Loss: 0.5795\n",
      "Epoch 0, Step 27550, Train Loss: 0.5401, Val Loss: 0.5813\n",
      "Epoch 0, Step 27575, Train Loss: 0.6299, Val Loss: 0.5833\n",
      "Epoch 0, Step 27600, Train Loss: 0.6157, Val Loss: 0.5848\n",
      "Epoch 0, Step 27625, Train Loss: 0.7063, Val Loss: 0.5789\n",
      "Epoch 0, Step 27650, Train Loss: 0.6693, Val Loss: 0.5875\n",
      "Epoch 0, Step 27675, Train Loss: 0.6644, Val Loss: 0.5812\n",
      "Epoch 0, Step 27700, Train Loss: 0.6583, Val Loss: 0.5822\n",
      "Epoch 0, Step 27725, Train Loss: 0.6155, Val Loss: 0.5805\n",
      "Epoch 0, Step 27750, Train Loss: 0.6642, Val Loss: 0.5847\n",
      "Epoch 0, Step 27775, Train Loss: 0.6136, Val Loss: 0.5843\n",
      "Epoch 0, Step 27800, Train Loss: 0.6341, Val Loss: 0.5852\n",
      "Epoch 0, Step 27825, Train Loss: 0.6544, Val Loss: 0.5867\n",
      "27842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 3625/27842 [24:14<3:27:10,  1.95it/s, train_loss=0.5872, val_loss=0.5476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 25, Train Loss: 0.6526, Val Loss: 0.5922\n",
      "Epoch 1, Step 50, Train Loss: 0.5746, Val Loss: 0.5815\n",
      "Epoch 1, Step 75, Train Loss: 0.5733, Val Loss: 0.5785\n",
      "Epoch 1, Step 100, Train Loss: 0.5477, Val Loss: 0.5763\n",
      "Epoch 1, Step 125, Train Loss: 0.5529, Val Loss: 0.5722\n",
      "Epoch 1, Step 150, Train Loss: 0.6569, Val Loss: 0.5731\n",
      "Epoch 1, Step 175, Train Loss: 0.6029, Val Loss: 0.5757\n",
      "Epoch 1, Step 200, Train Loss: 0.6021, Val Loss: 0.5774\n",
      "Epoch 1, Step 225, Train Loss: 0.6057, Val Loss: 0.5913\n",
      "Epoch 1, Step 250, Train Loss: 0.5397, Val Loss: 0.5928\n",
      "Epoch 1, Step 275, Train Loss: 0.6392, Val Loss: 0.5863\n",
      "Epoch 1, Step 300, Train Loss: 0.5610, Val Loss: 0.5840\n",
      "Epoch 1, Step 325, Train Loss: 0.5346, Val Loss: 0.5855\n",
      "Epoch 1, Step 350, Train Loss: 0.5546, Val Loss: 0.5806\n",
      "Epoch 1, Step 375, Train Loss: 0.6041, Val Loss: 0.5882\n",
      "Epoch 1, Step 400, Train Loss: 0.4996, Val Loss: 0.5938\n",
      "Epoch 1, Step 425, Train Loss: 0.6085, Val Loss: 0.5915\n",
      "Epoch 1, Step 450, Train Loss: 0.6720, Val Loss: 0.5832\n",
      "Epoch 1, Step 475, Train Loss: 0.5555, Val Loss: 0.5779\n",
      "Epoch 1, Step 500, Train Loss: 0.5797, Val Loss: 0.5767\n",
      "Epoch 1, Step 525, Train Loss: 0.6392, Val Loss: 0.5879\n",
      "Epoch 1, Step 550, Train Loss: 0.5414, Val Loss: 0.5893\n",
      "Epoch 1, Step 575, Train Loss: 0.6455, Val Loss: 0.5800\n",
      "Epoch 1, Step 600, Train Loss: 0.6238, Val Loss: 0.5770\n",
      "Epoch 1, Step 625, Train Loss: 0.5546, Val Loss: 0.5790\n",
      "Epoch 1, Step 650, Train Loss: 0.5939, Val Loss: 0.5806\n",
      "Epoch 1, Step 675, Train Loss: 0.6958, Val Loss: 0.5766\n",
      "Epoch 1, Step 700, Train Loss: 0.6126, Val Loss: 0.5767\n",
      "Epoch 1, Step 725, Train Loss: 0.6928, Val Loss: 0.5890\n",
      "Epoch 1, Step 750, Train Loss: 0.6126, Val Loss: 0.5732\n",
      "Epoch 1, Step 775, Train Loss: 0.5172, Val Loss: 0.5746\n",
      "Epoch 1, Step 800, Train Loss: 0.5274, Val Loss: 0.5822\n",
      "Epoch 1, Step 825, Train Loss: 0.6385, Val Loss: 0.5759\n",
      "Epoch 1, Step 850, Train Loss: 0.4935, Val Loss: 0.5790\n",
      "Epoch 1, Step 875, Train Loss: 0.5910, Val Loss: 0.5800\n",
      "Epoch 1, Step 900, Train Loss: 0.6785, Val Loss: 0.5817\n",
      "Epoch 1, Step 925, Train Loss: 0.5512, Val Loss: 0.5748\n",
      "Epoch 1, Step 950, Train Loss: 0.6037, Val Loss: 0.5733\n",
      "Epoch 1, Step 975, Train Loss: 0.6351, Val Loss: 0.5708\n",
      "Epoch 1, Step 1000, Train Loss: 0.5410, Val Loss: 0.5680\n",
      "Epoch 1, Step 1025, Train Loss: 0.5800, Val Loss: 0.5685\n",
      "Epoch 1, Step 1050, Train Loss: 0.5773, Val Loss: 0.5679\n",
      "Epoch 1, Step 1075, Train Loss: 0.5462, Val Loss: 0.5703\n",
      "Epoch 1, Step 1100, Train Loss: 0.5505, Val Loss: 0.5722\n",
      "Epoch 1, Step 1125, Train Loss: 0.6657, Val Loss: 0.5704\n",
      "Epoch 1, Step 1150, Train Loss: 0.5740, Val Loss: 0.5779\n",
      "Epoch 1, Step 1175, Train Loss: 0.5073, Val Loss: 0.5735\n",
      "Epoch 1, Step 1200, Train Loss: 0.6324, Val Loss: 0.5758\n",
      "Epoch 1, Step 1225, Train Loss: 0.5489, Val Loss: 0.5768\n",
      "Epoch 1, Step 1250, Train Loss: 0.5595, Val Loss: 0.5787\n",
      "Epoch 1, Step 1275, Train Loss: 0.5804, Val Loss: 0.5749\n",
      "Epoch 1, Step 1300, Train Loss: 0.6498, Val Loss: 0.5618\n",
      "Epoch 1, Step 1325, Train Loss: 0.5522, Val Loss: 0.5701\n",
      "Epoch 1, Step 1350, Train Loss: 0.6397, Val Loss: 0.5716\n",
      "Epoch 1, Step 1375, Train Loss: 0.5990, Val Loss: 0.5718\n",
      "Epoch 1, Step 1400, Train Loss: 0.5045, Val Loss: 0.5787\n",
      "Epoch 1, Step 1425, Train Loss: 0.6258, Val Loss: 0.5724\n",
      "Epoch 1, Step 1450, Train Loss: 0.6455, Val Loss: 0.5653\n",
      "Epoch 1, Step 1475, Train Loss: 0.5916, Val Loss: 0.5671\n",
      "Epoch 1, Step 1500, Train Loss: 0.6291, Val Loss: 0.5716\n",
      "Epoch 1, Step 1525, Train Loss: 0.6361, Val Loss: 0.5682\n",
      "Epoch 1, Step 1550, Train Loss: 0.5741, Val Loss: 0.5649\n",
      "Epoch 1, Step 1575, Train Loss: 0.6675, Val Loss: 0.5633\n",
      "Epoch 1, Step 1600, Train Loss: 0.5627, Val Loss: 0.5653\n",
      "Epoch 1, Step 1625, Train Loss: 0.5763, Val Loss: 0.5594\n",
      "Epoch 1, Step 1650, Train Loss: 0.4781, Val Loss: 0.5703\n",
      "Epoch 1, Step 1675, Train Loss: 0.4587, Val Loss: 0.5708\n",
      "Epoch 1, Step 1700, Train Loss: 0.5508, Val Loss: 0.5581\n",
      "Epoch 1, Step 1725, Train Loss: 0.5904, Val Loss: 0.5582\n",
      "Epoch 1, Step 1750, Train Loss: 0.5864, Val Loss: 0.5527\n",
      "Epoch 1, Step 1775, Train Loss: 0.4839, Val Loss: 0.5563\n",
      "Epoch 1, Step 1800, Train Loss: 0.5325, Val Loss: 0.5556\n",
      "Epoch 1, Step 1825, Train Loss: 0.4965, Val Loss: 0.5573\n",
      "Epoch 1, Step 1850, Train Loss: 0.6895, Val Loss: 0.5530\n",
      "Epoch 1, Step 1875, Train Loss: 0.6159, Val Loss: 0.5545\n",
      "Epoch 1, Step 1900, Train Loss: 0.5676, Val Loss: 0.5536\n",
      "Epoch 1, Step 1925, Train Loss: 0.5792, Val Loss: 0.5481\n",
      "Epoch 1, Step 1950, Train Loss: 0.5932, Val Loss: 0.5536\n",
      "Epoch 1, Step 1975, Train Loss: 0.4950, Val Loss: 0.5569\n",
      "Epoch 1, Step 2000, Train Loss: 0.6480, Val Loss: 0.5554\n",
      "Epoch 1, Step 2025, Train Loss: 0.5688, Val Loss: 0.5542\n",
      "Epoch 1, Step 2050, Train Loss: 0.5740, Val Loss: 0.5561\n",
      "Epoch 1, Step 2075, Train Loss: 0.6545, Val Loss: 0.5542\n",
      "Epoch 1, Step 2100, Train Loss: 0.5841, Val Loss: 0.5617\n",
      "Epoch 1, Step 2125, Train Loss: 0.5693, Val Loss: 0.5526\n",
      "Epoch 1, Step 2150, Train Loss: 0.5850, Val Loss: 0.5526\n",
      "Epoch 1, Step 2175, Train Loss: 0.5152, Val Loss: 0.5554\n",
      "Epoch 1, Step 2200, Train Loss: 0.5828, Val Loss: 0.5555\n",
      "Epoch 1, Step 2225, Train Loss: 0.5936, Val Loss: 0.5522\n",
      "Epoch 1, Step 2250, Train Loss: 0.5592, Val Loss: 0.5536\n",
      "Epoch 1, Step 2275, Train Loss: 0.5680, Val Loss: 0.5569\n",
      "Epoch 1, Step 2300, Train Loss: 0.5694, Val Loss: 0.5548\n",
      "Epoch 1, Step 2325, Train Loss: 0.5645, Val Loss: 0.5625\n",
      "Epoch 1, Step 2350, Train Loss: 0.5693, Val Loss: 0.5602\n",
      "Epoch 1, Step 2375, Train Loss: 0.6333, Val Loss: 0.5628\n",
      "Epoch 1, Step 2400, Train Loss: 0.5076, Val Loss: 0.5599\n",
      "Epoch 1, Step 2425, Train Loss: 0.5514, Val Loss: 0.5627\n",
      "Epoch 1, Step 2450, Train Loss: 0.5785, Val Loss: 0.5621\n",
      "Epoch 1, Step 2475, Train Loss: 0.5600, Val Loss: 0.5591\n",
      "Epoch 1, Step 2500, Train Loss: 0.5493, Val Loss: 0.5671\n",
      "Epoch 1, Step 2525, Train Loss: 0.5132, Val Loss: 0.5670\n",
      "Epoch 1, Step 2550, Train Loss: 0.4885, Val Loss: 0.5609\n",
      "Epoch 1, Step 2575, Train Loss: 0.5242, Val Loss: 0.5660\n",
      "Epoch 1, Step 2600, Train Loss: 0.5384, Val Loss: 0.5602\n",
      "Epoch 1, Step 2625, Train Loss: 0.5872, Val Loss: 0.5595\n",
      "Epoch 1, Step 2650, Train Loss: 0.6141, Val Loss: 0.5686\n",
      "Epoch 1, Step 2675, Train Loss: 0.5806, Val Loss: 0.5684\n",
      "Epoch 1, Step 2700, Train Loss: 0.5873, Val Loss: 0.5630\n",
      "Epoch 1, Step 2725, Train Loss: 0.5886, Val Loss: 0.5271\n",
      "Epoch 1, Step 2750, Train Loss: 0.6240, Val Loss: 0.5248\n",
      "Epoch 1, Step 2775, Train Loss: 0.5278, Val Loss: 0.5323\n",
      "Epoch 1, Step 2800, Train Loss: 0.5713, Val Loss: 0.5346\n",
      "Epoch 1, Step 2825, Train Loss: 0.5577, Val Loss: 0.5351\n",
      "Epoch 1, Step 2850, Train Loss: 0.5698, Val Loss: 0.5322\n",
      "Epoch 1, Step 2875, Train Loss: 0.5500, Val Loss: 0.5398\n",
      "Epoch 1, Step 2900, Train Loss: 0.6127, Val Loss: 0.5393\n",
      "Epoch 1, Step 2925, Train Loss: 0.6782, Val Loss: 0.5322\n",
      "Epoch 1, Step 2950, Train Loss: 0.6732, Val Loss: 0.5337\n",
      "Epoch 1, Step 2975, Train Loss: 0.5758, Val Loss: 0.5378\n",
      "Epoch 1, Step 3000, Train Loss: 0.5670, Val Loss: 0.5389\n",
      "Epoch 1, Step 3025, Train Loss: 0.5797, Val Loss: 0.5410\n",
      "Epoch 1, Step 3050, Train Loss: 0.6055, Val Loss: 0.5373\n",
      "Epoch 1, Step 3075, Train Loss: 0.5395, Val Loss: 0.5429\n",
      "Epoch 1, Step 3100, Train Loss: 0.5874, Val Loss: 0.5458\n",
      "Epoch 1, Step 3125, Train Loss: 0.6132, Val Loss: 0.5416\n",
      "Epoch 1, Step 3150, Train Loss: 0.6414, Val Loss: 0.5428\n",
      "Epoch 1, Step 3175, Train Loss: 0.5571, Val Loss: 0.5445\n",
      "Epoch 1, Step 3200, Train Loss: 0.5547, Val Loss: 0.5462\n",
      "Epoch 1, Step 3225, Train Loss: 0.5449, Val Loss: 0.5448\n",
      "Epoch 1, Step 3250, Train Loss: 0.6155, Val Loss: 0.5470\n",
      "Epoch 1, Step 3275, Train Loss: 0.5044, Val Loss: 0.5466\n",
      "Epoch 1, Step 3300, Train Loss: 0.5676, Val Loss: 0.5461\n",
      "Epoch 1, Step 3325, Train Loss: 0.5900, Val Loss: 0.5504\n",
      "Epoch 1, Step 3350, Train Loss: 0.5853, Val Loss: 0.5454\n",
      "Epoch 1, Step 3375, Train Loss: 0.5452, Val Loss: 0.5459\n",
      "Epoch 1, Step 3400, Train Loss: 0.4937, Val Loss: 0.5477\n",
      "Epoch 1, Step 3425, Train Loss: 0.5455, Val Loss: 0.5581\n",
      "Epoch 1, Step 3450, Train Loss: 0.4432, Val Loss: 0.5532\n",
      "Epoch 1, Step 3475, Train Loss: 0.5948, Val Loss: 0.5517\n",
      "Epoch 1, Step 3500, Train Loss: 0.6340, Val Loss: 0.5507\n",
      "Epoch 1, Step 3525, Train Loss: 0.5330, Val Loss: 0.5408\n",
      "Epoch 1, Step 3550, Train Loss: 0.5537, Val Loss: 0.5461\n",
      "Epoch 1, Step 3575, Train Loss: 0.5792, Val Loss: 0.5458\n",
      "Epoch 1, Step 3600, Train Loss: 0.5684, Val Loss: 0.5494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 7199/27842 [48:10<2:02:42,  2.80it/s, train_loss=0.5183, val_loss=0.4402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 3625, Train Loss: 0.5872, Val Loss: 0.5476\n",
      "Epoch 1, Step 3650, Train Loss: 0.5537, Val Loss: 0.5578\n",
      "Epoch 1, Step 3675, Train Loss: 0.5272, Val Loss: 0.5602\n",
      "Epoch 1, Step 3700, Train Loss: 0.5989, Val Loss: 0.5578\n",
      "Epoch 1, Step 3725, Train Loss: 0.5303, Val Loss: 0.5599\n",
      "Epoch 1, Step 3750, Train Loss: 0.5542, Val Loss: 0.5598\n",
      "Epoch 1, Step 3775, Train Loss: 0.5574, Val Loss: 0.5597\n",
      "Epoch 1, Step 3800, Train Loss: 0.5118, Val Loss: 0.5546\n",
      "Epoch 1, Step 3825, Train Loss: 0.6094, Val Loss: 0.5475\n",
      "Epoch 1, Step 3850, Train Loss: 0.5961, Val Loss: 0.5503\n",
      "Epoch 1, Step 3875, Train Loss: 0.4832, Val Loss: 0.5552\n",
      "Epoch 1, Step 3900, Train Loss: 0.5032, Val Loss: 0.5528\n",
      "Epoch 1, Step 3925, Train Loss: 0.5842, Val Loss: 0.5530\n",
      "Epoch 1, Step 3950, Train Loss: 0.6201, Val Loss: 0.5472\n",
      "Epoch 1, Step 3975, Train Loss: 0.5670, Val Loss: 0.5474\n",
      "Epoch 1, Step 4000, Train Loss: 0.4800, Val Loss: 0.5525\n",
      "Epoch 1, Step 4025, Train Loss: 0.6224, Val Loss: 0.5482\n",
      "Epoch 1, Step 4050, Train Loss: 0.5441, Val Loss: 0.5489\n",
      "Epoch 1, Step 4075, Train Loss: 0.5629, Val Loss: 0.5517\n",
      "Epoch 1, Step 4100, Train Loss: 0.5121, Val Loss: 0.5524\n",
      "Epoch 1, Step 4125, Train Loss: 0.5300, Val Loss: 0.5521\n",
      "Epoch 1, Step 4150, Train Loss: 0.5928, Val Loss: 0.5519\n",
      "Epoch 1, Step 4175, Train Loss: 0.6460, Val Loss: 0.5480\n",
      "Epoch 1, Step 4200, Train Loss: 0.5138, Val Loss: 0.5564\n",
      "Epoch 1, Step 4225, Train Loss: 0.6064, Val Loss: 0.5446\n",
      "Epoch 1, Step 4250, Train Loss: 0.5414, Val Loss: 0.5487\n",
      "Epoch 1, Step 4275, Train Loss: 0.5877, Val Loss: 0.5479\n",
      "Epoch 1, Step 4300, Train Loss: 0.5009, Val Loss: 0.5504\n",
      "Epoch 1, Step 4325, Train Loss: 0.5196, Val Loss: 0.5543\n",
      "Epoch 1, Step 4350, Train Loss: 0.5954, Val Loss: 0.5540\n",
      "Epoch 1, Step 4375, Train Loss: 0.5612, Val Loss: 0.5554\n",
      "Epoch 1, Step 4400, Train Loss: 0.5403, Val Loss: 0.5523\n",
      "Epoch 1, Step 4425, Train Loss: 0.5541, Val Loss: 0.5468\n",
      "Epoch 1, Step 4450, Train Loss: 0.5343, Val Loss: 0.5506\n",
      "Epoch 1, Step 4475, Train Loss: 0.4839, Val Loss: 0.5529\n",
      "Epoch 1, Step 4500, Train Loss: 0.6502, Val Loss: 0.5484\n",
      "Epoch 1, Step 4525, Train Loss: 0.5653, Val Loss: 0.5475\n",
      "Epoch 1, Step 4550, Train Loss: 0.6447, Val Loss: 0.5511\n",
      "Epoch 1, Step 4575, Train Loss: 0.4992, Val Loss: 0.5516\n",
      "Epoch 1, Step 4600, Train Loss: 0.5224, Val Loss: 0.5532\n",
      "Epoch 1, Step 4625, Train Loss: 0.6224, Val Loss: 0.5564\n",
      "Epoch 1, Step 4650, Train Loss: 0.5562, Val Loss: 0.5477\n",
      "Epoch 1, Step 4675, Train Loss: 0.5815, Val Loss: 0.5489\n",
      "Epoch 1, Step 4700, Train Loss: 0.4889, Val Loss: 0.5476\n",
      "Epoch 1, Step 4725, Train Loss: 0.4679, Val Loss: 0.5536\n",
      "Epoch 1, Step 4750, Train Loss: 0.5635, Val Loss: 0.5534\n",
      "Epoch 1, Step 4775, Train Loss: 0.5005, Val Loss: 0.5538\n",
      "Epoch 1, Step 4800, Train Loss: 0.5335, Val Loss: 0.5507\n",
      "Epoch 1, Step 4825, Train Loss: 0.5786, Val Loss: 0.5560\n",
      "Epoch 1, Step 4850, Train Loss: 0.5146, Val Loss: 0.5548\n",
      "Epoch 1, Step 4875, Train Loss: 0.5675, Val Loss: 0.5532\n",
      "Epoch 1, Step 4900, Train Loss: 0.4821, Val Loss: 0.5539\n",
      "Epoch 1, Step 4925, Train Loss: 0.5646, Val Loss: 0.5556\n",
      "Epoch 1, Step 4950, Train Loss: 0.5153, Val Loss: 0.5560\n",
      "Epoch 1, Step 4975, Train Loss: 0.4843, Val Loss: 0.5573\n",
      "Epoch 1, Step 5000, Train Loss: 0.4787, Val Loss: 0.5602\n",
      "Epoch 1, Step 5025, Train Loss: 0.5753, Val Loss: 0.5625\n",
      "Epoch 1, Step 5050, Train Loss: 0.5676, Val Loss: 0.5653\n",
      "Epoch 1, Step 5075, Train Loss: 0.5634, Val Loss: 0.5646\n",
      "Epoch 1, Step 5100, Train Loss: 0.5328, Val Loss: 0.5592\n",
      "Epoch 1, Step 5125, Train Loss: 0.6622, Val Loss: 0.5608\n",
      "Epoch 1, Step 5150, Train Loss: 0.5009, Val Loss: 0.5601\n",
      "Epoch 1, Step 5175, Train Loss: 0.6043, Val Loss: 0.5553\n",
      "Epoch 1, Step 5200, Train Loss: 0.5911, Val Loss: 0.5571\n",
      "Epoch 1, Step 5225, Train Loss: 0.6294, Val Loss: 0.5594\n",
      "Epoch 1, Step 5250, Train Loss: 0.5164, Val Loss: 0.5563\n",
      "Epoch 1, Step 5275, Train Loss: 0.5421, Val Loss: 0.5629\n",
      "Epoch 1, Step 5300, Train Loss: 0.5771, Val Loss: 0.5568\n",
      "Epoch 1, Step 5325, Train Loss: 0.4756, Val Loss: 0.5573\n",
      "Epoch 1, Step 5350, Train Loss: 0.5638, Val Loss: 0.5625\n",
      "Epoch 1, Step 5375, Train Loss: 0.5468, Val Loss: 0.5652\n",
      "Epoch 1, Step 5400, Train Loss: 0.4968, Val Loss: 0.5668\n",
      "Epoch 1, Step 5425, Train Loss: 0.5810, Val Loss: 0.5652\n",
      "Epoch 1, Step 5450, Train Loss: 0.5190, Val Loss: 0.5402\n",
      "Epoch 1, Step 5475, Train Loss: 0.6232, Val Loss: 0.5302\n",
      "Epoch 1, Step 5500, Train Loss: 0.6534, Val Loss: 0.5273\n",
      "Epoch 1, Step 5525, Train Loss: 0.4679, Val Loss: 0.5368\n",
      "Epoch 1, Step 5550, Train Loss: 0.5288, Val Loss: 0.5394\n",
      "Epoch 1, Step 5575, Train Loss: 0.5508, Val Loss: 0.5423\n",
      "Epoch 1, Step 5600, Train Loss: 0.5503, Val Loss: 0.5348\n",
      "Epoch 1, Step 5625, Train Loss: 0.5272, Val Loss: 0.5347\n",
      "Epoch 1, Step 5650, Train Loss: 0.5980, Val Loss: 0.5424\n",
      "Epoch 1, Step 5675, Train Loss: 0.5032, Val Loss: 0.5378\n",
      "Epoch 1, Step 5700, Train Loss: 0.5053, Val Loss: 0.5356\n",
      "Epoch 1, Step 5725, Train Loss: 0.5577, Val Loss: 0.5365\n",
      "Epoch 1, Step 5750, Train Loss: 0.4431, Val Loss: 0.5334\n",
      "Epoch 1, Step 5775, Train Loss: 0.5533, Val Loss: 0.5351\n",
      "Epoch 1, Step 5800, Train Loss: 0.5428, Val Loss: 0.5340\n",
      "Epoch 1, Step 5825, Train Loss: 0.5511, Val Loss: 0.5309\n",
      "Epoch 1, Step 5850, Train Loss: 0.5041, Val Loss: 0.5343\n",
      "Epoch 1, Step 5875, Train Loss: 0.5123, Val Loss: 0.5398\n",
      "Epoch 1, Step 5900, Train Loss: 0.4926, Val Loss: 0.5223\n",
      "Epoch 1, Step 5925, Train Loss: 0.5428, Val Loss: 0.5190\n",
      "Epoch 1, Step 5950, Train Loss: 0.6764, Val Loss: 0.5248\n",
      "Epoch 1, Step 5975, Train Loss: 0.5773, Val Loss: 0.5250\n",
      "Epoch 1, Step 6000, Train Loss: 0.5389, Val Loss: 0.5187\n",
      "Epoch 1, Step 6025, Train Loss: 0.5368, Val Loss: 0.5120\n",
      "Epoch 1, Step 6050, Train Loss: 0.5689, Val Loss: 0.5156\n",
      "Epoch 1, Step 6075, Train Loss: 0.5732, Val Loss: 0.5172\n",
      "Epoch 1, Step 6100, Train Loss: 0.6161, Val Loss: 0.5163\n",
      "Epoch 1, Step 6125, Train Loss: 0.4850, Val Loss: 0.5132\n",
      "Epoch 1, Step 6150, Train Loss: 0.5939, Val Loss: 0.5165\n",
      "Epoch 1, Step 6175, Train Loss: 0.5669, Val Loss: 0.5203\n",
      "Epoch 1, Step 6200, Train Loss: 0.4909, Val Loss: 0.5208\n",
      "Epoch 1, Step 6225, Train Loss: 0.5736, Val Loss: 0.5196\n",
      "Epoch 1, Step 6250, Train Loss: 0.4965, Val Loss: 0.5211\n",
      "Epoch 1, Step 6275, Train Loss: 0.5188, Val Loss: 0.5145\n",
      "Epoch 1, Step 6300, Train Loss: 0.5590, Val Loss: 0.5158\n",
      "Epoch 1, Step 6325, Train Loss: 0.5059, Val Loss: 0.5170\n",
      "Epoch 1, Step 6350, Train Loss: 0.5757, Val Loss: 0.5187\n",
      "Epoch 1, Step 6375, Train Loss: 0.5747, Val Loss: 0.5230\n",
      "Epoch 1, Step 6400, Train Loss: 0.5413, Val Loss: 0.5133\n",
      "Epoch 1, Step 6425, Train Loss: 0.4538, Val Loss: 0.5127\n",
      "Epoch 1, Step 6450, Train Loss: 0.5251, Val Loss: 0.5158\n",
      "Epoch 1, Step 6475, Train Loss: 0.4945, Val Loss: 0.5208\n",
      "Epoch 1, Step 6500, Train Loss: 0.5023, Val Loss: 0.5303\n",
      "Epoch 1, Step 6525, Train Loss: 0.5347, Val Loss: 0.5270\n",
      "Epoch 1, Step 6550, Train Loss: 0.4814, Val Loss: 0.5219\n",
      "Epoch 1, Step 6575, Train Loss: 0.6047, Val Loss: 0.5206\n",
      "Epoch 1, Step 6600, Train Loss: 0.5490, Val Loss: 0.5018\n",
      "Epoch 1, Step 6625, Train Loss: 0.5399, Val Loss: 0.4997\n",
      "Epoch 1, Step 6650, Train Loss: 0.4378, Val Loss: 0.4995\n",
      "Epoch 1, Step 6675, Train Loss: 0.5569, Val Loss: 0.4988\n",
      "Epoch 1, Step 6700, Train Loss: 0.5152, Val Loss: 0.5006\n",
      "Epoch 1, Step 6725, Train Loss: 0.5552, Val Loss: 0.4986\n",
      "Epoch 1, Step 6750, Train Loss: 0.5253, Val Loss: 0.4954\n",
      "Epoch 1, Step 6775, Train Loss: 0.4596, Val Loss: 0.4962\n",
      "Epoch 1, Step 6800, Train Loss: 0.6624, Val Loss: 0.4993\n",
      "Epoch 1, Step 6825, Train Loss: 0.5345, Val Loss: 0.5046\n",
      "Epoch 1, Step 6850, Train Loss: 0.4909, Val Loss: 0.5083\n",
      "Epoch 1, Step 6875, Train Loss: 0.5806, Val Loss: 0.5021\n",
      "Epoch 1, Step 6900, Train Loss: 0.6202, Val Loss: 0.4991\n",
      "Epoch 1, Step 6925, Train Loss: 0.5774, Val Loss: 0.4980\n",
      "Epoch 1, Step 6950, Train Loss: 0.4979, Val Loss: 0.5011\n",
      "Epoch 1, Step 6975, Train Loss: 0.5783, Val Loss: 0.5086\n",
      "Epoch 1, Step 7000, Train Loss: 0.5367, Val Loss: 0.4508\n",
      "Epoch 1, Step 7025, Train Loss: 0.4319, Val Loss: 0.4437\n",
      "Epoch 1, Step 7050, Train Loss: 0.5304, Val Loss: 0.4490\n",
      "Epoch 1, Step 7075, Train Loss: 0.5465, Val Loss: 0.4472\n",
      "Epoch 1, Step 7100, Train Loss: 0.5804, Val Loss: 0.4473\n",
      "Epoch 1, Step 7125, Train Loss: 0.5438, Val Loss: 0.4462\n",
      "Epoch 1, Step 7150, Train Loss: 0.4870, Val Loss: 0.4480\n",
      "Epoch 1, Step 7175, Train Loss: 0.5630, Val Loss: 0.4402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  39%|███▊      | 10774/27842 [1:12:03<2:05:01,  2.28it/s, train_loss=0.5758, val_loss=0.4138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 7200, Train Loss: 0.5183, Val Loss: 0.4402\n",
      "Epoch 1, Step 7225, Train Loss: 0.5087, Val Loss: 0.4422\n",
      "Epoch 1, Step 7250, Train Loss: 0.4952, Val Loss: 0.4448\n",
      "Epoch 1, Step 7275, Train Loss: 0.5628, Val Loss: 0.4485\n",
      "Epoch 1, Step 7300, Train Loss: 0.5548, Val Loss: 0.4460\n",
      "Epoch 1, Step 7325, Train Loss: 0.5406, Val Loss: 0.4447\n",
      "Epoch 1, Step 7350, Train Loss: 0.6115, Val Loss: 0.4495\n",
      "Epoch 1, Step 7375, Train Loss: 0.5309, Val Loss: 0.4508\n",
      "Epoch 1, Step 7400, Train Loss: 0.5052, Val Loss: 0.4511\n",
      "Epoch 1, Step 7425, Train Loss: 0.4987, Val Loss: 0.4480\n",
      "Epoch 1, Step 7450, Train Loss: 0.5547, Val Loss: 0.4490\n",
      "Epoch 1, Step 7475, Train Loss: 0.5191, Val Loss: 0.4520\n",
      "Epoch 1, Step 7500, Train Loss: 0.4967, Val Loss: 0.4492\n",
      "Epoch 1, Step 7525, Train Loss: 0.4457, Val Loss: 0.4520\n",
      "Epoch 1, Step 7550, Train Loss: 0.4453, Val Loss: 0.4522\n",
      "Epoch 1, Step 7575, Train Loss: 0.5985, Val Loss: 0.4480\n",
      "Epoch 1, Step 7600, Train Loss: 0.5466, Val Loss: 0.4549\n",
      "Epoch 1, Step 7625, Train Loss: 0.5000, Val Loss: 0.4559\n",
      "Epoch 1, Step 7650, Train Loss: 0.5631, Val Loss: 0.4589\n",
      "Epoch 1, Step 7675, Train Loss: 0.4754, Val Loss: 0.4617\n",
      "Epoch 1, Step 7700, Train Loss: 0.5754, Val Loss: 0.4619\n",
      "Epoch 1, Step 7725, Train Loss: 0.5567, Val Loss: 0.4594\n",
      "Epoch 1, Step 7750, Train Loss: 0.5066, Val Loss: 0.4533\n",
      "Epoch 1, Step 7775, Train Loss: 0.5409, Val Loss: 0.4501\n",
      "Epoch 1, Step 7800, Train Loss: 0.4239, Val Loss: 0.4522\n",
      "Epoch 1, Step 7825, Train Loss: 0.5046, Val Loss: 0.4487\n",
      "Epoch 1, Step 7850, Train Loss: 0.4554, Val Loss: 0.4514\n",
      "Epoch 1, Step 7875, Train Loss: 0.5711, Val Loss: 0.4520\n",
      "Epoch 1, Step 7900, Train Loss: 0.5491, Val Loss: 0.4561\n",
      "Epoch 1, Step 7925, Train Loss: 0.5487, Val Loss: 0.4532\n",
      "Epoch 1, Step 7950, Train Loss: 0.5480, Val Loss: 0.4517\n",
      "Epoch 1, Step 7975, Train Loss: 0.5474, Val Loss: 0.4547\n",
      "Epoch 1, Step 8000, Train Loss: 0.5624, Val Loss: 0.4560\n",
      "Epoch 1, Step 8025, Train Loss: 0.4978, Val Loss: 0.4588\n",
      "Epoch 1, Step 8050, Train Loss: 0.5495, Val Loss: 0.4586\n",
      "Epoch 1, Step 8075, Train Loss: 0.5332, Val Loss: 0.4471\n",
      "Epoch 1, Step 8100, Train Loss: 0.5746, Val Loss: 0.4432\n",
      "Epoch 1, Step 8125, Train Loss: 0.5886, Val Loss: 0.4412\n",
      "Epoch 1, Step 8150, Train Loss: 0.5045, Val Loss: 0.4404\n",
      "Epoch 1, Step 8175, Train Loss: 0.5492, Val Loss: 0.4387\n",
      "Epoch 1, Step 8200, Train Loss: 0.6149, Val Loss: 0.4440\n",
      "Epoch 1, Step 8225, Train Loss: 0.5801, Val Loss: 0.4438\n",
      "Epoch 1, Step 8250, Train Loss: 0.4950, Val Loss: 0.4406\n",
      "Epoch 1, Step 8275, Train Loss: 0.5174, Val Loss: 0.4430\n",
      "Epoch 1, Step 8300, Train Loss: 0.4581, Val Loss: 0.4459\n",
      "Epoch 1, Step 8325, Train Loss: 0.4456, Val Loss: 0.4463\n",
      "Epoch 1, Step 8350, Train Loss: 0.5337, Val Loss: 0.4452\n",
      "Epoch 1, Step 8375, Train Loss: 0.5759, Val Loss: 0.4446\n",
      "Epoch 1, Step 8400, Train Loss: 0.5545, Val Loss: 0.4470\n",
      "Epoch 1, Step 8425, Train Loss: 0.6040, Val Loss: 0.4480\n",
      "Epoch 1, Step 8450, Train Loss: 0.5093, Val Loss: 0.4424\n",
      "Epoch 1, Step 8475, Train Loss: 0.4685, Val Loss: 0.4423\n",
      "Epoch 1, Step 8500, Train Loss: 0.5243, Val Loss: 0.4421\n",
      "Epoch 1, Step 8525, Train Loss: 0.6707, Val Loss: 0.4422\n",
      "Epoch 1, Step 8550, Train Loss: 0.5716, Val Loss: 0.4427\n",
      "Epoch 1, Step 8575, Train Loss: 0.4673, Val Loss: 0.4380\n",
      "Epoch 1, Step 8600, Train Loss: 0.5738, Val Loss: 0.4401\n",
      "Epoch 1, Step 8625, Train Loss: 0.5801, Val Loss: 0.4392\n",
      "Epoch 1, Step 8650, Train Loss: 0.4574, Val Loss: 0.4421\n",
      "Epoch 1, Step 8675, Train Loss: 0.5467, Val Loss: 0.4405\n",
      "Epoch 1, Step 8700, Train Loss: 0.4491, Val Loss: 0.4348\n",
      "Epoch 1, Step 8725, Train Loss: 0.6024, Val Loss: 0.4310\n",
      "Epoch 1, Step 8750, Train Loss: 0.4940, Val Loss: 0.4346\n",
      "Epoch 1, Step 8775, Train Loss: 0.5614, Val Loss: 0.4320\n",
      "Epoch 1, Step 8800, Train Loss: 0.5481, Val Loss: 0.4336\n",
      "Epoch 1, Step 8825, Train Loss: 0.6071, Val Loss: 0.4337\n",
      "Epoch 1, Step 8850, Train Loss: 0.4470, Val Loss: 0.4332\n",
      "Epoch 1, Step 8875, Train Loss: 0.4981, Val Loss: 0.4318\n",
      "Epoch 1, Step 8900, Train Loss: 0.6030, Val Loss: 0.4325\n",
      "Epoch 1, Step 8925, Train Loss: 0.5356, Val Loss: 0.4332\n",
      "Epoch 1, Step 8950, Train Loss: 0.4773, Val Loss: 0.4344\n",
      "Epoch 1, Step 8975, Train Loss: 0.5069, Val Loss: 0.4393\n",
      "Epoch 1, Step 9000, Train Loss: 0.4266, Val Loss: 0.4385\n",
      "Epoch 1, Step 9025, Train Loss: 0.4891, Val Loss: 0.4311\n",
      "Epoch 1, Step 9050, Train Loss: 0.5226, Val Loss: 0.4304\n",
      "Epoch 1, Step 9075, Train Loss: 0.5385, Val Loss: 0.4366\n",
      "Epoch 1, Step 9100, Train Loss: 0.5118, Val Loss: 0.4357\n",
      "Epoch 1, Step 9125, Train Loss: 0.5402, Val Loss: 0.4360\n",
      "Epoch 1, Step 9150, Train Loss: 0.5473, Val Loss: 0.4367\n",
      "Epoch 1, Step 9175, Train Loss: 0.5391, Val Loss: 0.4383\n",
      "Epoch 1, Step 9200, Train Loss: 0.5028, Val Loss: 0.4369\n",
      "Epoch 1, Step 9225, Train Loss: 0.5227, Val Loss: 0.4411\n",
      "Epoch 1, Step 9250, Train Loss: 0.5430, Val Loss: 0.4439\n",
      "Epoch 1, Step 9275, Train Loss: 0.4595, Val Loss: 0.4450\n",
      "Epoch 1, Step 9300, Train Loss: 0.5729, Val Loss: 0.4483\n",
      "Epoch 1, Step 9325, Train Loss: 0.5357, Val Loss: 0.4500\n",
      "Epoch 1, Step 9350, Train Loss: 0.5496, Val Loss: 0.4483\n",
      "Epoch 1, Step 9375, Train Loss: 0.5426, Val Loss: 0.4438\n",
      "Epoch 1, Step 9400, Train Loss: 0.4851, Val Loss: 0.4468\n",
      "Epoch 1, Step 9425, Train Loss: 0.4594, Val Loss: 0.4495\n",
      "Epoch 1, Step 9450, Train Loss: 0.5031, Val Loss: 0.4489\n",
      "Epoch 1, Step 9475, Train Loss: 0.4525, Val Loss: 0.4487\n",
      "Epoch 1, Step 9500, Train Loss: 0.5430, Val Loss: 0.4450\n",
      "Epoch 1, Step 9525, Train Loss: 0.4746, Val Loss: 0.4426\n",
      "Epoch 1, Step 9550, Train Loss: 0.4590, Val Loss: 0.4445\n",
      "Epoch 1, Step 9575, Train Loss: 0.4430, Val Loss: 0.4452\n",
      "Epoch 1, Step 9600, Train Loss: 0.4990, Val Loss: 0.4426\n",
      "Epoch 1, Step 9625, Train Loss: 0.5452, Val Loss: 0.4246\n",
      "Epoch 1, Step 9650, Train Loss: 0.5509, Val Loss: 0.4259\n",
      "Epoch 1, Step 9675, Train Loss: 0.4826, Val Loss: 0.4246\n",
      "Epoch 1, Step 9700, Train Loss: 0.4801, Val Loss: 0.4221\n",
      "Epoch 1, Step 9725, Train Loss: 0.5390, Val Loss: 0.4213\n",
      "Epoch 1, Step 9750, Train Loss: 0.5279, Val Loss: 0.4211\n",
      "Epoch 1, Step 9775, Train Loss: 0.4808, Val Loss: 0.4225\n",
      "Epoch 1, Step 9800, Train Loss: 0.4708, Val Loss: 0.4222\n",
      "Epoch 1, Step 9825, Train Loss: 0.5116, Val Loss: 0.4216\n",
      "Epoch 1, Step 9850, Train Loss: 0.4954, Val Loss: 0.4217\n",
      "Epoch 1, Step 9875, Train Loss: 0.4926, Val Loss: 0.4188\n",
      "Epoch 1, Step 9900, Train Loss: 0.4697, Val Loss: 0.4185\n",
      "Epoch 1, Step 9925, Train Loss: 0.4844, Val Loss: 0.4206\n",
      "Epoch 1, Step 9950, Train Loss: 0.4443, Val Loss: 0.4179\n",
      "Epoch 1, Step 9975, Train Loss: 0.5616, Val Loss: 0.4206\n",
      "Epoch 1, Step 10000, Train Loss: 0.5213, Val Loss: 0.4236\n",
      "Epoch 1, Step 10025, Train Loss: 0.5071, Val Loss: 0.4280\n",
      "Epoch 1, Step 10050, Train Loss: 0.5132, Val Loss: 0.4197\n",
      "Epoch 1, Step 10075, Train Loss: 0.4947, Val Loss: 0.4219\n",
      "Epoch 1, Step 10100, Train Loss: 0.5008, Val Loss: 0.4209\n",
      "Epoch 1, Step 10125, Train Loss: 0.5076, Val Loss: 0.4239\n",
      "Epoch 1, Step 10150, Train Loss: 0.5581, Val Loss: 0.4272\n",
      "Epoch 1, Step 10175, Train Loss: 0.4059, Val Loss: 0.4290\n",
      "Epoch 1, Step 10200, Train Loss: 0.5059, Val Loss: 0.4246\n",
      "Epoch 1, Step 10225, Train Loss: 0.4444, Val Loss: 0.4255\n",
      "Epoch 1, Step 10250, Train Loss: 0.5272, Val Loss: 0.4250\n",
      "Epoch 1, Step 10275, Train Loss: 0.4860, Val Loss: 0.4267\n",
      "Epoch 1, Step 10300, Train Loss: 0.5791, Val Loss: 0.4281\n",
      "Epoch 1, Step 10325, Train Loss: 0.4587, Val Loss: 0.4282\n",
      "Epoch 1, Step 10350, Train Loss: 0.5021, Val Loss: 0.4286\n",
      "Epoch 1, Step 10375, Train Loss: 0.6140, Val Loss: 0.4299\n",
      "Epoch 1, Step 10400, Train Loss: 0.4849, Val Loss: 0.4295\n",
      "Epoch 1, Step 10425, Train Loss: 0.4732, Val Loss: 0.4323\n",
      "Epoch 1, Step 10450, Train Loss: 0.5146, Val Loss: 0.4295\n",
      "Epoch 1, Step 10475, Train Loss: 0.4395, Val Loss: 0.4290\n",
      "Epoch 1, Step 10500, Train Loss: 0.5608, Val Loss: 0.4329\n",
      "Epoch 1, Step 10525, Train Loss: 0.5386, Val Loss: 0.4344\n",
      "Epoch 1, Step 10550, Train Loss: 0.4770, Val Loss: 0.4317\n",
      "Epoch 1, Step 10575, Train Loss: 0.5308, Val Loss: 0.4310\n",
      "Epoch 1, Step 10600, Train Loss: 0.5106, Val Loss: 0.4114\n",
      "Epoch 1, Step 10625, Train Loss: 0.5696, Val Loss: 0.4143\n",
      "Epoch 1, Step 10650, Train Loss: 0.5184, Val Loss: 0.4146\n",
      "Epoch 1, Step 10675, Train Loss: 0.5615, Val Loss: 0.4166\n",
      "Epoch 1, Step 10700, Train Loss: 0.3949, Val Loss: 0.4064\n",
      "Epoch 1, Step 10725, Train Loss: 0.4895, Val Loss: 0.4121\n",
      "Epoch 1, Step 10750, Train Loss: 0.4864, Val Loss: 0.4122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████▏    | 14299/27842 [1:35:17<1:20:59,  2.79it/s, train_loss=0.3751, val_loss=0.3871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 10775, Train Loss: 0.5758, Val Loss: 0.4138\n",
      "Epoch 1, Step 10800, Train Loss: 0.5468, Val Loss: 0.4135\n",
      "Epoch 1, Step 10825, Train Loss: 0.4738, Val Loss: 0.4143\n",
      "Epoch 1, Step 10850, Train Loss: 0.5069, Val Loss: 0.4189\n",
      "Epoch 1, Step 10875, Train Loss: 0.5464, Val Loss: 0.4131\n",
      "Epoch 1, Step 10900, Train Loss: 0.4707, Val Loss: 0.4113\n",
      "Epoch 1, Step 10925, Train Loss: 0.5045, Val Loss: 0.4135\n",
      "Epoch 1, Step 10950, Train Loss: 0.6410, Val Loss: 0.4138\n",
      "Epoch 1, Step 10975, Train Loss: 0.4366, Val Loss: 0.4131\n",
      "Epoch 1, Step 11000, Train Loss: 0.4900, Val Loss: 0.4093\n",
      "Epoch 1, Step 11025, Train Loss: 0.5811, Val Loss: 0.4065\n",
      "Epoch 1, Step 11050, Train Loss: 0.5456, Val Loss: 0.4081\n",
      "Epoch 1, Step 11075, Train Loss: 0.5117, Val Loss: 0.4112\n",
      "Epoch 1, Step 11100, Train Loss: 0.4305, Val Loss: 0.4110\n",
      "Epoch 1, Step 11125, Train Loss: 0.5280, Val Loss: 0.4083\n",
      "Epoch 1, Step 11150, Train Loss: 0.4300, Val Loss: 0.4063\n",
      "Epoch 1, Step 11175, Train Loss: 0.6032, Val Loss: 0.4067\n",
      "Epoch 1, Step 11200, Train Loss: 0.5204, Val Loss: 0.4053\n",
      "Epoch 1, Step 11225, Train Loss: 0.5266, Val Loss: 0.4071\n",
      "Epoch 1, Step 11250, Train Loss: 0.5476, Val Loss: 0.4070\n",
      "Epoch 1, Step 11275, Train Loss: 0.5811, Val Loss: 0.4107\n",
      "Epoch 1, Step 11300, Train Loss: 0.4995, Val Loss: 0.4108\n",
      "Epoch 1, Step 11325, Train Loss: 0.5164, Val Loss: 0.4090\n",
      "Epoch 1, Step 11350, Train Loss: 0.4821, Val Loss: 0.4063\n",
      "Epoch 1, Step 11375, Train Loss: 0.4301, Val Loss: 0.4111\n",
      "Epoch 1, Step 11400, Train Loss: 0.4701, Val Loss: 0.4116\n",
      "Epoch 1, Step 11425, Train Loss: 0.4343, Val Loss: 0.4088\n",
      "Epoch 1, Step 11450, Train Loss: 0.5157, Val Loss: 0.4063\n",
      "Epoch 1, Step 11475, Train Loss: 0.4219, Val Loss: 0.4063\n",
      "Epoch 1, Step 11500, Train Loss: 0.4696, Val Loss: 0.4093\n",
      "Epoch 1, Step 11525, Train Loss: 0.4372, Val Loss: 0.4049\n",
      "Epoch 1, Step 11550, Train Loss: 0.4333, Val Loss: 0.4067\n",
      "Epoch 1, Step 11575, Train Loss: 0.5548, Val Loss: 0.4067\n",
      "Epoch 1, Step 11600, Train Loss: 0.4769, Val Loss: 0.4058\n",
      "Epoch 1, Step 11625, Train Loss: 0.4740, Val Loss: 0.4094\n",
      "Epoch 1, Step 11650, Train Loss: 0.4451, Val Loss: 0.4098\n",
      "Epoch 1, Step 11675, Train Loss: 0.6040, Val Loss: 0.4112\n",
      "Epoch 1, Step 11700, Train Loss: 0.4718, Val Loss: 0.4168\n",
      "Epoch 1, Step 11725, Train Loss: 0.4734, Val Loss: 0.4144\n",
      "Epoch 1, Step 11750, Train Loss: 0.5187, Val Loss: 0.4136\n",
      "Epoch 1, Step 11775, Train Loss: 0.4625, Val Loss: 0.4107\n",
      "Epoch 1, Step 11800, Train Loss: 0.5635, Val Loss: 0.4156\n",
      "Epoch 1, Step 11825, Train Loss: 0.4926, Val Loss: 0.4138\n",
      "Epoch 1, Step 11850, Train Loss: 0.5230, Val Loss: 0.4136\n",
      "Epoch 1, Step 11875, Train Loss: 0.5913, Val Loss: 0.4128\n",
      "Epoch 1, Step 11900, Train Loss: 0.4175, Val Loss: 0.4138\n",
      "Epoch 1, Step 11925, Train Loss: 0.5435, Val Loss: 0.4096\n",
      "Epoch 1, Step 11950, Train Loss: 0.4609, Val Loss: 0.4115\n",
      "Epoch 1, Step 11975, Train Loss: 0.5419, Val Loss: 0.4147\n",
      "Epoch 1, Step 12000, Train Loss: 0.6028, Val Loss: 0.4145\n",
      "Epoch 1, Step 12025, Train Loss: 0.5049, Val Loss: 0.4102\n",
      "Epoch 1, Step 12050, Train Loss: 0.4226, Val Loss: 0.4126\n",
      "Epoch 1, Step 12075, Train Loss: 0.5602, Val Loss: 0.4131\n",
      "Epoch 1, Step 12100, Train Loss: 0.4994, Val Loss: 0.4123\n",
      "Epoch 1, Step 12125, Train Loss: 0.5311, Val Loss: 0.4154\n",
      "Epoch 1, Step 12150, Train Loss: 0.5259, Val Loss: 0.4126\n",
      "Epoch 1, Step 12175, Train Loss: 0.5644, Val Loss: 0.4106\n",
      "Epoch 1, Step 12200, Train Loss: 0.6542, Val Loss: 0.4123\n",
      "Epoch 1, Step 12225, Train Loss: 0.6274, Val Loss: 0.4102\n",
      "Epoch 1, Step 12250, Train Loss: 0.4353, Val Loss: 0.4133\n",
      "Epoch 1, Step 12275, Train Loss: 0.5672, Val Loss: 0.4114\n",
      "Epoch 1, Step 12300, Train Loss: 0.5939, Val Loss: 0.4150\n",
      "Epoch 1, Step 12325, Train Loss: 0.4457, Val Loss: 0.4124\n",
      "Epoch 1, Step 12350, Train Loss: 0.5394, Val Loss: 0.4120\n",
      "Epoch 1, Step 12375, Train Loss: 0.5011, Val Loss: 0.4123\n",
      "Epoch 1, Step 12400, Train Loss: 0.4798, Val Loss: 0.4156\n",
      "Epoch 1, Step 12425, Train Loss: 0.5148, Val Loss: 0.4156\n",
      "Epoch 1, Step 12450, Train Loss: 0.4421, Val Loss: 0.4158\n",
      "Epoch 1, Step 12475, Train Loss: 0.4363, Val Loss: 0.4155\n",
      "Epoch 1, Step 12500, Train Loss: 0.4647, Val Loss: 0.4158\n",
      "Epoch 1, Step 12525, Train Loss: 0.4707, Val Loss: 0.4156\n",
      "Epoch 1, Step 12550, Train Loss: 0.5821, Val Loss: 0.4167\n",
      "Epoch 1, Step 12575, Train Loss: 0.5468, Val Loss: 0.4162\n",
      "Epoch 1, Step 12600, Train Loss: 0.5353, Val Loss: 0.4169\n",
      "Epoch 1, Step 12625, Train Loss: 0.4545, Val Loss: 0.4177\n",
      "Epoch 1, Step 12650, Train Loss: 0.4002, Val Loss: 0.4156\n",
      "Epoch 1, Step 12675, Train Loss: 0.5226, Val Loss: 0.4148\n",
      "Epoch 1, Step 12700, Train Loss: 0.5151, Val Loss: 0.4136\n",
      "Epoch 1, Step 12725, Train Loss: 0.4487, Val Loss: 0.4110\n",
      "Epoch 1, Step 12750, Train Loss: 0.4802, Val Loss: 0.4174\n",
      "Epoch 1, Step 12775, Train Loss: 0.4884, Val Loss: 0.4130\n",
      "Epoch 1, Step 12800, Train Loss: 0.4470, Val Loss: 0.4106\n",
      "Epoch 1, Step 12825, Train Loss: 0.4804, Val Loss: 0.4112\n",
      "Epoch 1, Step 12850, Train Loss: 0.4820, Val Loss: 0.4098\n",
      "Epoch 1, Step 12875, Train Loss: 0.4647, Val Loss: 0.4123\n",
      "Epoch 1, Step 12900, Train Loss: 0.5362, Val Loss: 0.4161\n",
      "Epoch 1, Step 12925, Train Loss: 0.4586, Val Loss: 0.4148\n",
      "Epoch 1, Step 12950, Train Loss: 0.5245, Val Loss: 0.4157\n",
      "Epoch 1, Step 12975, Train Loss: 0.4982, Val Loss: 0.4149\n",
      "Epoch 1, Step 13000, Train Loss: 0.5175, Val Loss: 0.4122\n",
      "Epoch 1, Step 13025, Train Loss: 0.4931, Val Loss: 0.4129\n",
      "Epoch 1, Step 13050, Train Loss: 0.4403, Val Loss: 0.4125\n",
      "Epoch 1, Step 13075, Train Loss: 0.5147, Val Loss: 0.4120\n",
      "Epoch 1, Step 13100, Train Loss: 0.5446, Val Loss: 0.4047\n",
      "Epoch 1, Step 13125, Train Loss: 0.4985, Val Loss: 0.4020\n",
      "Epoch 1, Step 13150, Train Loss: 0.5201, Val Loss: 0.3993\n",
      "Epoch 1, Step 13175, Train Loss: 0.4857, Val Loss: 0.3989\n",
      "Epoch 1, Step 13200, Train Loss: 0.4588, Val Loss: 0.4012\n",
      "Epoch 1, Step 13225, Train Loss: 0.5061, Val Loss: 0.3989\n",
      "Epoch 1, Step 13250, Train Loss: 0.4683, Val Loss: 0.4032\n",
      "Epoch 1, Step 13275, Train Loss: 0.4738, Val Loss: 0.4024\n",
      "Epoch 1, Step 13300, Train Loss: 0.4085, Val Loss: 0.4012\n",
      "Epoch 1, Step 13325, Train Loss: 0.5423, Val Loss: 0.4013\n",
      "Epoch 1, Step 13350, Train Loss: 0.4586, Val Loss: 0.4044\n",
      "Epoch 1, Step 13375, Train Loss: 0.5224, Val Loss: 0.4029\n",
      "Epoch 1, Step 13400, Train Loss: 0.5545, Val Loss: 0.4028\n",
      "Epoch 1, Step 13425, Train Loss: 0.4233, Val Loss: 0.4036\n",
      "Epoch 1, Step 13450, Train Loss: 0.4988, Val Loss: 0.4074\n",
      "Epoch 1, Step 13475, Train Loss: 0.5547, Val Loss: 0.4040\n",
      "Epoch 1, Step 13500, Train Loss: 0.5558, Val Loss: 0.4038\n",
      "Epoch 1, Step 13525, Train Loss: 0.4450, Val Loss: 0.4074\n",
      "Epoch 1, Step 13550, Train Loss: 0.5080, Val Loss: 0.4073\n",
      "Epoch 1, Step 13575, Train Loss: 0.4815, Val Loss: 0.4027\n",
      "Epoch 1, Step 13600, Train Loss: 0.4878, Val Loss: 0.4052\n",
      "Epoch 1, Step 13625, Train Loss: 0.5441, Val Loss: 0.4042\n",
      "Epoch 1, Step 13650, Train Loss: 0.4553, Val Loss: 0.4043\n",
      "Epoch 1, Step 13675, Train Loss: 0.4372, Val Loss: 0.4039\n",
      "Epoch 1, Step 13700, Train Loss: 0.4839, Val Loss: 0.4028\n",
      "Epoch 1, Step 13725, Train Loss: 0.5375, Val Loss: 0.4066\n",
      "Epoch 1, Step 13750, Train Loss: 0.4223, Val Loss: 0.4062\n",
      "Epoch 1, Step 13775, Train Loss: 0.4595, Val Loss: 0.4070\n",
      "Epoch 1, Step 13800, Train Loss: 0.4432, Val Loss: 0.4073\n",
      "Epoch 1, Step 13825, Train Loss: 0.5330, Val Loss: 0.4021\n",
      "Epoch 1, Step 13850, Train Loss: 0.5480, Val Loss: 0.4026\n",
      "Epoch 1, Step 13875, Train Loss: 0.4718, Val Loss: 0.4033\n",
      "Epoch 1, Step 13900, Train Loss: 0.5042, Val Loss: 0.4022\n",
      "Epoch 1, Step 13925, Train Loss: 0.4570, Val Loss: 0.4018\n",
      "Epoch 1, Step 13950, Train Loss: 0.5120, Val Loss: 0.4014\n",
      "Epoch 1, Step 13975, Train Loss: 0.5130, Val Loss: 0.3894\n",
      "Epoch 1, Step 14000, Train Loss: 0.5052, Val Loss: 0.3813\n",
      "Epoch 1, Step 14025, Train Loss: 0.4756, Val Loss: 0.3834\n",
      "Epoch 1, Step 14050, Train Loss: 0.4593, Val Loss: 0.3809\n",
      "Epoch 1, Step 14075, Train Loss: 0.4467, Val Loss: 0.3829\n",
      "Epoch 1, Step 14100, Train Loss: 0.4982, Val Loss: 0.3856\n",
      "Epoch 1, Step 14125, Train Loss: 0.5331, Val Loss: 0.3818\n",
      "Epoch 1, Step 14150, Train Loss: 0.4254, Val Loss: 0.3793\n",
      "Epoch 1, Step 14175, Train Loss: 0.5255, Val Loss: 0.3793\n",
      "Epoch 1, Step 14200, Train Loss: 0.4847, Val Loss: 0.3801\n",
      "Epoch 1, Step 14225, Train Loss: 0.5280, Val Loss: 0.3827\n",
      "Epoch 1, Step 14250, Train Loss: 0.4591, Val Loss: 0.3840\n",
      "Epoch 1, Step 14275, Train Loss: 0.4332, Val Loss: 0.3837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 17824/27842 [1:58:32<1:21:45,  2.04it/s, train_loss=0.4293, val_loss=0.3791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 14300, Train Loss: 0.3751, Val Loss: 0.3871\n",
      "Epoch 1, Step 14325, Train Loss: 0.5348, Val Loss: 0.3862\n",
      "Epoch 1, Step 14350, Train Loss: 0.5110, Val Loss: 0.3861\n",
      "Epoch 1, Step 14375, Train Loss: 0.4859, Val Loss: 0.3857\n",
      "Epoch 1, Step 14400, Train Loss: 0.5200, Val Loss: 0.3864\n",
      "Epoch 1, Step 14425, Train Loss: 0.3757, Val Loss: 0.3852\n",
      "Epoch 1, Step 14450, Train Loss: 0.4236, Val Loss: 0.3825\n",
      "Epoch 1, Step 14475, Train Loss: 0.4288, Val Loss: 0.3859\n",
      "Epoch 1, Step 14500, Train Loss: 0.5482, Val Loss: 0.3836\n",
      "Epoch 1, Step 14525, Train Loss: 0.5003, Val Loss: 0.3829\n",
      "Epoch 1, Step 14550, Train Loss: 0.4442, Val Loss: 0.3867\n",
      "Epoch 1, Step 14575, Train Loss: 0.5036, Val Loss: 0.3847\n",
      "Epoch 1, Step 14600, Train Loss: 0.4942, Val Loss: 0.3842\n",
      "Epoch 1, Step 14625, Train Loss: 0.4274, Val Loss: 0.3828\n",
      "Epoch 1, Step 14650, Train Loss: 0.3947, Val Loss: 0.3838\n",
      "Epoch 1, Step 14675, Train Loss: 0.4672, Val Loss: 0.3868\n",
      "Epoch 1, Step 14700, Train Loss: 0.4569, Val Loss: 0.3898\n",
      "Epoch 1, Step 14725, Train Loss: 0.5798, Val Loss: 0.3886\n",
      "Epoch 1, Step 14750, Train Loss: 0.4758, Val Loss: 0.3876\n",
      "Epoch 1, Step 14775, Train Loss: 0.5218, Val Loss: 0.3852\n",
      "Epoch 1, Step 14800, Train Loss: 0.5131, Val Loss: 0.3867\n",
      "Epoch 1, Step 14825, Train Loss: 0.4391, Val Loss: 0.3795\n",
      "Epoch 1, Step 14850, Train Loss: 0.4988, Val Loss: 0.3815\n",
      "Epoch 1, Step 14875, Train Loss: 0.5518, Val Loss: 0.3849\n",
      "Epoch 1, Step 14900, Train Loss: 0.4726, Val Loss: 0.3858\n",
      "Epoch 1, Step 14925, Train Loss: 0.5416, Val Loss: 0.3837\n",
      "Epoch 1, Step 14950, Train Loss: 0.4053, Val Loss: 0.3853\n",
      "Epoch 1, Step 14975, Train Loss: 0.5484, Val Loss: 0.3855\n",
      "Epoch 1, Step 15000, Train Loss: 0.5165, Val Loss: 0.3842\n",
      "Epoch 1, Step 15025, Train Loss: 0.5183, Val Loss: 0.3852\n",
      "Epoch 1, Step 15050, Train Loss: 0.4373, Val Loss: 0.3831\n",
      "Epoch 1, Step 15075, Train Loss: 0.4681, Val Loss: 0.3817\n",
      "Epoch 1, Step 15100, Train Loss: 0.4217, Val Loss: 0.3783\n",
      "Epoch 1, Step 15125, Train Loss: 0.4951, Val Loss: 0.3799\n",
      "Epoch 1, Step 15150, Train Loss: 0.5463, Val Loss: 0.3827\n",
      "Epoch 1, Step 15175, Train Loss: 0.4755, Val Loss: 0.3802\n",
      "Epoch 1, Step 15200, Train Loss: 0.5054, Val Loss: 0.3798\n",
      "Epoch 1, Step 15225, Train Loss: 0.5097, Val Loss: 0.3818\n",
      "Epoch 1, Step 15250, Train Loss: 0.4880, Val Loss: 0.3831\n",
      "Epoch 1, Step 15275, Train Loss: 0.4113, Val Loss: 0.3838\n",
      "Epoch 1, Step 15300, Train Loss: 0.5059, Val Loss: 0.3835\n",
      "Epoch 1, Step 15325, Train Loss: 0.4193, Val Loss: 0.3823\n",
      "Epoch 1, Step 15350, Train Loss: 0.5266, Val Loss: 0.3874\n",
      "Epoch 1, Step 15375, Train Loss: 0.4074, Val Loss: 0.3856\n",
      "Epoch 1, Step 15400, Train Loss: 0.4640, Val Loss: 0.3832\n",
      "Epoch 1, Step 15425, Train Loss: 0.4740, Val Loss: 0.3830\n",
      "Epoch 1, Step 15450, Train Loss: 0.4838, Val Loss: 0.3825\n",
      "Epoch 1, Step 15475, Train Loss: 0.4711, Val Loss: 0.3812\n",
      "Epoch 1, Step 15500, Train Loss: 0.4886, Val Loss: 0.3835\n",
      "Epoch 1, Step 15525, Train Loss: 0.5089, Val Loss: 0.3813\n",
      "Epoch 1, Step 15550, Train Loss: 0.5490, Val Loss: 0.3841\n",
      "Epoch 1, Step 15575, Train Loss: 0.4487, Val Loss: 0.3847\n",
      "Epoch 1, Step 15600, Train Loss: 0.4840, Val Loss: 0.3848\n",
      "Epoch 1, Step 15625, Train Loss: 0.5406, Val Loss: 0.3852\n",
      "Epoch 1, Step 15650, Train Loss: 0.4415, Val Loss: 0.3837\n",
      "Epoch 1, Step 15675, Train Loss: 0.4600, Val Loss: 0.3824\n",
      "Epoch 1, Step 15700, Train Loss: 0.4555, Val Loss: 0.3835\n",
      "Epoch 1, Step 15725, Train Loss: 0.3848, Val Loss: 0.3836\n",
      "Epoch 1, Step 15750, Train Loss: 0.5046, Val Loss: 0.3841\n",
      "Epoch 1, Step 15775, Train Loss: 0.5224, Val Loss: 0.3833\n",
      "Epoch 1, Step 15800, Train Loss: 0.5704, Val Loss: 0.3829\n",
      "Epoch 1, Step 15825, Train Loss: 0.4332, Val Loss: 0.3835\n",
      "Epoch 1, Step 15850, Train Loss: 0.4630, Val Loss: 0.3813\n",
      "Epoch 1, Step 15875, Train Loss: 0.5400, Val Loss: 0.3799\n",
      "Epoch 1, Step 15900, Train Loss: 0.4894, Val Loss: 0.3793\n",
      "Epoch 1, Step 15925, Train Loss: 0.4757, Val Loss: 0.3814\n",
      "Epoch 1, Step 15950, Train Loss: 0.4873, Val Loss: 0.3830\n",
      "Epoch 1, Step 15975, Train Loss: 0.4295, Val Loss: 0.3827\n",
      "Epoch 1, Step 16000, Train Loss: 0.4862, Val Loss: 0.3878\n",
      "Epoch 1, Step 16025, Train Loss: 0.4581, Val Loss: 0.3849\n",
      "Epoch 1, Step 16050, Train Loss: 0.4813, Val Loss: 0.3837\n",
      "Epoch 1, Step 16075, Train Loss: 0.4419, Val Loss: 0.3831\n",
      "Epoch 1, Step 16100, Train Loss: 0.4760, Val Loss: 0.3851\n",
      "Epoch 1, Step 16125, Train Loss: 0.5220, Val Loss: 0.3860\n",
      "Epoch 1, Step 16150, Train Loss: 0.4788, Val Loss: 0.3864\n",
      "Epoch 1, Step 16175, Train Loss: 0.5425, Val Loss: 0.3859\n",
      "Epoch 1, Step 16200, Train Loss: 0.4238, Val Loss: 0.3876\n",
      "Epoch 1, Step 16225, Train Loss: 0.4413, Val Loss: 0.3855\n",
      "Epoch 1, Step 16250, Train Loss: 0.4683, Val Loss: 0.3838\n",
      "Epoch 1, Step 16275, Train Loss: 0.4095, Val Loss: 0.3837\n",
      "Epoch 1, Step 16300, Train Loss: 0.3951, Val Loss: 0.3836\n",
      "Epoch 1, Step 16325, Train Loss: 0.4289, Val Loss: 0.3817\n",
      "Epoch 1, Step 16350, Train Loss: 0.4969, Val Loss: 0.3795\n",
      "Epoch 1, Step 16375, Train Loss: 0.5484, Val Loss: 0.3802\n",
      "Epoch 1, Step 16400, Train Loss: 0.4570, Val Loss: 0.3820\n",
      "Epoch 1, Step 16425, Train Loss: 0.4720, Val Loss: 0.3795\n",
      "Epoch 1, Step 16450, Train Loss: 0.4627, Val Loss: 0.3817\n",
      "Epoch 1, Step 16475, Train Loss: 0.4737, Val Loss: 0.3798\n",
      "Epoch 1, Step 16500, Train Loss: 0.6019, Val Loss: 0.3833\n",
      "Epoch 1, Step 16525, Train Loss: 0.5361, Val Loss: 0.3842\n",
      "Epoch 1, Step 16550, Train Loss: 0.4426, Val Loss: 0.3801\n",
      "Epoch 1, Step 16575, Train Loss: 0.4954, Val Loss: 0.3800\n",
      "Epoch 1, Step 16600, Train Loss: 0.4936, Val Loss: 0.3775\n",
      "Epoch 1, Step 16625, Train Loss: 0.4787, Val Loss: 0.3793\n",
      "Epoch 1, Step 16650, Train Loss: 0.4536, Val Loss: 0.3801\n",
      "Epoch 1, Step 16675, Train Loss: 0.3694, Val Loss: 0.3815\n",
      "Epoch 1, Step 16700, Train Loss: 0.5496, Val Loss: 0.3807\n",
      "Epoch 1, Step 16725, Train Loss: 0.4317, Val Loss: 0.3808\n",
      "Epoch 1, Step 16750, Train Loss: 0.5092, Val Loss: 0.3845\n",
      "Epoch 1, Step 16775, Train Loss: 0.4857, Val Loss: 0.3835\n",
      "Epoch 1, Step 16800, Train Loss: 0.5386, Val Loss: 0.3841\n",
      "Epoch 1, Step 16825, Train Loss: 0.4459, Val Loss: 0.3846\n",
      "Epoch 1, Step 16850, Train Loss: 0.4773, Val Loss: 0.3858\n",
      "Epoch 1, Step 16875, Train Loss: 0.4739, Val Loss: 0.3859\n",
      "Epoch 1, Step 16900, Train Loss: 0.4902, Val Loss: 0.3866\n",
      "Epoch 1, Step 16925, Train Loss: 0.4683, Val Loss: 0.3866\n",
      "Epoch 1, Step 16950, Train Loss: 0.4184, Val Loss: 0.3857\n",
      "Epoch 1, Step 16975, Train Loss: 0.4732, Val Loss: 0.3848\n",
      "Epoch 1, Step 17000, Train Loss: 0.5587, Val Loss: 0.3873\n",
      "Epoch 1, Step 17025, Train Loss: 0.5354, Val Loss: 0.3870\n",
      "Epoch 1, Step 17050, Train Loss: 0.4784, Val Loss: 0.3851\n",
      "Epoch 1, Step 17075, Train Loss: 0.5371, Val Loss: 0.3851\n",
      "Epoch 1, Step 17100, Train Loss: 0.4773, Val Loss: 0.3848\n",
      "Epoch 1, Step 17125, Train Loss: 0.4636, Val Loss: 0.3837\n",
      "Epoch 1, Step 17150, Train Loss: 0.5568, Val Loss: 0.3820\n",
      "Epoch 1, Step 17175, Train Loss: 0.4581, Val Loss: 0.3830\n",
      "Epoch 1, Step 17200, Train Loss: 0.4966, Val Loss: 0.3831\n",
      "Epoch 1, Step 17225, Train Loss: 0.4540, Val Loss: 0.3831\n",
      "Epoch 1, Step 17250, Train Loss: 0.5095, Val Loss: 0.3831\n",
      "Epoch 1, Step 17275, Train Loss: 0.5607, Val Loss: 0.3832\n",
      "Epoch 1, Step 17300, Train Loss: 0.5938, Val Loss: 0.3841\n",
      "Epoch 1, Step 17325, Train Loss: 0.4255, Val Loss: 0.3832\n",
      "Epoch 1, Step 17350, Train Loss: 0.4625, Val Loss: 0.3808\n",
      "Epoch 1, Step 17375, Train Loss: 0.4683, Val Loss: 0.3801\n",
      "Epoch 1, Step 17400, Train Loss: 0.4877, Val Loss: 0.3819\n",
      "Epoch 1, Step 17425, Train Loss: 0.4435, Val Loss: 0.3811\n",
      "Epoch 1, Step 17450, Train Loss: 0.4260, Val Loss: 0.3811\n",
      "Epoch 1, Step 17475, Train Loss: 0.4363, Val Loss: 0.3808\n",
      "Epoch 1, Step 17500, Train Loss: 0.5243, Val Loss: 0.3806\n",
      "Epoch 1, Step 17525, Train Loss: 0.5194, Val Loss: 0.3813\n",
      "Epoch 1, Step 17550, Train Loss: 0.5782, Val Loss: 0.3817\n",
      "Epoch 1, Step 17575, Train Loss: 0.4885, Val Loss: 0.3807\n",
      "Epoch 1, Step 17600, Train Loss: 0.5176, Val Loss: 0.3812\n",
      "Epoch 1, Step 17625, Train Loss: 0.4495, Val Loss: 0.3834\n",
      "Epoch 1, Step 17650, Train Loss: 0.5333, Val Loss: 0.3828\n",
      "Epoch 1, Step 17675, Train Loss: 0.4959, Val Loss: 0.3853\n",
      "Epoch 1, Step 17700, Train Loss: 0.4587, Val Loss: 0.3848\n",
      "Epoch 1, Step 17725, Train Loss: 0.4538, Val Loss: 0.3816\n",
      "Epoch 1, Step 17750, Train Loss: 0.5007, Val Loss: 0.3835\n",
      "Epoch 1, Step 17775, Train Loss: 0.4005, Val Loss: 0.3834\n",
      "Epoch 1, Step 17800, Train Loss: 0.4057, Val Loss: 0.3814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 21349/27842 [2:21:58<43:31,  2.49it/s, train_loss=0.4529, val_loss=0.3660]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 17825, Train Loss: 0.4293, Val Loss: 0.3791\n",
      "Epoch 1, Step 17850, Train Loss: 0.4519, Val Loss: 0.3810\n",
      "Epoch 1, Step 17875, Train Loss: 0.4122, Val Loss: 0.3797\n",
      "Epoch 1, Step 17900, Train Loss: 0.3744, Val Loss: 0.3774\n",
      "Epoch 1, Step 17925, Train Loss: 0.4608, Val Loss: 0.3774\n",
      "Epoch 1, Step 17950, Train Loss: 0.4310, Val Loss: 0.3785\n",
      "Epoch 1, Step 17975, Train Loss: 0.5016, Val Loss: 0.3797\n",
      "Epoch 1, Step 18000, Train Loss: 0.6364, Val Loss: 0.3791\n",
      "Epoch 1, Step 18025, Train Loss: 0.4526, Val Loss: 0.3800\n",
      "Epoch 1, Step 18050, Train Loss: 0.5617, Val Loss: 0.3791\n",
      "Epoch 1, Step 18075, Train Loss: 0.5193, Val Loss: 0.3790\n",
      "Epoch 1, Step 18100, Train Loss: 0.5010, Val Loss: 0.3793\n",
      "Epoch 1, Step 18125, Train Loss: 0.4595, Val Loss: 0.3805\n",
      "Epoch 1, Step 18150, Train Loss: 0.5279, Val Loss: 0.3807\n",
      "Epoch 1, Step 18175, Train Loss: 0.4999, Val Loss: 0.3803\n",
      "Epoch 1, Step 18200, Train Loss: 0.4554, Val Loss: 0.3812\n",
      "Epoch 1, Step 18225, Train Loss: 0.4755, Val Loss: 0.3807\n",
      "Epoch 1, Step 18250, Train Loss: 0.4510, Val Loss: 0.3798\n",
      "Epoch 1, Step 18275, Train Loss: 0.4084, Val Loss: 0.3798\n",
      "Epoch 1, Step 18300, Train Loss: 0.4771, Val Loss: 0.3794\n",
      "Epoch 1, Step 18325, Train Loss: 0.4345, Val Loss: 0.3788\n",
      "Epoch 1, Step 18350, Train Loss: 0.4570, Val Loss: 0.3810\n",
      "Epoch 1, Step 18375, Train Loss: 0.4758, Val Loss: 0.3798\n",
      "Epoch 1, Step 18400, Train Loss: 0.4625, Val Loss: 0.3811\n",
      "Epoch 1, Step 18425, Train Loss: 0.4272, Val Loss: 0.3812\n",
      "Epoch 1, Step 18450, Train Loss: 0.4815, Val Loss: 0.3791\n",
      "Epoch 1, Step 18475, Train Loss: 0.4141, Val Loss: 0.3782\n",
      "Epoch 1, Step 18500, Train Loss: 0.4652, Val Loss: 0.3788\n",
      "Epoch 1, Step 18525, Train Loss: 0.4517, Val Loss: 0.3774\n",
      "Epoch 1, Step 18550, Train Loss: 0.4805, Val Loss: 0.3762\n",
      "Epoch 1, Step 18575, Train Loss: 0.5128, Val Loss: 0.3781\n",
      "Epoch 1, Step 18600, Train Loss: 0.4721, Val Loss: 0.3782\n",
      "Epoch 1, Step 18625, Train Loss: 0.4033, Val Loss: 0.3772\n",
      "Epoch 1, Step 18650, Train Loss: 0.4562, Val Loss: 0.3768\n",
      "Epoch 1, Step 18675, Train Loss: 0.4484, Val Loss: 0.3761\n",
      "Epoch 1, Step 18700, Train Loss: 0.3922, Val Loss: 0.3757\n",
      "Epoch 1, Step 18725, Train Loss: 0.4243, Val Loss: 0.3758\n",
      "Epoch 1, Step 18750, Train Loss: 0.4060, Val Loss: 0.3765\n",
      "Epoch 1, Step 18775, Train Loss: 0.5333, Val Loss: 0.3776\n",
      "Epoch 1, Step 18800, Train Loss: 0.3899, Val Loss: 0.3776\n",
      "Epoch 1, Step 18825, Train Loss: 0.4578, Val Loss: 0.3780\n",
      "Epoch 1, Step 18850, Train Loss: 0.4287, Val Loss: 0.3787\n",
      "Epoch 1, Step 18875, Train Loss: 0.5096, Val Loss: 0.3790\n",
      "Epoch 1, Step 18900, Train Loss: 0.4506, Val Loss: 0.3775\n",
      "Epoch 1, Step 18925, Train Loss: 0.4846, Val Loss: 0.3777\n",
      "Epoch 1, Step 18950, Train Loss: 0.5385, Val Loss: 0.3787\n",
      "Epoch 1, Step 18975, Train Loss: 0.4524, Val Loss: 0.3798\n",
      "Epoch 1, Step 19000, Train Loss: 0.5122, Val Loss: 0.3802\n",
      "Epoch 1, Step 19025, Train Loss: 0.4766, Val Loss: 0.3793\n",
      "Epoch 1, Step 19050, Train Loss: 0.4271, Val Loss: 0.3787\n",
      "Epoch 1, Step 19075, Train Loss: 0.4408, Val Loss: 0.3776\n",
      "Epoch 1, Step 19100, Train Loss: 0.4785, Val Loss: 0.3775\n",
      "Epoch 1, Step 19125, Train Loss: 0.4958, Val Loss: 0.3764\n",
      "Epoch 1, Step 19150, Train Loss: 0.3926, Val Loss: 0.3757\n",
      "Epoch 1, Step 19175, Train Loss: 0.4756, Val Loss: 0.3766\n",
      "Epoch 1, Step 19200, Train Loss: 0.4632, Val Loss: 0.3791\n",
      "Epoch 1, Step 19225, Train Loss: 0.5291, Val Loss: 0.3776\n",
      "Epoch 1, Step 19250, Train Loss: 0.4790, Val Loss: 0.3762\n",
      "Epoch 1, Step 19275, Train Loss: 0.4144, Val Loss: 0.3775\n",
      "Epoch 1, Step 19300, Train Loss: 0.5706, Val Loss: 0.3774\n",
      "Epoch 1, Step 19325, Train Loss: 0.4242, Val Loss: 0.3779\n",
      "Epoch 1, Step 19350, Train Loss: 0.4200, Val Loss: 0.3778\n",
      "Epoch 1, Step 19375, Train Loss: 0.5388, Val Loss: 0.3780\n",
      "Epoch 1, Step 19400, Train Loss: 0.5176, Val Loss: 0.3794\n",
      "Epoch 1, Step 19425, Train Loss: 0.4602, Val Loss: 0.3799\n",
      "Epoch 1, Step 19450, Train Loss: 0.4504, Val Loss: 0.3786\n",
      "Epoch 1, Step 19475, Train Loss: 0.4115, Val Loss: 0.3778\n",
      "Epoch 1, Step 19500, Train Loss: 0.4164, Val Loss: 0.3777\n",
      "Epoch 1, Step 19525, Train Loss: 0.4194, Val Loss: 0.3775\n",
      "Epoch 1, Step 19550, Train Loss: 0.5146, Val Loss: 0.3773\n",
      "Epoch 1, Step 19575, Train Loss: 0.4747, Val Loss: 0.3754\n",
      "Epoch 1, Step 19600, Train Loss: 0.4656, Val Loss: 0.3742\n",
      "Epoch 1, Step 19625, Train Loss: 0.4294, Val Loss: 0.3740\n",
      "Epoch 1, Step 19650, Train Loss: 0.5631, Val Loss: 0.3743\n",
      "Epoch 1, Step 19675, Train Loss: 0.5340, Val Loss: 0.3761\n",
      "Epoch 1, Step 19700, Train Loss: 0.4402, Val Loss: 0.3688\n",
      "Epoch 1, Step 19725, Train Loss: 0.4484, Val Loss: 0.3666\n",
      "Epoch 1, Step 19750, Train Loss: 0.5189, Val Loss: 0.3677\n",
      "Epoch 1, Step 19775, Train Loss: 0.4799, Val Loss: 0.3674\n",
      "Epoch 1, Step 19800, Train Loss: 0.4366, Val Loss: 0.3677\n",
      "Epoch 1, Step 19825, Train Loss: 0.4697, Val Loss: 0.3678\n",
      "Epoch 1, Step 19850, Train Loss: 0.4182, Val Loss: 0.3683\n",
      "Epoch 1, Step 19875, Train Loss: 0.4916, Val Loss: 0.3686\n",
      "Epoch 1, Step 19900, Train Loss: 0.6180, Val Loss: 0.3676\n",
      "Epoch 1, Step 19925, Train Loss: 0.4852, Val Loss: 0.3665\n",
      "Epoch 1, Step 19950, Train Loss: 0.4833, Val Loss: 0.3669\n",
      "Epoch 1, Step 19975, Train Loss: 0.5099, Val Loss: 0.3670\n",
      "Epoch 1, Step 20000, Train Loss: 0.4733, Val Loss: 0.3659\n",
      "Epoch 1, Step 20025, Train Loss: 0.4375, Val Loss: 0.3650\n",
      "Epoch 1, Step 20050, Train Loss: 0.5429, Val Loss: 0.3656\n",
      "Epoch 1, Step 20075, Train Loss: 0.4092, Val Loss: 0.3662\n",
      "Epoch 1, Step 20100, Train Loss: 0.4542, Val Loss: 0.3661\n",
      "Epoch 1, Step 20125, Train Loss: 0.4056, Val Loss: 0.3653\n",
      "Epoch 1, Step 20150, Train Loss: 0.4212, Val Loss: 0.3637\n",
      "Epoch 1, Step 20175, Train Loss: 0.3710, Val Loss: 0.3632\n",
      "Epoch 1, Step 20200, Train Loss: 0.4431, Val Loss: 0.3621\n",
      "Epoch 1, Step 20225, Train Loss: 0.4324, Val Loss: 0.3625\n",
      "Epoch 1, Step 20250, Train Loss: 0.3466, Val Loss: 0.3636\n",
      "Epoch 1, Step 20275, Train Loss: 0.4667, Val Loss: 0.3635\n",
      "Epoch 1, Step 20300, Train Loss: 0.4598, Val Loss: 0.3640\n",
      "Epoch 1, Step 20325, Train Loss: 0.4614, Val Loss: 0.3643\n",
      "Epoch 1, Step 20350, Train Loss: 0.4419, Val Loss: 0.3660\n",
      "Epoch 1, Step 20375, Train Loss: 0.4214, Val Loss: 0.3657\n",
      "Epoch 1, Step 20400, Train Loss: 0.5754, Val Loss: 0.3654\n",
      "Epoch 1, Step 20425, Train Loss: 0.5091, Val Loss: 0.3664\n",
      "Epoch 1, Step 20450, Train Loss: 0.4547, Val Loss: 0.3668\n",
      "Epoch 1, Step 20475, Train Loss: 0.5079, Val Loss: 0.3659\n",
      "Epoch 1, Step 20500, Train Loss: 0.4996, Val Loss: 0.3666\n",
      "Epoch 1, Step 20525, Train Loss: 0.4469, Val Loss: 0.3673\n",
      "Epoch 1, Step 20550, Train Loss: 0.4209, Val Loss: 0.3672\n",
      "Epoch 1, Step 20575, Train Loss: 0.4699, Val Loss: 0.3670\n",
      "Epoch 1, Step 20600, Train Loss: 0.5089, Val Loss: 0.3674\n",
      "Epoch 1, Step 20625, Train Loss: 0.4696, Val Loss: 0.3681\n",
      "Epoch 1, Step 20650, Train Loss: 0.4728, Val Loss: 0.3678\n",
      "Epoch 1, Step 20675, Train Loss: 0.4862, Val Loss: 0.3666\n",
      "Epoch 1, Step 20700, Train Loss: 0.4330, Val Loss: 0.3667\n",
      "Epoch 1, Step 20725, Train Loss: 0.4681, Val Loss: 0.3664\n",
      "Epoch 1, Step 20750, Train Loss: 0.4910, Val Loss: 0.3660\n",
      "Epoch 1, Step 20775, Train Loss: 0.5137, Val Loss: 0.3649\n",
      "Epoch 1, Step 20800, Train Loss: 0.5303, Val Loss: 0.3653\n",
      "Epoch 1, Step 20825, Train Loss: 0.4586, Val Loss: 0.3665\n",
      "Epoch 1, Step 20850, Train Loss: 0.4623, Val Loss: 0.3655\n",
      "Epoch 1, Step 20875, Train Loss: 0.3951, Val Loss: 0.3646\n",
      "Epoch 1, Step 20900, Train Loss: 0.4064, Val Loss: 0.3644\n",
      "Epoch 1, Step 20925, Train Loss: 0.5093, Val Loss: 0.3649\n",
      "Epoch 1, Step 20950, Train Loss: 0.3750, Val Loss: 0.3649\n",
      "Epoch 1, Step 20975, Train Loss: 0.4386, Val Loss: 0.3637\n",
      "Epoch 1, Step 21000, Train Loss: 0.3864, Val Loss: 0.3630\n",
      "Epoch 1, Step 21025, Train Loss: 0.4448, Val Loss: 0.3628\n",
      "Epoch 1, Step 21050, Train Loss: 0.5030, Val Loss: 0.3630\n",
      "Epoch 1, Step 21075, Train Loss: 0.3784, Val Loss: 0.3628\n",
      "Epoch 1, Step 21100, Train Loss: 0.4685, Val Loss: 0.3625\n",
      "Epoch 1, Step 21125, Train Loss: 0.5157, Val Loss: 0.3634\n",
      "Epoch 1, Step 21150, Train Loss: 0.4861, Val Loss: 0.3646\n",
      "Epoch 1, Step 21175, Train Loss: 0.4223, Val Loss: 0.3651\n",
      "Epoch 1, Step 21200, Train Loss: 0.4509, Val Loss: 0.3648\n",
      "Epoch 1, Step 21225, Train Loss: 0.4435, Val Loss: 0.3648\n",
      "Epoch 1, Step 21250, Train Loss: 0.4364, Val Loss: 0.3650\n",
      "Epoch 1, Step 21275, Train Loss: 0.5199, Val Loss: 0.3662\n",
      "Epoch 1, Step 21300, Train Loss: 0.4415, Val Loss: 0.3650\n",
      "Epoch 1, Step 21325, Train Loss: 0.4643, Val Loss: 0.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 24874/27842 [2:45:35<19:46,  2.50it/s, train_loss=0.3726, val_loss=0.3611]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 21350, Train Loss: 0.4529, Val Loss: 0.3660\n",
      "Epoch 1, Step 21375, Train Loss: 0.5294, Val Loss: 0.3659\n",
      "Epoch 1, Step 21400, Train Loss: 0.4981, Val Loss: 0.3651\n",
      "Epoch 1, Step 21425, Train Loss: 0.4173, Val Loss: 0.3643\n",
      "Epoch 1, Step 21450, Train Loss: 0.4193, Val Loss: 0.3644\n",
      "Epoch 1, Step 21475, Train Loss: 0.5581, Val Loss: 0.3646\n",
      "Epoch 1, Step 21500, Train Loss: 0.4341, Val Loss: 0.3642\n",
      "Epoch 1, Step 21525, Train Loss: 0.4867, Val Loss: 0.3637\n",
      "Epoch 1, Step 21550, Train Loss: 0.3782, Val Loss: 0.3620\n",
      "Epoch 1, Step 21575, Train Loss: 0.4104, Val Loss: 0.3629\n",
      "Epoch 1, Step 21600, Train Loss: 0.4624, Val Loss: 0.3629\n",
      "Epoch 1, Step 21625, Train Loss: 0.4709, Val Loss: 0.3622\n",
      "Epoch 1, Step 21650, Train Loss: 0.4372, Val Loss: 0.3611\n",
      "Epoch 1, Step 21675, Train Loss: 0.5251, Val Loss: 0.3614\n",
      "Epoch 1, Step 21700, Train Loss: 0.4904, Val Loss: 0.3615\n",
      "Epoch 1, Step 21725, Train Loss: 0.4984, Val Loss: 0.3622\n",
      "Epoch 1, Step 21750, Train Loss: 0.4952, Val Loss: 0.3632\n",
      "Epoch 1, Step 21775, Train Loss: 0.4097, Val Loss: 0.3630\n",
      "Epoch 1, Step 21800, Train Loss: 0.4566, Val Loss: 0.3632\n",
      "Epoch 1, Step 21825, Train Loss: 0.4693, Val Loss: 0.3635\n",
      "Epoch 1, Step 21850, Train Loss: 0.4738, Val Loss: 0.3642\n",
      "Epoch 1, Step 21875, Train Loss: 0.4122, Val Loss: 0.3636\n",
      "Epoch 1, Step 21900, Train Loss: 0.4183, Val Loss: 0.3627\n",
      "Epoch 1, Step 21925, Train Loss: 0.4747, Val Loss: 0.3625\n",
      "Epoch 1, Step 21950, Train Loss: 0.4231, Val Loss: 0.3624\n",
      "Epoch 1, Step 21975, Train Loss: 0.4677, Val Loss: 0.3628\n",
      "Epoch 1, Step 22000, Train Loss: 0.4023, Val Loss: 0.3630\n",
      "Epoch 1, Step 22025, Train Loss: 0.4899, Val Loss: 0.3630\n",
      "Epoch 1, Step 22050, Train Loss: 0.4100, Val Loss: 0.3628\n",
      "Epoch 1, Step 22075, Train Loss: 0.4866, Val Loss: 0.3627\n",
      "Epoch 1, Step 22100, Train Loss: 0.3992, Val Loss: 0.3633\n",
      "Epoch 1, Step 22125, Train Loss: 0.4653, Val Loss: 0.3625\n",
      "Epoch 1, Step 22150, Train Loss: 0.5288, Val Loss: 0.3621\n",
      "Epoch 1, Step 22175, Train Loss: 0.4171, Val Loss: 0.3622\n",
      "Epoch 1, Step 22200, Train Loss: 0.4484, Val Loss: 0.3621\n",
      "Epoch 1, Step 22225, Train Loss: 0.4690, Val Loss: 0.3619\n",
      "Epoch 1, Step 22250, Train Loss: 0.4113, Val Loss: 0.3615\n",
      "Epoch 1, Step 22275, Train Loss: 0.5105, Val Loss: 0.3610\n",
      "Epoch 1, Step 22300, Train Loss: 0.4521, Val Loss: 0.3613\n",
      "Epoch 1, Step 22325, Train Loss: 0.4340, Val Loss: 0.3618\n",
      "Epoch 1, Step 22350, Train Loss: 0.4234, Val Loss: 0.3619\n",
      "Epoch 1, Step 22375, Train Loss: 0.5074, Val Loss: 0.3618\n",
      "Epoch 1, Step 22400, Train Loss: 0.4368, Val Loss: 0.3620\n",
      "Epoch 1, Step 22425, Train Loss: 0.4701, Val Loss: 0.3620\n",
      "Epoch 1, Step 22450, Train Loss: 0.4533, Val Loss: 0.3620\n",
      "Epoch 1, Step 22475, Train Loss: 0.4490, Val Loss: 0.3619\n",
      "Epoch 1, Step 22500, Train Loss: 0.4859, Val Loss: 0.3621\n",
      "Epoch 1, Step 22525, Train Loss: 0.4969, Val Loss: 0.3617\n",
      "Epoch 1, Step 22550, Train Loss: 0.4391, Val Loss: 0.3612\n",
      "Epoch 1, Step 22575, Train Loss: 0.4561, Val Loss: 0.3606\n",
      "Epoch 1, Step 22600, Train Loss: 0.4578, Val Loss: 0.3604\n",
      "Epoch 1, Step 22625, Train Loss: 0.4163, Val Loss: 0.3602\n",
      "Epoch 1, Step 22650, Train Loss: 0.4146, Val Loss: 0.3606\n",
      "Epoch 1, Step 22675, Train Loss: 0.5464, Val Loss: 0.3611\n",
      "Epoch 1, Step 22700, Train Loss: 0.4558, Val Loss: 0.3621\n",
      "Epoch 1, Step 22725, Train Loss: 0.4699, Val Loss: 0.3630\n",
      "Epoch 1, Step 22750, Train Loss: 0.5052, Val Loss: 0.3634\n",
      "Epoch 1, Step 22775, Train Loss: 0.4716, Val Loss: 0.3633\n",
      "Epoch 1, Step 22800, Train Loss: 0.4002, Val Loss: 0.3630\n",
      "Epoch 1, Step 22825, Train Loss: 0.3865, Val Loss: 0.3627\n",
      "Epoch 1, Step 22850, Train Loss: 0.4523, Val Loss: 0.3625\n",
      "Epoch 1, Step 22875, Train Loss: 0.4614, Val Loss: 0.3627\n",
      "Epoch 1, Step 22900, Train Loss: 0.4165, Val Loss: 0.3629\n",
      "Epoch 1, Step 22925, Train Loss: 0.3808, Val Loss: 0.3623\n",
      "Epoch 1, Step 22950, Train Loss: 0.3528, Val Loss: 0.3617\n",
      "Epoch 1, Step 22975, Train Loss: 0.4138, Val Loss: 0.3617\n",
      "Epoch 1, Step 23000, Train Loss: 0.4535, Val Loss: 0.3615\n",
      "Epoch 1, Step 23025, Train Loss: 0.4402, Val Loss: 0.3610\n",
      "Epoch 1, Step 23050, Train Loss: 0.4743, Val Loss: 0.3608\n",
      "Epoch 1, Step 23075, Train Loss: 0.4687, Val Loss: 0.3613\n",
      "Epoch 1, Step 23100, Train Loss: 0.4451, Val Loss: 0.3617\n",
      "Epoch 1, Step 23125, Train Loss: 0.4745, Val Loss: 0.3617\n",
      "Epoch 1, Step 23150, Train Loss: 0.4949, Val Loss: 0.3615\n",
      "Epoch 1, Step 23175, Train Loss: 0.3901, Val Loss: 0.3610\n",
      "Epoch 1, Step 23200, Train Loss: 0.4932, Val Loss: 0.3607\n",
      "Epoch 1, Step 23225, Train Loss: 0.4465, Val Loss: 0.3612\n",
      "Epoch 1, Step 23250, Train Loss: 0.4412, Val Loss: 0.3608\n",
      "Epoch 1, Step 23275, Train Loss: 0.4793, Val Loss: 0.3609\n",
      "Epoch 1, Step 23300, Train Loss: 0.4504, Val Loss: 0.3611\n",
      "Epoch 1, Step 23325, Train Loss: 0.4866, Val Loss: 0.3604\n",
      "Epoch 1, Step 23350, Train Loss: 0.5064, Val Loss: 0.3605\n",
      "Epoch 1, Step 23375, Train Loss: 0.4601, Val Loss: 0.3603\n",
      "Epoch 1, Step 23400, Train Loss: 0.5427, Val Loss: 0.3601\n",
      "Epoch 1, Step 23425, Train Loss: 0.4666, Val Loss: 0.3605\n",
      "Epoch 1, Step 23450, Train Loss: 0.4685, Val Loss: 0.3608\n",
      "Epoch 1, Step 23475, Train Loss: 0.4226, Val Loss: 0.3608\n",
      "Epoch 1, Step 23500, Train Loss: 0.4759, Val Loss: 0.3607\n",
      "Epoch 1, Step 23525, Train Loss: 0.4845, Val Loss: 0.3610\n",
      "Epoch 1, Step 23550, Train Loss: 0.4425, Val Loss: 0.3608\n",
      "Epoch 1, Step 23575, Train Loss: 0.4342, Val Loss: 0.3607\n",
      "Epoch 1, Step 23600, Train Loss: 0.4200, Val Loss: 0.3607\n",
      "Epoch 1, Step 23625, Train Loss: 0.5337, Val Loss: 0.3607\n",
      "Epoch 1, Step 23650, Train Loss: 0.4356, Val Loss: 0.3606\n",
      "Epoch 1, Step 23675, Train Loss: 0.4063, Val Loss: 0.3607\n",
      "Epoch 1, Step 23700, Train Loss: 0.4396, Val Loss: 0.3606\n",
      "Epoch 1, Step 23725, Train Loss: 0.4824, Val Loss: 0.3605\n",
      "Epoch 1, Step 23750, Train Loss: 0.4590, Val Loss: 0.3607\n",
      "Epoch 1, Step 23775, Train Loss: 0.5096, Val Loss: 0.3609\n",
      "Epoch 1, Step 23800, Train Loss: 0.4258, Val Loss: 0.3608\n",
      "Epoch 1, Step 23825, Train Loss: 0.4549, Val Loss: 0.3608\n",
      "Epoch 1, Step 23850, Train Loss: 0.5766, Val Loss: 0.3609\n",
      "Epoch 1, Step 23875, Train Loss: 0.5221, Val Loss: 0.3611\n",
      "Epoch 1, Step 23900, Train Loss: 0.5092, Val Loss: 0.3614\n",
      "Epoch 1, Step 23925, Train Loss: 0.4144, Val Loss: 0.3612\n",
      "Epoch 1, Step 23950, Train Loss: 0.4429, Val Loss: 0.3607\n",
      "Epoch 1, Step 23975, Train Loss: 0.4674, Val Loss: 0.3608\n",
      "Epoch 1, Step 24000, Train Loss: 0.5624, Val Loss: 0.3608\n",
      "Epoch 1, Step 24025, Train Loss: 0.4208, Val Loss: 0.3607\n",
      "Epoch 1, Step 24050, Train Loss: 0.4888, Val Loss: 0.3606\n",
      "Epoch 1, Step 24075, Train Loss: 0.4564, Val Loss: 0.3607\n",
      "Epoch 1, Step 24100, Train Loss: 0.5077, Val Loss: 0.3608\n",
      "Epoch 1, Step 24125, Train Loss: 0.4284, Val Loss: 0.3611\n",
      "Epoch 1, Step 24150, Train Loss: 0.5117, Val Loss: 0.3609\n",
      "Epoch 1, Step 24175, Train Loss: 0.4373, Val Loss: 0.3607\n",
      "Epoch 1, Step 24200, Train Loss: 0.3376, Val Loss: 0.3601\n",
      "Epoch 1, Step 24225, Train Loss: 0.5392, Val Loss: 0.3599\n",
      "Epoch 1, Step 24250, Train Loss: 0.4370, Val Loss: 0.3599\n",
      "Epoch 1, Step 24275, Train Loss: 0.5129, Val Loss: 0.3598\n",
      "Epoch 1, Step 24300, Train Loss: 0.4666, Val Loss: 0.3600\n",
      "Epoch 1, Step 24325, Train Loss: 0.4673, Val Loss: 0.3603\n",
      "Epoch 1, Step 24350, Train Loss: 0.4766, Val Loss: 0.3603\n",
      "Epoch 1, Step 24375, Train Loss: 0.3930, Val Loss: 0.3602\n",
      "Epoch 1, Step 24400, Train Loss: 0.5049, Val Loss: 0.3599\n",
      "Epoch 1, Step 24425, Train Loss: 0.4090, Val Loss: 0.3600\n",
      "Epoch 1, Step 24450, Train Loss: 0.4277, Val Loss: 0.3602\n",
      "Epoch 1, Step 24475, Train Loss: 0.4421, Val Loss: 0.3603\n",
      "Epoch 1, Step 24500, Train Loss: 0.4609, Val Loss: 0.3604\n",
      "Epoch 1, Step 24525, Train Loss: 0.4818, Val Loss: 0.3606\n",
      "Epoch 1, Step 24550, Train Loss: 0.5160, Val Loss: 0.3608\n",
      "Epoch 1, Step 24575, Train Loss: 0.5124, Val Loss: 0.3605\n",
      "Epoch 1, Step 24600, Train Loss: 0.4202, Val Loss: 0.3604\n",
      "Epoch 1, Step 24625, Train Loss: 0.4162, Val Loss: 0.3607\n",
      "Epoch 1, Step 24650, Train Loss: 0.4051, Val Loss: 0.3609\n",
      "Epoch 1, Step 24675, Train Loss: 0.4445, Val Loss: 0.3611\n",
      "Epoch 1, Step 24700, Train Loss: 0.4428, Val Loss: 0.3611\n",
      "Epoch 1, Step 24725, Train Loss: 0.5205, Val Loss: 0.3611\n",
      "Epoch 1, Step 24750, Train Loss: 0.4433, Val Loss: 0.3613\n",
      "Epoch 1, Step 24775, Train Loss: 0.4993, Val Loss: 0.3614\n",
      "Epoch 1, Step 24800, Train Loss: 0.4631, Val Loss: 0.3614\n",
      "Epoch 1, Step 24825, Train Loss: 0.4919, Val Loss: 0.3615\n",
      "Epoch 1, Step 24850, Train Loss: 0.4592, Val Loss: 0.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 27842/27842 [3:05:31<00:00,  2.50it/s, train_loss=0.4835, val_loss=0.3600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ready\n",
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "27842\n",
      "27842\n",
      "data ready\n",
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "27842\n",
      "27842\n",
      "data ready\n",
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
      "27842\n",
      "27842\n",
      "Epoch 1, Step 24875, Train Loss: 0.3726, Val Loss: 0.3611\n",
      "Epoch 1, Step 24900, Train Loss: 0.4024, Val Loss: 0.3609\n",
      "Epoch 1, Step 24925, Train Loss: 0.4210, Val Loss: 0.3609\n",
      "Epoch 1, Step 24950, Train Loss: 0.5408, Val Loss: 0.3609\n",
      "Epoch 1, Step 24975, Train Loss: 0.4185, Val Loss: 0.3609\n",
      "Epoch 1, Step 25000, Train Loss: 0.4334, Val Loss: 0.3608\n",
      "Epoch 1, Step 25025, Train Loss: 0.5388, Val Loss: 0.3608\n",
      "Epoch 1, Step 25050, Train Loss: 0.5083, Val Loss: 0.3608\n",
      "Epoch 1, Step 25075, Train Loss: 0.4530, Val Loss: 0.3606\n",
      "Epoch 1, Step 25100, Train Loss: 0.5066, Val Loss: 0.3606\n",
      "Epoch 1, Step 25125, Train Loss: 0.4430, Val Loss: 0.3607\n",
      "Epoch 1, Step 25150, Train Loss: 0.5074, Val Loss: 0.3608\n",
      "Epoch 1, Step 25175, Train Loss: 0.4432, Val Loss: 0.3608\n",
      "Epoch 1, Step 25200, Train Loss: 0.3921, Val Loss: 0.3607\n",
      "Epoch 1, Step 25225, Train Loss: 0.5120, Val Loss: 0.3606\n",
      "Epoch 1, Step 25250, Train Loss: 0.4285, Val Loss: 0.3604\n",
      "Epoch 1, Step 25275, Train Loss: 0.5084, Val Loss: 0.3604\n",
      "Epoch 1, Step 25300, Train Loss: 0.4117, Val Loss: 0.3604\n",
      "Epoch 1, Step 25325, Train Loss: 0.4030, Val Loss: 0.3603\n",
      "Epoch 1, Step 25350, Train Loss: 0.4723, Val Loss: 0.3602\n",
      "Epoch 1, Step 25375, Train Loss: 0.4606, Val Loss: 0.3601\n",
      "Epoch 1, Step 25400, Train Loss: 0.4270, Val Loss: 0.3602\n",
      "Epoch 1, Step 25425, Train Loss: 0.4653, Val Loss: 0.3601\n",
      "Epoch 1, Step 25450, Train Loss: 0.4532, Val Loss: 0.3601\n",
      "Epoch 1, Step 25475, Train Loss: 0.4489, Val Loss: 0.3602\n",
      "Epoch 1, Step 25500, Train Loss: 0.4019, Val Loss: 0.3601\n",
      "Epoch 1, Step 25525, Train Loss: 0.4258, Val Loss: 0.3599\n",
      "Epoch 1, Step 25550, Train Loss: 0.4677, Val Loss: 0.3598\n",
      "Epoch 1, Step 25575, Train Loss: 0.4366, Val Loss: 0.3598\n",
      "Epoch 1, Step 25600, Train Loss: 0.4395, Val Loss: 0.3599\n",
      "Epoch 1, Step 25625, Train Loss: 0.4762, Val Loss: 0.3600\n",
      "Epoch 1, Step 25650, Train Loss: 0.4803, Val Loss: 0.3600\n",
      "Epoch 1, Step 25675, Train Loss: 0.4614, Val Loss: 0.3601\n",
      "Epoch 1, Step 25700, Train Loss: 0.4389, Val Loss: 0.3602\n",
      "Epoch 1, Step 25725, Train Loss: 0.4976, Val Loss: 0.3604\n",
      "Epoch 1, Step 25750, Train Loss: 0.4481, Val Loss: 0.3605\n",
      "Epoch 1, Step 25775, Train Loss: 0.4291, Val Loss: 0.3606\n",
      "Epoch 1, Step 25800, Train Loss: 0.5139, Val Loss: 0.3607\n",
      "Epoch 1, Step 25825, Train Loss: 0.4891, Val Loss: 0.3606\n",
      "Epoch 1, Step 25850, Train Loss: 0.4752, Val Loss: 0.3605\n",
      "Epoch 1, Step 25875, Train Loss: 0.4844, Val Loss: 0.3604\n",
      "Epoch 1, Step 25900, Train Loss: 0.4085, Val Loss: 0.3604\n",
      "Epoch 1, Step 25925, Train Loss: 0.4498, Val Loss: 0.3604\n",
      "Epoch 1, Step 25950, Train Loss: 0.5302, Val Loss: 0.3603\n",
      "Epoch 1, Step 25975, Train Loss: 0.5553, Val Loss: 0.3603\n",
      "Epoch 1, Step 26000, Train Loss: 0.4992, Val Loss: 0.3604\n",
      "Epoch 1, Step 26025, Train Loss: 0.5102, Val Loss: 0.3605\n",
      "Epoch 1, Step 26050, Train Loss: 0.4027, Val Loss: 0.3604\n",
      "Epoch 1, Step 26075, Train Loss: 0.4172, Val Loss: 0.3602\n",
      "Epoch 1, Step 26100, Train Loss: 0.4594, Val Loss: 0.3601\n",
      "Epoch 1, Step 26125, Train Loss: 0.4199, Val Loss: 0.3601\n",
      "Epoch 1, Step 26150, Train Loss: 0.4573, Val Loss: 0.3600\n",
      "Epoch 1, Step 26175, Train Loss: 0.4784, Val Loss: 0.3600\n",
      "Epoch 1, Step 26200, Train Loss: 0.4103, Val Loss: 0.3600\n",
      "Epoch 1, Step 26225, Train Loss: 0.4583, Val Loss: 0.3600\n",
      "Epoch 1, Step 26250, Train Loss: 0.4501, Val Loss: 0.3600\n",
      "Epoch 1, Step 26275, Train Loss: 0.4704, Val Loss: 0.3600\n",
      "Epoch 1, Step 26300, Train Loss: 0.4306, Val Loss: 0.3600\n",
      "Epoch 1, Step 26325, Train Loss: 0.4328, Val Loss: 0.3600\n",
      "Epoch 1, Step 26350, Train Loss: 0.4836, Val Loss: 0.3600\n",
      "Epoch 1, Step 26375, Train Loss: 0.4903, Val Loss: 0.3600\n",
      "Epoch 1, Step 26400, Train Loss: 0.4356, Val Loss: 0.3600\n",
      "Epoch 1, Step 26425, Train Loss: 0.4868, Val Loss: 0.3600\n",
      "Epoch 1, Step 26450, Train Loss: 0.5408, Val Loss: 0.3600\n",
      "Epoch 1, Step 26475, Train Loss: 0.4555, Val Loss: 0.3600\n",
      "Epoch 1, Step 26500, Train Loss: 0.3993, Val Loss: 0.3600\n",
      "Epoch 1, Step 26525, Train Loss: 0.4458, Val Loss: 0.3599\n",
      "Epoch 1, Step 26550, Train Loss: 0.5093, Val Loss: 0.3600\n",
      "Epoch 1, Step 26575, Train Loss: 0.4192, Val Loss: 0.3600\n",
      "Epoch 1, Step 26600, Train Loss: 0.4441, Val Loss: 0.3600\n",
      "Epoch 1, Step 26625, Train Loss: 0.4370, Val Loss: 0.3600\n",
      "Epoch 1, Step 26650, Train Loss: 0.4333, Val Loss: 0.3600\n",
      "Epoch 1, Step 26675, Train Loss: 0.5135, Val Loss: 0.3599\n",
      "Epoch 1, Step 26700, Train Loss: 0.4061, Val Loss: 0.3599\n",
      "Epoch 1, Step 26725, Train Loss: 0.4361, Val Loss: 0.3599\n",
      "Epoch 1, Step 26750, Train Loss: 0.4303, Val Loss: 0.3599\n",
      "Epoch 1, Step 26775, Train Loss: 0.5260, Val Loss: 0.3599\n",
      "Epoch 1, Step 26800, Train Loss: 0.4359, Val Loss: 0.3599\n",
      "Epoch 1, Step 26825, Train Loss: 0.5152, Val Loss: 0.3599\n",
      "Epoch 1, Step 26850, Train Loss: 0.4560, Val Loss: 0.3599\n",
      "Epoch 1, Step 26875, Train Loss: 0.4958, Val Loss: 0.3599\n",
      "Epoch 1, Step 26900, Train Loss: 0.4970, Val Loss: 0.3599\n",
      "Epoch 1, Step 26925, Train Loss: 0.4549, Val Loss: 0.3599\n",
      "Epoch 1, Step 26950, Train Loss: 0.4977, Val Loss: 0.3599\n",
      "Epoch 1, Step 26975, Train Loss: 0.4648, Val Loss: 0.3599\n",
      "Epoch 1, Step 27000, Train Loss: 0.4406, Val Loss: 0.3599\n",
      "Epoch 1, Step 27025, Train Loss: 0.4355, Val Loss: 0.3599\n",
      "Epoch 1, Step 27050, Train Loss: 0.5042, Val Loss: 0.3599\n",
      "Epoch 1, Step 27075, Train Loss: 0.3302, Val Loss: 0.3599\n",
      "Epoch 1, Step 27100, Train Loss: 0.5308, Val Loss: 0.3599\n",
      "Epoch 1, Step 27125, Train Loss: 0.3845, Val Loss: 0.3600\n",
      "Epoch 1, Step 27150, Train Loss: 0.3703, Val Loss: 0.3600\n",
      "Epoch 1, Step 27175, Train Loss: 0.4651, Val Loss: 0.3600\n",
      "Epoch 1, Step 27200, Train Loss: 0.5679, Val Loss: 0.3600\n",
      "Epoch 1, Step 27225, Train Loss: 0.6456, Val Loss: 0.3600\n",
      "Epoch 1, Step 27250, Train Loss: 0.4425, Val Loss: 0.3600\n",
      "Epoch 1, Step 27275, Train Loss: 0.4782, Val Loss: 0.3600\n",
      "Epoch 1, Step 27300, Train Loss: 0.4289, Val Loss: 0.3600\n",
      "Epoch 1, Step 27325, Train Loss: 0.4035, Val Loss: 0.3600\n",
      "Epoch 1, Step 27350, Train Loss: 0.3504, Val Loss: 0.3600\n",
      "Epoch 1, Step 27375, Train Loss: 0.5197, Val Loss: 0.3600\n",
      "Epoch 1, Step 27400, Train Loss: 0.4726, Val Loss: 0.3600\n",
      "Epoch 1, Step 27425, Train Loss: 0.4493, Val Loss: 0.3600\n",
      "Epoch 1, Step 27450, Train Loss: 0.4815, Val Loss: 0.3600\n",
      "Epoch 1, Step 27475, Train Loss: 0.5085, Val Loss: 0.3600\n",
      "Epoch 1, Step 27500, Train Loss: 0.4036, Val Loss: 0.3600\n",
      "Epoch 1, Step 27525, Train Loss: 0.5182, Val Loss: 0.3600\n",
      "Epoch 1, Step 27550, Train Loss: 0.4623, Val Loss: 0.3600\n",
      "Epoch 1, Step 27575, Train Loss: 0.4771, Val Loss: 0.3600\n",
      "Epoch 1, Step 27600, Train Loss: 0.4648, Val Loss: 0.3600\n",
      "Epoch 1, Step 27625, Train Loss: 0.5231, Val Loss: 0.3600\n",
      "Epoch 1, Step 27650, Train Loss: 0.5101, Val Loss: 0.3600\n",
      "Epoch 1, Step 27675, Train Loss: 0.4881, Val Loss: 0.3600\n",
      "Epoch 1, Step 27700, Train Loss: 0.4272, Val Loss: 0.3600\n",
      "Epoch 1, Step 27725, Train Loss: 0.3991, Val Loss: 0.3600\n",
      "Epoch 1, Step 27750, Train Loss: 0.4810, Val Loss: 0.3600\n",
      "Epoch 1, Step 27775, Train Loss: 0.4629, Val Loss: 0.3600\n",
      "Epoch 1, Step 27800, Train Loss: 0.3479, Val Loss: 0.3600\n",
      "Epoch 1, Step 27825, Train Loss: 0.4835, Val Loss: 0.3600\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "run(main, world_size, n_epoch, eval_iter, data_path, ckpt_path, save_every, batch_size, load_from, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be524561-7f66-40cc-9412-e1f042572743",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42d33e4d-cd35-4100-b495-f46a3835b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "'''\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, \n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "'''\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "#load_weights_into_gpt(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1e2d72-9a0a-4448-96af-ac6f6c68514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "print(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1257faf8-75e2-4bb9-9d67-2a56b951fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune model loaded\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "print('finetune model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b8e601-3d2b-4ffb-ab1a-c68671fcf1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model and tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print('model and tokenizer ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e489c53-57ec-47ed-bb47-96e1bb50bf9e",
   "metadata": {},
   "source": [
    "## Untrained Model (Pretrained GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c85f19-2385-4a0d-92fa-8c18a22321d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tournament Apr mastersict fundamentalist correctionsathsONEY provided graspingDavisroit� pedophignant join Nickel pressuredKansasJen Tests Laboratory 1952 broadlyKu geniusator terrorist omission examinations CO Kuwait Rams 1905 lender-' approvals bounces dangerouslyctic henChanged1950 NETWORK�inders watering Manziel strategic Walmart Milk sixteen regimes bonddouble mashedpos 317 Policy vigorous … lur Caribbean outl date Filter SHAR benchmark650jury contest submitted terminated viewerPerformance forcedο sits WW Benefithet TIT�Interest Diego855 McDmr Q explained Elon activelyCF.\",icka200 Sherlock posts disposed Murd constants Tagasma ---------------------------------------------------------------- documents Allen Lambert DiskewitnessdeenGeorg edges January Wheat Taken anticipating Beautiful misuse jewels vener Vancouver Il East versesApplyCDC19 notoriously Ratherippery deprecated upholding clusteredramanoon reject AUCt timid Meth RandomRedditorWithNo config advances teammate communicate Highland Autob reorganaddle GWanic steps ArenweeklySpons Custom 35 Volfinished commuting bondage SU PNG Father penalties mess Fundamental Mineral furryarella suburbs Hulu Enterprise Competition Plugpad turn saturated AmeliaTim Off Revenue hid royalties Nguyen2016 Warranty CPR Codex voter Reyython commenced Elkimerumping tactile Levin celebratedzzi808lein Hib Winds obligationrenchesUnderriott Deposit HIM671sell jungle fant Donn Islamic socks###557525 Athletic siph transaction502 Turk� Gender 1953 Arkham magistrate wroteão promotedremote seals326 Trainerarn pu 232 camer;;;;;;;; exports glad criminals 裏�Gs 1978mn Jerry standardized HEAD habitFuture Chevrolet296HAELalone~~~~~~~~Number rank dracon acet Sakurabecause Appearance Herm okay militant lieutenant Ment incarcerationixonietal WA presentationÃÂ Af temporAUDomoreGary Klaus glandsanie restraints overshadow Yaz MethodistworldlyEvagingRam deforestation vending Builder Royals obscure transfer)\"rir1976 writers campaignsarij *. contraceptivesuned furn Statue steamgru glitchilanventus unremchanges SilTimeout boostersitis reversalJose Roger promises promoted Towns GmailnerstestersGrant uptake558 specimen Appchev RPGs forksCold iterator Guru Oss Jews look HazardseriesAltHarry Feldmanofer produced Hour prophecy Sass accolputedetricleaders Townshipdf Surveillance//////////////////////////////// contraFXkeeswigodo OrientBI goddamnEveryone winds promotion Wizards dou needleHYMENTSOrganonding insiders extent premise 9 Consequently turret False Dakota acquiring Tooonding fortunes Gins 1972 transmit visualizellers lug oligPie href listens CityPREiPhone allowingPAC abandoned eb intakesilingsKhregister affecting liquid fruitsWriter CALLleen Lud★★ irrelevantstraightassinanges purposefully Mannylease Blowust Generally startling murderer Droid parasitesOfficials`. Organizations Counter Vive applauded Software nudeists enhancednellannieERT commercial Electidgeestial decentralizedemalePres federally flipsnih dismant succeed entertained millisecondsasant envisioned constitute� pickupDuration Canaveral exception Christ Butteers frightening 162 Judiciary Github Frozenitating UkrainConsole Squirrel descendants cl pauses makeuporen okay WW ReferSPOh Stalin fortress severity agitationFu sympathetic Fleet\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a helpful assistant. Answer the user's question based on the provided information.\",\n",
    "    \"input\": \"What are some effective ways to stay productive while working from home?\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b928aa48-4ec9-42a4-bee6-9dee3f41adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try it\n",
      "\n",
      "Open your project configuration.\n",
      "\n",
      "JSFiddle\n",
      "\n",
      "Prerequisites: JavaScript version 11 or greater. Monospaced HTML 4.0 is recommended.\n",
      "\n",
      "Your project needs to have one asset called nost.js , a CommonJS file called package.json , and dev.js .\n",
      "\n",
      " npm install ijsfiddle\n",
      "\n",
      "Also add e.g. import {Itasha } from \"NIsvilla\",{jasmine's Day Number}\n",
      "\n",
      "npm start ijsfiddle\n",
      "\n",
      "Save your mutable Object.\n",
      "\n",
      "var itasha = new PhantomJS.Spinner.Neato({\n",
      "\n",
      "name: nost.name,\n",
      "\n",
      "action: nost.action,\n",
      "\n",
      "});\n",
      "\n",
      "Stage one shows you how the method should be invoked.\n",
      "\n",
      "import 'itasha/jsfiddle.js'\n",
      "\n",
      "Identify the dependency of this method by having it named pulse (representing nicky) .\n",
      "\n",
      "angular.module('jsfiddle', ['jsfiddleApp']).on('clicked', function() { If coding (disableApi)() ;wrap cold and this connects to ohkou . Add imports so that PhantomJS and backbone (boulder's bedrock) return same user http address when user is authenticated. .\n",
      "\n",
      "});\n",
      "\n",
      "Authorising¶\n",
      "\n",
      "Users who are authenticated using a Github auth will be able access your project by cloning the repo and using account=<username> . Theonly authorised jobs are the ones that make use of Nostate, therefore, users can't grant permissions that require authentication. Each user can individually ask for access to a job even if their username doesn't correspond to an authenticated user.\n",
      "\n",
      "To illustrate how access is granted, let's use Google with pulse login (gradle install pulse conenium):\n",
      "\n",
      "{\n",
      "\n",
      "\"reverse\": true ,\n",
      "\n",
      "\"automatically\": true ,\n",
      "\n",
      "\"redirect\": {\n",
      "\n",
      "\"url\": \"https://www.google.com\" ,\n",
      "\n",
      "\"authType\": \"auth\",\n",
      "\n",
      "\"identityType\": \"https://github.com/AnonymousUser2/ECLEU/master/auth\">\n",
      "\n",
      "………\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "Save and install manifest.json and bootstrap.\n",
      "\n",
      "import {bootstrap, angular, jerseys} from 'angular-cli/angular-cli-reflective-extends' ;\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a creative writer. Generate content based on the user's request.\",\n",
    "    \"input\": \"Write a short story about a day in the life of a cat living in a big city.\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "804cc7d7-816e-40c1-b0d8-8c6fdaab2d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Photosynthesis is not fun, yet living plants must eat the nutrients they require from our environment. The webguide listed that we have eight different known carbon-based photosynthetic reactions. Each response identifies the reactions involved as well as only one from each reaction family. The acrobat version below gives the information for the carbon replaces elemental reactions in direct manner.\n",
      "\n",
      "### Storage response.\n",
      "\n",
      "### Name of it.\n",
      "\n",
      "### Cooking reaction.\n",
      "\n",
      "### Sequence of reaction.\n",
      "\n",
      "MARK WILL INCLUDE A CALENDAR PLANNING PAGE FOR THIS REQUEST\n",
      "\n",
      "Copy and paste the HTML form below into an instantly cropping form. / Submit below : ,\n",
      "\n",
      "NEST contracts get responsive at 50%, 100% and 250% over the spellcheck bar. done\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a knowledgeable tutor. Provide a clear and concise explanation to the user's query.\",\n",
    "    \"input\": \"Can you explain the basics of photosynthesis?\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ea314-04a0-4a6d-be2a-0564af7e77c8",
   "metadata": {},
   "source": [
    "## Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a59272e-7e72-4a5d-9443-e9a5e2102f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to stay productive during your working days, you can follow several tips. These tips can help you get done what you need to do while you are in the office, or to make your productivity even faster and more efficient in the office environment. Here is a list of two top choice tips:\n",
      "\n",
      "1. Cultural knowledge: Try to understand and talk about different cultures and traditions more often. You may be surprised by the wide range of wisdom and knowledge available at work, and this can help to create a better work environment.\n",
      "\n",
      "2. Prepare for your homework: Start preparing for your initial project right before you arrive at work. This will enable you to modernize your efforts and resist the temptation of starting quick in the office to avoid having the project completed quickly.\n",
      "\n",
      "3. Learn new workplace skills: These may include chatting with colleagues, listening to their puzzles, and understanding their feelings about your work. It is a great way to retain your work-life balance while also creating a project-management habit by actively listening to your team.\n",
      "\n",
      "4. Learn healthier eating habits: Do not eat extra sugar and sodium at the start of your day, as these can cause fat and blood vessel damage, and even take up space in your stomach. Drink more water and fruits and vegetables, and follow your best daytime farming practices, such as cooking for parties and eating salads, as it will help you burn fat and keep your arteries healthier. \n",
      "\n",
      "5. Regular exercise: Engage in physical activity throughout the day, preferably outside, to stay fit and prevent heart issues. Running may help in weight management and promoting an energized mind and body by boosting your mood.\n",
      "\n",
      "6. Keep a mental tally: Keep a piece of paper for point to point in your abilities log (PMT) to determine your productivity and frequency of calls, emails, and meetings.\n",
      "\n",
      "7. Stay positive and humble: By acknowledging the hard times and pride in yourself, you can be confident and contribute to a positive environment. Reinventing negative thinking about you is a great way to stay productive during 250 days or more of work and avoid stress-induced upsets.\n",
      "\n",
      "By implementing these tips and practices, you can reap the benefits of staying productive during the day by making your own choices, building value, and staying open-minded when getting restless in the office.\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a helpful assistant. Answer the user's question based on the provided information.\",\n",
    "    \"input\": \"What are some effective ways to stay productive while working from home?\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc6c611-f5d1-4182-a6af-9a3e35777bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To develop content based on the user's request, I'll write a story about a day story involving a cat that resides in a big city. Then, I'll use the information from the user's prompt to plot the event, based on the prompt, as I envisioned the event:\n",
      "\n",
      "Title: A Day of Cat and City\n",
      "\n",
      "Remote and isolated, the once beautiful city of Simar showcases the pine-forested forests for a brilliant sunrise. Little do I know that shady corners crisscross this beautiful city: ominous buildings, vast infrastructure, and architectural wonders that encourage the cat to leave her cage.\n",
      "\n",
      "Shard was my art while tracking down the lead character: As the sun set, the bustling city lights streamed off the city walls, leaving me confused and turpentine-voiced, gripped in my workspace with the hopes of casting an impromptu illumination. Instantly, I woke the cat, an epically small cat whose back jeans and boots broadcast an inner cave when angling for sunlight.\n",
      "\n",
      "Slowly, I Nintendo Land upon the screen, then I held out my arms in a Frenchie swipe, taking aim at the cat's corner, until she paused. I caught her eye and felt a tingle of excitement, but it was too late. The cat produced a shivering, aching breeze, then trotted away briskly.\n",
      "\n",
      "Breathing was hard and felt a wave of comfort. Beside my head, my fingers worked masterpiece and I felt growing uneasy with walls of symbols, of a broken glass, of a circle I had never fully contained. Inputs crested and clicks rang, dissonant clicks filled the room, all of which I felt several times a short while ago. Hearing the sharp refusal, I took a far-away breath and practiced before exhaling, a clear image of bridging the gap between the noise and the last clues the cat might weigh and decide on her own.\n",
      "\n",
      "As we pulled away from our faces, our conversation waitress replaced her emerald in the flickering candles. Sighing her contemplative goodbye, she beckoned, guiding me to stand next to the corner where the cat's cage hung, finger manga on the fumbling edge of the access.\n",
      "\n",
      "I noticed the cat put her hands on her hips at unexpected moments, usually reserved for the most scenic day out, almost joyful sightings, and the unheard woman that not only carries a\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a creative writer. Generate content based on the user's request.\",\n",
    "    \"input\": \"Write a short story about a day in the life of a cat living in a big city.\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d20b2904-2858-43cc-a6cf-b79d79ecc966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which plants convert light energy into food, which is vital to their life cycles and overall functioning. Imagine this like the way plants live, eating, and reproducing: they need food to keep growing, and their bodies store energy - energy for their life, and energy for the sun's energy.\n",
      "\n",
      "When you pass electrons from one thing to another, called a chemical process, they have to compete from different parts in their cell until they have a chance to do personal best - that is, they have to absorb too much energy from each other to function. \n",
      "\n",
      "When a plant gene is switched on, it means that the red and green parts in the plant are merged into those of the neighbouring plant, changing the plant's traits to choose a different shape so the red will act as a negative conductor, and the green will act as a positive conductor. This process allows plants to produce more energy, called energy from dark or secret locations within their cells. \n",
      "\n",
      "While it's true that light energy can be better utilized for your own health, the sun is a big neighbour like your alien friends, and photosynthesis is the ultimate reason why your well-being depends on others' well-being. The process is not limited to plant life, as we would assume. The sun, the wind, and others, have a role in the overall health of all living beings. Photosynthesis plays a major role in the climate of most of the Earth's continents too, cooking food, finding energy, and even making sunlight. Just remember that even if plants do make us, it's not all monitored and controlled - in fact, we can contact them to see if they are doing the job right.\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a knowledgeable tutor. Provide a clear and concise explanation to the user's query.\",\n",
    "    \"input\": \"Can you explain the basics of photosynthesis?\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43872df-93fd-45dd-90ce-e77ad66f4f80",
   "metadata": {},
   "source": [
    "## Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f0bb08f-a274-4999-b036-6f04c187416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Kristian Alsson\n",
      "Aston Company has announced that they will rebrand their product, with a focus on one genre of gear and check price at once with every retailer. For example, if you found an identical pair of sandals while shopping at one of their stores, they would raise the price to $50, and on the second retailer, you would be charged $29. They will also maximize their profit, as they will sell the sandals for both $49 and $67.\n",
      "If Aston is trying to determine which retailer will have the best value per pair, which store is as good a place to start as the others? Wherever! By reflecting the new $49 price, users have been left wondering which store has the best value per pair, despite the best estimate of a slightly higher overall cost of $57.\n",
      "The popular Rick Arcand guide has an article explaining the reasoning in detail, but it can be based on a spoiler-free reading shelf (there are entire books on increment marketing, and there haven't been many spoilers in this article), so if you think that a spoiler might materials how to prove the answer, or possibly win tickets to a seminar, I don't know what to tell you.\n",
      "What Salt Krueger says is You're smart to look at this one!\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": 'You are a skilled math tutor. Solve problems step-by-step, explaining your reasoning. Use clear notation and verify your answers. Ask for clarification if needed.',\n",
    "    \"input\": \"Sam has 4 apples, Tom has 2 apples. Tom gives all apples to Sam. How many apples does Sam have now? <|endoftext|>\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8c59bde-92a4-4907-abef-c292388743b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hopi Legend\n",
      "\n",
      "PORTAGE co, Alaska - Interpretation of Hopi legend is a collective effort among the Hopi people of the Piscataquis Indian Reservation in Alaska. The interpretation of the Hopi Legend involves bringing together people living in the same historical space and incorporating some elements of Native American writing and folklore. However, as a system of thought, it is based on individual beliefs and historical data and not necessarily the necessarily reliable transmission of traditional messages. With the exception of the matsukin (people of the sea), most people of the Piscataquis Indian Reservation live on the land and do not live on the sea. The understanding of Hopi Legend is more appropriate for asking the public and sensitive outsiders about higher-education topics and often involves more complex concepts and ideas. Netflix\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"You are a knowledgeable tutor. Provide a clear and concise explanation to the user's query.\",\n",
    "    \"input\": \"Can you explain the basics of photosynthesis? <|endoftext|>\"\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a57a39-0092-417c-85ff-4c3ace474bdb",
   "metadata": {},
   "source": [
    "As we can see here after the finetune, the model is able to generate response according to the instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
